,position,company,location,details
0,"Data Engineer, 1+ Years of Experience",Snap Inc.,"New York, NY","Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Snap Engineering teams build fun and technically sophisticated products that reach hundreds of millions of Snapchatters around the world, every day. We’re deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. We move fast, with precision, and always execute with privacy at the forefront.

Some core features we build and maintain include Snapchat’s Camera, Creative Tools, Maps, Chat, Memories, Stories, Discover, Games, and Minis. Our Infrastructure teams deliver an innovative and cost-efficient platform that ensures Snapchat is the fastest way to communicate with your friends, no matter where you are in the world. We have one of the fastest growing digital ad platforms, and our Monetization teams drive measurable returns for advertisers through novel ad formats like Augmented Reality. As a Snap Engineering team member, you’ll help us build the future of communication.

We’re looking for a Data Engineer to join Snap Inc!

What You’ll Do
Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner
Develop data pipelines adhering with privacy and governance principles
Become familiar with our data consumption portals and their capabilities
Build expertise and ownership of data quality for supported domains
Establish and implement data quality standards and controls
Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate
Drive adoption of the data sets you’ve produced
Knowledge, Skills & Abilities
Experience in building data pipelines to serve reporting needs
Experience owning all or part of a team roadmap
Ability to prioritize requests from multiple stakeholders in disparate domains
Ability to effectively communicate complex projects to non-technical stakeholders
Minimum Qualifications
BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field
1+ years experience in SQL or similar languages
1+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)
Experience in ETL / Data application development
Preferred Qualifications
Hands on experience with Google BigQuery
Experience in version control systems such as Git
Data architecture and warehousing experience
Experience leading a small team of data or software engineers
Experience with Airflow
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .

Our Benefits : Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid maternity & paternity leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!"
1,Senior Data Engineer,theSkimm,"New York, NY","theSkimm’

We’re hiring a Senior Data Engineer.

About our team and what we’ll build together

We’re looking for an experienced data engineer and mentor to join theSkimm's Tech team. Our mission is to enable our partners across theSkimm, from Editorial and Audio to Marketing and Advertising, to achieve their goals with the best systems and processes we can offer. We build tools throughout the stack, share knowledge across departments, and learn quickly so we can take best advantage of what’s coming.

As we grow, we're looking for a Senior Data Engineer who will help solidify and expand our pipelines and maintain our data warehouse. Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.

How you’ll contribute to our mission

Create and maintain data pipelines to provide insights and drive business decisions
Establish theSkimm’s data warehousing strategy (ex. Kimball, Data Vault, etc.)
Maintain theSkimm’s data infrastructure on our AWS accounts
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization
Write unit/integration tests, contributes to engineering wiki, and documents work
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it

You’re ready for this! Here’s a bit more about what we’re looking for

4-6+ years of industry experience measuring product performance and user behavior
Experience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others
Experience implementing BI reporting tools such as Looker
Experience interfacing with engineers, product managers and analysts to understand data needs
Knowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers
A commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support
A focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results
Ability to thrive in a dynamic, fast-paced, collaborative, and high-growth environment
Facility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealism
An empathetic leadership style that encourages open communication and trust
Understanding of the typical metrics a subscription and advertising-supported business needs to measure success
Experience with ML techniques as applied to behavioral segmentation or anomaly identification
Familiarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless)

Why we love working here

We are mission driven and values driven:

We are collectively building an impactful brand for women by helping them maneuver through life’s necessities. We are providing this community with information and resources so they have the ability, agency and confidence to live their smartest lives. Every role has a purpose and allows us to equip and enable women to make their best decisions, feel more educated and have the right knowledge at the right time.

We are values driven at every point - how we work, the way we work and our culture come straight from our core values. Each of us support this incredible brand and take part in its continued growth, fulfilling the needs of our audience each and every day.

We put you and your personal lives first - we have a generous benefits policy that demonstrates we are listening to what our employees need:

Unlimited vacation policy and generous holiday observances
A hybrid working model with flexibility for remote work
Comprehensive insurance plans and commuter benefits
Auto-enrollment into a MassMutual 401(k) plan starting on your first day and employer contribution available after one year of employment
Access to family building and fertility benefits
Rewards for every work anniversary
One month paid sabbatical after your five year anniversary as a full-time employee

We support our parents at all points on their journey:

Generous paid parental leave
Access to family building and fertility benefits
Flexible and broad scale child support program

Your well being is our priority:

We honor Sacred Time in our workplace
We have aligned company “time off” during the summer to truly allow for us all to break from our work at the same time
We offer fitness membership reimbursement
One Medical Membership is included with our benefits

We have a vibrant, collaborative and supportive culture (we celebrate and have fun!):

Weekly company updates led by our co-founders/co-CEOs
Weekly lunch stipend for HQ employees
ERGs
Employee awards ceremonies to celebrate individual accomplishments
Clubs and activities designed to meet other employees like book clubs, new hire buddy programs, rooftop lunches, off-sites, and wellness focused classes
Numerous culture events that enable our workforce, in the office and remote, to connect and have fun

Your career and development are a priority:

Annual learning and development stipend
LEAD@theSkimm, our leadership development program
DEI initiatives to ensure we are the best partners and colleagues to each other
Career development guidance and planning

And more…

Competitive salary and equity packages
Complimentary access to theSkimm’s suite of membership products and swag
The opportunity to be part of a values-driven, hardworking, and diverse group of people building a media company that makes it easier to live smarter

Our story, Skimm’d

We’re a mission-driven digital media company giving millennial women the information they need to live their smartest lives.

At our core, we are writers, editors, producers, designers, marketers, engineers, analysts, sellers, creatives and strategists all working together to achieve this goal.

Since launching in 2012, we have become a trusted source for a community of millions of Skimm’rs by seamlessly integrating into their existing routines, fundamentally changing the way our audience consumes news and makes decisions. Our flagship product, the Daily Skimm, remains the fastest-growing newsletter on the market, and we continue to innovate, find new ways to Skimm everything from finance and careers to voting, wellness and downtime. Our product suite has expanded to include three top-rated podcasts, “Skimm This'', “Pop Cultured”, and “9 to 5ish,” theSkimm mobile app, Skimm Money and Skimm Your Life newsletters, a new virtual course series called SkimmU, and Skimm Studios for innovative in-house video and audio content. Our first book, How to Skimm Your Life, was released in June 2019 and debuted at #1 on The New York Times Best Seller list.

And we’re not done! We continue to grow the breadth of our offerings in life’s necessary categories and by expanding our products across events, commerce, customized content and audio, our ecosystem will become the go-to resource for this generation - and the ones that follow - in the most critical areas of their lives.

Come join us!

What about the office?

This is a full-time role based with flexibility for remote work.

Our office is available to use three days a week - Tuesday, Wednesday, and Thursday. We have a hybrid environment and ask that employees within the New York Metro area come together in person a minimum of three days per month. All other days are up to the discretion of the employees during the three days a week that the office will be open. We are continuing to closely monitor the state of COVID-19 in New York and will adjust this plan accordingly.

We will require COVID-19 vaccinations for anyone coming to our office. We are following CDC recommendations and will also require additional boosters, and will always follow CDC and NY state guidance on wearing masks in the office."
2,Sustainable Finance Data Engineer,Morgan Stanley,"New York, NY","The Global Sustainable Finance Group (GSF) aims to drive the growth of sustainable investing through ongoing development of products and solutions, economic analysis, thought leadership and capacity building initiatives.

The GSF team is seeking a Data Engineer to support the team's activities, with a particular focus on the team's expanding efforts in geospatial, high-frequency and distributed ledger data and analytics.

Successful candidates will have a dual passion for sustainability and technology, as well as demonstrated expertise in data engineering and distributed systems.

Additionally, successful candidates will be well-organized and detail oriented, and will work well in a collaborative team environment.

Description

The Data Engineer will be responsible for:
Working with GSF data team and various internal partners to maintain the existing infrastructure for ingesting, fusing, and distributing sustainability data.
Working alongside cross-functional/firm-wide business and technology teams to design and implement solutions to further the integration of sustainability considerations into investment processes across various asset classes.
Collaborating with the data infrastructure team to build tools and processes to rapidly ingest additional sustainability content and ESG data feeds.
Ensuring a high standard of data quality and availability with focus on automated testing/regression, monitoring workflows and adjusting processes as needed.
Overseeing different data related projects across all phases of development, requirements analysis, application design and implementation.
Analyzing and planning implementation of data governance requirements across data sets and entities.
Required
Bachelors or Masters in Computer Science, Software Engineering, or a similar field, with strong exposure to data infrastructures.
Experience with data engineering, building, deploying and maintaining large, complex data services and pipelines on distributed systems.
Fluency in Python and SQL, experience with workflow scheduling tools.
Experience maintaining and curating meta-data libraries and Git code repositories and supporting notebook-based data science workflow.
Exposure to one or more of the following areas is a plus:
Experience implementing machine learning algorithms of prediction and classification.
Experience with geospatial data integration, management and processing.
Experience in implementing data visualizations in Tableau, Dash or Power BI.
Working knowledge of Scala, Java, and/or kdb/Q.
Posting Date

Mar 30, 2022

Primary Location

Americas-United States of America-New York-New York

Education Level

Bachelor's Degree

Job

Global Sustainable Finance

Employment Type

Full Time

Job Level

Associate"
3,Data Engineer- Security Analytics & Reporting,IBM,"Armonk, NY","526316BR

Introduction

As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities

Data Engineers work with Data Analysts and Data Scientists to improve the quality and accuracy of the information, enabling businesses to make more responsible and informed decisions.

We are looking for an experienced data engineer to join our team:

You will use various methods to transform raw data into useful data systems.
You will create data models to facilitate statistical analysis and optimal data retrieval.
You will strive for efficiency by aligning data systems with business goals.
You will have strong analytical skills and the ability to combine data from different sources creating effective data models that will provide accurate business answers.

Above all, we are looking for applicants who will thrive in an open, vibrant, flexible, fun-spirited, collaborative environment and desire creative freedom and an opportunity to work on high performing teams. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition communicating with internal customers
Work in an agile, collaborative environment
Collaborate with data analysts, data scientists and architects on several projects
Build algorithms and prototypes

About The Team

The SOS Security Analytics & Reporting Tribe

SYSTEMSPROUD

Required Technical and Professional Expertise

4+ years of experience developing and implementing data models
2+ years of experience writing and analysing complex SQL
1+ years Cognos (or similar) experience
1+ years of experience in data analytics, statistics, or similar field.
Exceptional technical knowledge in data modeling
Excellent technical understanding of databases and data storage systems
Excellent problem-solving skills
Excellent communication and collaboration skills

Preferred Technical And Professional Expertise

Understanding of security and compliance
Experience with dashboards and reports design
Experience with Python, R, Java or similar

About Business Unit

IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world’s most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfolio designed for cognitive business and optimized for cloud computing.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
4,NEED FOR DATA ENGINEER @ (REMOTE),Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, 3A Soft Inc, is seeking the following. Apply via Dice today!

JD AWS, PAYTHON NAVEEN KUMAR mailto Direct 7329672703"
5,Data Engineer - Remote,Mayo Clinic,"Phoenix, AZ","Responsibilities

Mayo Clinic is seeking a motivated Data Engineer to be responsible for creating and maintaining the analytical infrastructure that enables most functions in the data world. You will be responsible for and development, testing, and maintenance, of architectures for large-scale medical related databases in GCP using Big Query and other state of the art systems. You will also be responsible for creating data set processes for verification, acquisition, mining and modeling of clinical data through micro-services and API’s.

Create and maintain optimal data pipeline architecture using state of the art systems that access data via API’s. Ability to build and optimize data sets, 'big data' data pipelines and architectures. Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Ability to build processes that support data transformation, workload management, data structures, dependency and metadata in GCP. Develop and test large, complex data sets that meet functional / non-functional business requirements. Implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data source formats using on premise and cloud technology. Integrate object storage features within cloud storage to create data at rest driven compute models. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Keep Mayo’s data separate and secure across national boundaries through multiple data centers and cloud regions Work with data and analytics experts to strive for greater functionality in our data systems. Continues to build knowledge of the organization, processes and customers. Performs a range of mainly straightforward assignments. Uses prescribed guidelines or policies to analyze and resolve problems. Receives a moderate level of guidance and direction.

Organizational Relationships:

Mayo Clinic is respected and recognized as a world leader in healthcare, medical research, and clinical expertise. As we accelerate movement into the digital future, Mayo fully recognizes the next generation of health care will dramatically change, driven by consumerism, technology, and data. The Mayo Clinic 2030 strategy of CURE, CONNECT & TRANSFORM sets the vision of transforming healthcare as the global authority in the care of serious or complex disease. Advanced use of data, ML/AI and traditional analytics is woven through all aspects of this strategy and considered foundational to achieving it. The Department of Data & Analytics (DDA) within the Center of Digital Health (CDH) is responsible for driving Mayo's Data, Data Management and Data consumption vision. DDA will strive to enable a data centric organization, provide supporting capabilities to meet the needs of all organizational stakeholders, and establish an environment of continuous improvement with high levels of proficiency.

Developer, Programmer, Software Engineer, Data Engineer

Why Mayo Clinic?

Mayo Clinic is the nation's best hospital (U.S. News & World Report, 2020-2021) and ranked #1 in more specialties than any other care provider. We have a vast array of opportunities ranging from Nursing, Clinical, to Finance, IT, Administrative, Research and Support Services to name a few. Across all locations, you’ll find career opportunities that support diversity, equity and inclusion. At Mayo Clinic, we invest in you with opportunities for growth and development and our benefits and compensation package are highly competitive. We invite you to be a part of our team where you’ll discover a culture of teamwork, professionalism, mutual respect, and most importantly, a life-changing career!

Mayo Clinic offers a variety of employee benefits. For additional information please visit Mayo Clinic Benefits . Eligibility may vary.

Qualifications

Bachelor’s degree in Computer Science or Engineering from an accredited University or College.

Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee).

Additional Qualifications

Additionally, experience with Python, Denodo, along with ETL and ELT toolsets would be benifitial. GCP Certification would also be a plus.

Have working knowledge and experience in Data Engineering with a minimum of 2 years of experience in data engineering and data science or analytical modeling.

Experience using scripting languages (Python, JavaScript).

A minimum of 2 years of experience leveraging micro-services using a high-level language (C#, C++, Java) for data access and analytics.

Strong interpersonal, time management skill and demonstrated experience working on cross functional teams

A minimum of 1 year of SQL or No-SQL experience.

Experience working in an agile development environment leveraging tools such as Jira.

Experience with scrum, coding from user stories, and performing retros.

Preferred qualifications for this position include:

Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Pig and Kafka.

Experience using big data, statistics and knowledge of data related aspects of machine learning.

Experience working with Linux or other Unix based operating systems.

Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems.

Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment.

Experience in practicing CI/CD (Jenkins, GitHub Actions, ADO)

Experience with cloud platforms such as GCP, Azure, AWS

Exemption Status

Exempt

Compensation Detail

Required education, experience, and tenure may be considered along with internal equity when job offers are extended. The salary range every 2 weeks is approx. $3,496-$4,894 based on a full-time position.

Benefit Eligible

Yes

Schedule

Full Time

Hours/Pay Period

Full Time

Schedule Details

Monday-Friday

Works remotely 50% or more; business need requires employees to live within a reasonable driving distance of campus.

Weekend Schedule

Weekend work as needed tied to product delivery needs.

Remote Worker

Yes

Site Description

This vacancy may be open to full time telework candidates within the U.S./ reasonable travel to a Mayo facility if ever a business need/ meeting or client-facing. Mayo Clinic has facilities in Minnesota, Wisconsin, Arizona, and Florida.

Department

Information Technology

International Assignment

No

Country

United States

Job Posting Category

IT

Career Profile Page

Information Technology

Recruiter

Miranda Grabner

Equal Opportunity Employer

As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available."
6,Data Engineer/SQL Developer,IBM,"Washington, DC","552929BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

We’re looking for an experienced data engineer with strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka.

This team is collaborative, disciplined and self-directed. They take a lot of pride in their work and are looking for like-minded professionals to join them

Required Technical and Professional Expertise

strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka

Preferred Technical And Professional Expertise

This is a dynamic Data Engineering team in the personal finance industry. This well-established organization offers information on credit card selection, college loans, banking, mortgage loans, stock trading, and insurance policies

About Business Unit

IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
7,Data Engineer,Morgan Stanley,"Alpharetta, GA","Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management, and wealth management services. The Firm's employees serve clients worldwide including corporations, governments and individuals from more than 1,200 offices. As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence and strong team ethic. Morgan Stanley can provide a superior foundation for building a professional career - a place for people to learn, to achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.

Technology works as a strategic partner with Morgan Stanley business units and the world's leading technology companies to redefine how we do business in ever more global, complex, and dynamic financial markets. Morgan Stanley's sizeable investment in technology results in quantitative trading systems, cutting-edge modeling and simulation software, comprehensive risk and security systems, and robust client-relationship capabilities, plus the worldwide infrastructure that forms the backbone of these systems and tools. Our insights, our applications and infrastructure give a competitive edge to clients' businesses—and to our own.

Morgan Stanley Enterprise Technology Service is looking for a data engineer to join our Data Service & Analytics team. The Data Engineer will support our data analysts and data scientist, ML engineer by working on analytics and ML projects as well creating fundamental component to support wider user communities across line of business in Morgan Stanley. The candidate must be self-motivated, dedicated, accountable and able to identify issue and find solutions independently.

Responsibilities
Support our existing big data platform.
Expand and enhance our data pipeline.
Design, build and support our analytics and AI/ML platform and ML pipeline on cloud.
Support AL/ML operation on ML governance, process and release management.
Identify process improvement opportunities, automate manual processes and optimize data delivery.
Create prototyping for analytics and AI/ML tools
Support user communities to build applications on the platforms the team build.
Required
Working experience on big data platform such as Hadoop, Databricks, etc.
Working experience with Cloud platform such as AWS and Azure.
Working coding experience on programming languages such as Python and Java.
Understanding of data analysis techniques and procedures
Understanding of machine learning process and operation
Understand security principal and practice on cloud platforms
Understand data governance and ML governance principals
Strong communication and organization skills.
Bachelor's degree in Computer Science or related fields required, Master's desired
Desired
Working experience on ETL tools such as AWS Glue ETL or Databricks
Working experience on AWS Glue and SageMaker
Working experience with tools used by Data Scientist
Working experience on network and security related areas
3 years’ of experience on data engineering, ml engineering or ml operation engineering
Posting Date

Mar 3, 2022

Primary Location

Americas-United States of America-Georgia-Alpharetta

Education Level

Bachelor's Degree

Job

Engineering

Employment Type

Full Time

Job Level

Associate"
8,"Data Engineer, Capital Management",Uplift Inc.,United States,"Posted by
Bradley Callahan
👨🏼‍💻 Technical Recruiter | Come Join Uplift🚀 - We're Hiring! 😄
Send InMail
Uplift’s mission is to help people get more out of life, one thoughtful purchase at a time. Our enterprise Buy Now, Pay Later solution is used by the world’s most loved brands including Southwest Airlines, Carnival Cruise Line, Universal Studios, and more. With flexible pay over time installments, we empower consumers to buy what matters most while unlocking higher conversions and customer lifetime value for our partners.

Our team is rapidly growing and comes from diverse backgrounds of leading technology and financial brands. Our HQ is in Sunnyvale, CA with offices in Toronto, New York, Reno, and Guadalajara, Mexico.

Working at Uplift allows you to push your limits, challenge the status quo, and collaborate with some of the brightest minds in the industry. We’re committed to building a diverse team and inclusive culture and believe your potential should only be limited by how big you can dream. We make this a reality by empowering you with the tools, resources, and support you need to grow your career.

The Capital Management Squad is an interdisciplinary team of subject matter experts in capital markets, Financial Analysts, Data Analysts, and Data Engineers. As a Data Engineer, we aim to design, implement, and operate a portfolio of financial processes with technology to be the best in class in Capital Operations. We deliver business value in financial and organizational agility through process automation, accurate data, and auditability support. We use many industry-leading technologies like Apache Spark, Delta Lake, Snowflake, and AWS. We also work closely with the rest of the Data Engineering team, who continues to build up the next-generation data lake on Databricks.

Our data pipelines and application stacks are mission-critical, and we move millions of dollars with each transaction. Come join us!

What You Will Do And Achieve
Be a jack of all trades and a master of two: Data Engineering and Capital Markets
Design, build and maintain mission-critical data pipelines in AWS that support our Capital Markets operations
Understand Capital Markets business processes/activities, and improve them with technology: automating manual processes, building alerting/monitoring bots, designing investor-facing workflows, etc.
Adapt and evolve the Loan & Transaction data models, as new features and products are launched
Help Data/Financial Analysts and Data Scientists to extract actionable insights from data that shape our Capital Markets strategy and the direction of the company as a whole
Actively engage in design and code reviews - learn from your peers and teach your peers
Take a part in researching and proposing new technologies and tooling for the Capital team’s Data and Application stacks
Who You Are
Passion for data both big and small, and passion for delivering business value
Interest in learning about Capital Markets and Finance (no prior experience necessary)
Strong communication skills to translate complicated technology and data issues into business-friendly, simple language
Solid Python skills to get jobs done with decimal-point precisions and without NumPy/Pandas
Solid SQL skills beyond DQL
Familiarity with AWS, or any other cloud infrastructure. We are all-in on AWS!
Familiarity with cloud data warehouse and data lakes: we use Snowflake and Databricks
Ability to align with rapid business changes: new requirements, evolving goals and strategies, and technological advancements
Comfortable working in a startup culture with the ability to earn trust: entrepreneurial, persistent, with the desire to go deep into details
Set yourself apart with
BA/BS degree in Engineering, CS, or equivalent, Master's degree preferred
2+ years of experience in data and/or software
Experience in Apache Spark (PySpark specifically), Apache Airflow
Experience with data warehouses and data lakes – we use Snowflake & Databricks
Experience with big data, ETL, and data modeling
Experience in developing and operating high-volume, high-available and scalable environments
Life at Uplift
Health Insurance and 401k plan: some plans cover 99-100% premiums for medical, dental, and vision insurance and a 401k plan
Work/Life Harmony: Flexible, remote-first work culture. Uplift fosters a culture where employees can achieve both their professional and personal goals. This balance is especially true for our working parents
Shared Success: competitive salary and Pre-IPO stock options
Health and Wellness Perks: Uplift is proud to reimburse our employees for exercise, wellness products and activities as well as free counseling and coaching for physical, mental and emotional support
Professional Development: We are committed to the growth and development of all of our employees. Uplift invests in professional conferences, certifications, and training for employees who want to grow in their careers
Pick-A-Perk: money that can go towards something of your choosing within tuition reimbursement, student loan payment reimbursement, vacation savings account, charitable donations, or home office expenses
We want you!

If you made it this far, chances are you’re as excited about working to change how people experience BNPL as we are — and we love that. Please apply even if you’re unsure about whether you meet every single requirement in this posting. Uplift is looking for smart, intellectually curious people who are invested in our mission, not just those who can “check all the boxes”.

Uplift is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Note: Uplift does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Uplift is not responsible for any fees related to unsolicited resumes."
9,Data Developer/Engineer,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, The Atlantic Group, is seeking the following. Apply via Dice today!

Our client, a high-profile equities investment management firm with over 400B in AUM with offices in San Francisco, NYC, Shanghai, Hong Kong, Tokyo and London, is hiring! They are looking for a talented Data DeveloperEngineer to work out of their NYC office. The ideal candidate is someone who can apply analytics and quantitative concepts to support investment needs and develop new data and reportingvisualization solutions. This candidate will also work with RiskMarketingFA teams on various internal initiatives and projects, including proof-of-concept, prototype, and production solutions. Responsibilities Develop and maintain programs on source systems, ETL applications, data cleansing functions, systems management functions including load automation, and data acquisition functions among others Manage and check regular reporting where necessary, such as for Marketing and Risk purposes Qualifications Bachelorrsquos Degree 5+ years of experience in analytical development andor data management Full-stack software development knowledge. Proven track record in hands-on development of analytical solutions High level of proficiency in SQL and quantitative programming (R, Python, MATLAB, VBA) Domain knowledge of fundamental equity Familiarity working with service providers is a plus ndash SSC, IVP, Bloomberg Experience with integrating data visualization platforms such as PowerBI, Tableau, Lightkeeper is a big plus"
10,Data Engineer/SQL Developer,IBM,"Chicago, IL","552929BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

We’re looking for an experienced data engineer with strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka.

This team is collaborative, disciplined and self-directed. They take a lot of pride in their work and are looking for like-minded professionals to join them

Required Technical and Professional Expertise

strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka

Preferred Technical And Professional Expertise

This is a dynamic Data Engineering team in the personal finance industry. This well-established organization offers information on credit card selection, college loans, banking, mortgage loans, stock trading, and insurance policies

About Business Unit

IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
11,Data Engineer,Deloitte,"Washington, DC","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
12,"Data Engineer, Spark",Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
13,Data Engineer: Application Modernization,IBM,"Armonk, NY","529272BR

Introduction

Engineers at IBM are the backbone of our strategic initiatives to design, code, test, and provide industry-leading solutions that make the world run today - planes and trains take off on time, bank transactions complete in the blink of an eye and the world remains safe because of the work our software developers do. Whether you are working on projects internally or for a client, software development is critical to the success of IBM and our clients worldwide. At IBM, you will use the latest software development tools, techniques and approaches and work with leading minds in the industry to build solutions you can be proud of.

Your Role and Responsibilities

Application modernization is a new initiative in IBM, which is transforming the way IBM works. We are creating a better developer experience for all developers within IBM. Data Engineer’s primary responsibility is to drive the transformation of IBM applications to modern database architecture, to be deployed on Hybrid Cloud. We cultivate an open, healthy, diverse, and engaging work environment where team members are continuously gaining new skills which align with individual interests.

Responsibilities May Include

Ability to analyze data and existing database design with the goal of enacting improvements geared towards making processes more efficient.
Evaluating and selecting appropriate data modernization tools to enhance our teams’ productivity and user experience.
Participate in defining CIO’s data architecture framework, standards, and principles including data management, security, and infrastructure.
Leads multi-functional teams or conducts special projects.

You will be challenged to design and develop scalable systems, leveraging a wide variety of open source and commercial technologies, closely collaborating with IBM Research and Red Hat.

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

IBM will not be providing visa sponsorship for these positions now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Required Technical and Professional Expertise

Education

A combination of education, technical training, and/or work/military experience equivalent to a bachelor’s degree in computer science, IT, etc.

General

Excellent time management and interpersonal skills
Familiarity with Agile methodologies and principles, experience in an Agile team
Understanding of DevOps practices (Continuous Integration/Delivery/Monitoring/etc.)
Willingness to work with the entire stack (from the database to containers to monitoring and alerting)
Ability to write and review code and make decisions regarding technical implementations
Growth mindset

Technical Skills

5+ years’ experience Applied Knowledge of Software Development
3+ years’ experience in DB2 or other relational databases or NoSQL databases
Knowledge of ETL practices and methodologies
Application SQL development and SQL Performance tuning, where applicable

Preferred Technical And Professional Expertise

Previous experience as a Backend or Full Stack Developer
Automated testing frameworks
Experience in working with containers and container management systems including Docker, Kubernetes and/or OpenShift
Cloud computing knowledge

About Business Unit

The Office of the Chief Information Officer (CIO) owns IBM’s IT strategy and provides the tools, workstations, devices, and infrastructure that IBMers use to do their jobs every day. Put simply, our mission is to create a productive environment for IBM's 350,000 worldwide employees. Join us as we lead with design to drive simplicity and ease of use, engineering the systems that run the business, and innovating to transform the business.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
14,Data Engineer,Deloitte,"McLean, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
15,Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
16,Data Engineer,"Casdin Capital, LLC",New York City Metropolitan Area,"Firm Overview

Casdin Capital, LLC (“Casdin”) is a New York-based fundamental research investment firm focused on the life sciences and healthcare industry. The firm manages a long-short equity fund and makes investments in early-stage to late-stage private investments. Casdin was launched in 2012 by founder and CIO Eli Casdin and manages a multi-billion dollar fund.

At Casdin we start from a position of curiosity, honesty, and passion to gain a deep and thoughtful understanding of opportunities in the life science ecosystem. We commit to finding the highest quality management teams doing the work to create high-growth companies. We use data to continually advance our knowledge and we are long-term investors. We believe in collaboration and transparency and operate at the forefront of innovation.

Position Overview

As the data team’s second hire, the data engineer will be a key part of the future development of the data strategy at Casdin Capital. The data team is highly integrated within the organization and the deliverables have high visibility and engagement with all senior leaders within the organization.

On the research side, our goal is to build a world-class data engine that advances the investment platform, by continuing to build a data-informed industry-leading portfolio construction program. This data engine will be utilized by our investment team to drive the success of the investment decision-making process and we are passionate about delivering best-in-class technology with a scalable and resilient approach. We utilize data to drive idea generation, decision making, and risk mitigation. The data engineer will be instrumental in developing the data flows that are the foundations of these processes.

On the reporting side, the data team drives analysis and data visualization to support key business initiatives and the key drivers of the business. We leverage technology for report automation, ETL, and web-based dashboards. The data engineer will continue the core development of the code repositories, working on automation, data flows, data quality, and data manipulation to both support and advance these efforts.

The data engineer will combine technical proficiency with an understanding or keen interest in markets to take ownership of existing data programs and look to further enhance and evolve our approach. Casdin is in a growth phase and there will be a significant opportunity for the role to grow through the build-out of additional internal products and programs. We are looking for someone with a high degree of intellectual curiosity who enjoys thinking about and discussing architectural and design approaches, engineering best practices, and risk analysis.

Responsibilities

Build automatable processes in SQL & Python related to data flows into and out of the SQL server data warehouse (data sources include third-party vendors, APIs, admins, etc).
Manage the internal mapping tables for underlier roll-ups, custom tagging, ticker mapping across vendors, etc.
Ensure data quality by continuing datacheck development and correcting errors highlighted in daily datacheck reports.
Continue core development of the reporting Git repositories as it relates to reporting process automation, reporting requirements across the organization, and ETL.
Maintain and expand the ETL and data management repo.
Handle ad-hoc data requests across the firm to support internal needs, taking vague requests and intuitively understanding the request to generate excel files, PDFs, Powerpoints, and emails.
Handle month and quarter-end reporting for marketing, finance, and the data scientist.
Maintain and enhance web-based dashboards that service the organization.
Experience with developing and maintaining internal web-based dashboards.

Requirements/Qualifications
4+ years of SQL experience, with the ability to code stored procedures and views.4+ years of Python experience with the ability to build data flow processes and interact with a SQL server database.
Development in a Git environment adhering to best practices around code quality.
Experience working with market data or exposure to investing and portfolio management.
Ability to work in an agile type development environment, with strong communication skills for cross-functional requirements gathering and stakeholder management. This individual will need to work across multiple teams simultaneously.
Proactive with a mind toward efficiency in the process and systems that we build and a keen interest in the effectiveness of the solutions we create.
Exceptional attention to detail with the ability to produce error-free work and quickly find issues within a dataset.
Passion for and understanding of equity capital markets."
17,"Data Engineer, Spark",Deloitte,"Jericho, NY","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
18,Data Engineer - GTPC Technology team,Amazon Web Services (AWS),"New York, NY","Job Summary

DESCRIPTION

Do you want to innovate technical solutions that can provide positive impact in the world?

Come join our new teams in , you’ll help us design and develop solutions that enables to expand our operations into new data centers worldwide. Our solutions will support programs in sustainability and compliance, and build a platform that will extend to first-class services. You’ll join our team in Security Assurance, collaborating with engineers designing solutions at massive scale.

As a Data Engineer, you will join the ranks of a large and growing community of senior engineers who are redefining how distributed systems are scaled and operated. In this role, you will have ownership of an end-to-end development of data engineering solutions to complex questions and you’ll play an integral role in strategic decision-making with our team of software developers. You believe in high standards for code quality, code reviews, , and operations, and help develop engineers in GTPC and beyond. You believe technical standards are a mechanism for achieving results and are committed to contribute to ’s overall technical architecture and engineering practices.

About Us

Inclusive Team Culture

Here at , we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee- led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. ’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team also puts a high value on work-life balance. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here, which is why we aren’t focused on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well balanced life—both in and outside of work.

This position involves on-call responsibilities, typically for one week every two months. We don’t like getting paged in the middle of the night or on the weekend, so we work to ensure that our systems are fault tolerant. When we do get paged, we work together to resolve the root cause so that we don’t get paged for the same issue twice.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.


Basic Qualifications
Bachelor’s Degree in Computer Science or related field, or equivalent experience
5+ years professional experience in data engineering, business intelligence, data science or related field
Experience with technologies including , , , , EMR, EML or similar solutions
Demonstrated strength in data , development, and data warehousing
Experience in , Java or other similar languages
Experience with data management fundamentals and data storage principles
Experience with distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience with serverless computing, enterprise-wide systems, and products
-prefer 5-10 years experience
Excellent knowledge of Advanced working with large data sets
Knowledge of Advanced Statistics and implementing models
Experience working successfully with partner teams at remote locations
Understanding of Agile software engineering practices
Experience mentoring software engineers
Meets/exceeds ’s leadership principles requirements for his role
Meets/exceeds ’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A2023757"
19,"Data Engineer, Spark",Deloitte,"Parsippany, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
20,Azure Data Engineer,Quadrant Resource,"Redmond, WA","Posted by
Chanukya Sailada
US IT Recruiter
Send InMail
Prior Microsoft Client Experience is needed.

Position: Azure Data Engineer
Location: Redmond, WA
Duration: Long term Contract
Job Description :
Qualifications:
5+ years’ experience with Data Pipeline workflow and orchestration. Expert level TSQL/SQL knowledge required.
5+ years’ experience designing/implementing/maintaining scalable ETL processes including data movement (SSIS, Azure Data Factory, etc.) and quality tools.
2+ years’ experience building cloud hosted data systems. Azure preferred.
Preferred:
Building pipelines in Azure Data Factory.
Working with data in and from Azure Data Explorer.
Nice to Have:
2+ years’ experience with modern Big Data Analytics using Data Lake, Spark and formats like Parquet.
Experience building and maintaining Delta Lakes."
21,Data Engineer,Deloitte,"Falls Church, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
22,Data Engineer/SQL Developer,IBM,"San Jose, CA","552929BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

We’re looking for an experienced data engineer with strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka.

This team is collaborative, disciplined and self-directed. They take a lot of pride in their work and are looking for like-minded professionals to join them

Required Technical and Professional Expertise

strong SQL and Python coding skills to support the data team. Currently migrating off Redshift to Snowflake.

Must have demonstrated experience with ETL pipelines, SQL and Python as well as knowledge of Airflow and Kafka

Preferred Technical And Professional Expertise

This is a dynamic Data Engineering team in the personal finance industry. This well-established organization offers information on credit card selection, college loans, banking, mortgage loans, stock trading, and insurance policies

About Business Unit

IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
23,"Data Engineer, Spark",Deloitte,"Princeton, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
24,Data Engineer,D+R International,United States,"Posted by
Meredith Donate, PHR
Senior Human Resources Manager
Send InMail
Data Engineer
This position is remote and candidates can live anywhere in the U.S. to apply. Applicants must apply via our website for consideration!

We are D+R International, an energy efficiency and environmental consulting firm. Our headquarters is based in Silver Spring, MD. We provide our clients with incisive market analyses, strategic guidance, creative marketing concepts, and program coordination services. At D+R, we cultivate changes that improve the environment and the quality of people’s everyday lives.
We are seeking a candidate who will work with our program and software development teams to lead reporting development and visualization efforts, as well as the creation of new data pipelines to meet the business needs of D+R’s internal and external stakeholders.
Our team members are hard-working, eager, and creative people who support energy efficiency, training and data analytics programs. Through our programs, we connect people, data, and opportunities to transform disruption into innovative market solutions.
Job Requirements
The primary job responsibilities will include:

Maintaining and improving current data pipelines that feed internal and external project dashboards and reporting applications or simplifying self-service modeling and production support for clients.
Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Collaborate with Software Development Team to help design, analysis, validation, and documentation.
Build ETL/ELT processes, implement ingestion routines both real time and batch using best practices in modeling.

EDUCATION:
BA/BS degree in a related discipline or relevant experience/training
EXPERIENCE AND SKILLS:
Basic Qualifications:
2+ years of experience in a data engineering or similar role.
Experience implementing solutions in Microsoft SQL, Including the Azure environment
Python, R, or other Object-Oriented programing language
Experience with developing dashboards in Microsoft BI, Tableau, R-Shiny Apps, or other data visualization tools
Experience in building CI/CD pipelines
Experience of data management fundamentals and data storage principles
Experience with data modeling, data warehousing, and building ETL/ELT pipelines
Familiarity with Microsoft Office applications (Word, Excel, Outlook, PowerPoint) Google Drive documentation and general business SaaS applications
Ability to interact effectively with staff at all levels internally and externally

Preferred Qualifications:

5+ years pf experience as a Data Engineer, BI Engineer, or similar role
Demonstrated strength in data modeling, ETL/ELT development, and data warehousing.
Experience providing technical leadership and mentoring other engineers for best practices on data engineering.
Experience using big data technologies (Hadoop, Spark, etc)
Experience with machine learning algorithms or other advanced data pipeline automation techniques
Join a leader in moving the residential, commercial, and industrial sectors toward greater energy efficiency and data control. For over 30 years, D+R has promoted sustainable practices with tailored market- and business-based solutions that support the bottom-line results critical to our partners and clients.

We are looking to expand our team of invested individuals looking to make an impact in the energy and data world. If you have a keen eye for detail, an interest in energy efficiency, and the desire to join a collaborative, knowledgeable, and adaptive work place, apply today!

We recognize that diversity, equity, and inclusion are essential values for a truly sustainable future. Only by engaging diverse perspectives and encouraging thoughtful collaboration, can we accomplish D+R’s mission to connect people, data, and technology to transform markets and build a sustainable future. Therefore, we work every day, at all stages of our hiring process and in employment, to encourage diversity and offer inclusiveness and equal opportunities.
D+R offers a competitive salary, generous benefits packages, and flexible work schedule.



PHYSICAL, MENTAL & LOGISTICAL REQUIREMENTS:
Language Skills: high skill
Mathematical Skills: high skill
Reasoning Skills: high Skill
Physical Demands: Very low
PHYSICAL
On the job the employee must:
1. Bend- Not at all
2. Squat- Not at all
3. Crawl- Not at all
4. Climb- Not at all
5. Kneel- Not at all
6. Use hands to finger, handle, or feel- Frequently
7. Reach above shoulder level- Not at all
8. Talk or hear- Frequently
9. Sit- Not at all
10. Stand- Not at all
11. Walk- Not at all
12. Push/Pull- Not at all
13. See color distinctions- Not at all
Must carry/lift loads of:
14. Light (up to 25lbs.)- Not at all
15. Moderate (25-50lbs.)- Not at all
16. Heavy (over 50lbs.)- Not at all
Work Environment
On the job the employee:
17. Works in a comfortable environment in typically pleasant conditions- Frequently
18. Is exposed to noise- Not at all
19. Is around moving machinery- Not at all
20. Is exposed to marked changes in temperature and/or humidity- Not at all
21. Has a risk of electric shock- Not at all
22. Is exposed to fumes or airborne particles- Not at all
23. Works in confined quarters- Not at all"
25,Data Engineer - NYC (Remote Till-COVID) - Full time/Contract,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, ClifyX, is seeking the following. Apply via Dice today!

Hi, Greetings, This is Hasan Choudhary from ClifyX. Hope you are doing well! ClifyX was formed in 1998 and since then providing staffing solution and services. ClifyX group is an award winning IT Consultancy formed in 1998. Our Mission is to provide our clients with Optimal Technology solutions that are effective and within budgets. We specialize in helping Organizations to review their strategic SOW ProjectsTalent needs and implement high value and cost effective solutions to increase profitability and efficiency. Our consulting capabilities include expertise in Cloud, Artificial Intelligence, Data Analytics and compliance aspects of Cyber Security development. Kindly respond to this requirement with your resume, contact, visa status, rate and current location info to speed up the interview processes. Title Data Engineer Locations NYC (Remote Till-COVID) Duration Full timeContract Responsibilities- Utilize strong SQL Python expertise to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features. Build reporting dashboards and visualizations to design, create and track campaignprogram KPIs Perform analyses on large data sets to understand drivers of marketing engagement and provide recommendations on campaign and product optimization Project manage end-to-end process of analytics tooling feature development, including request intake, requirements evaluation, cross-functional team alignment, feature execution, QA testing, and stakeholder communications Interface and consult with marketers, analysts, and cross-functional partners to understand their reporting and data needs, serving as the point of contact for requests, inquiries, and action items Interface with other data engineering, product, and data science teams to implement client needs and initiatives. TLDR Strong SQL and Python skills are essential to be successful at this role. Regards, Hasan Choudhary (Executive Recruiter ) Tel ndash Fax - Email - hasanatClifyXdotcom mailto"
26,Data Engineer I,Condé Nast,"New York, United States","Condé Nast is a global media company producing the highest quality content with a footprint of more than 1 billion consumers in 32 territories through print, digital, video and social platforms. The company’s portfolio includes many of the world’s most respected and influential media properties including Vogue, Vanity Fair, Glamour, Self, GQ, The New Yorker, Condé Nast Traveler/Traveller, Allure, AD, Bon Appétit and Wired, among others. Condé Nast Entertainment was launched in 2011 to develop film, television and premium digital video programming.

Job Description

Location:

New York, NY

Condé Nast is a premier media company renowned for producing the highest quality content for the world's most influential audiences, attracting more than 100 million consumers across its industry-leading print, digital and video brands.

Condé Nast is home to many of the world's most-celebrated magazine and website brands. The company's reputation for excellence is the result of our commitment to publishing the best consumer, trade, and lifestyle content. Our brands include Vogue, Epicurious, Vanity Fair, The New Yorker, Wired, and many more. Passion is the core of our philosophy at Condé Nast. Our mission is not only to inform readers but to ignite and nourish their passions.

The Data Engineering team within the Data Organization have a wide range of responsibilities and play a critical role in shaping how Condé Nast enables its business using data. The team is responsible for building data pipelines, data products and tools that enable our Data Scientists, Analysts in various business units, Business Intelligence Engineers and Executives to solve challenging use cases in our industry.

We are seeking a Data Engineer who will build and maintain data pipelines across business areas such as consumer revenue, video, clickstream, commerce, social and ad revenue within Condé Nast. If you are looking for a challenging environment and to work with a world class team of data engineers in a well balanced environment and seasoned company, come join us:

Responsibilities

Responsibilities include, but are not limited to:
Design, build and test batch and streaming data pipelines capable of processing large volumes of data
Build efficient code to transform raw data into datasets for analysis, reporting and machine learning models
Collaborate with other data engineers to implement a shared technical vision
Participate in the entire software development lifecycle, from concept to release

Minimum Qualifications
Applicants should have a degree (B.S. or higher) in Computer Science or a related discipline or relevant professional experience
1+ years of software development experience designing scalable & automated software systems
Experience in processing structured and unstructured data into a form suitable for analysis and reporting
Experience building batch or real-time data pipelines
Proficiency in Python/PySpark or Scala
Proficiency in SQL
Experience with data processing frameworks such as Spark, Flink, or Beam
Experience in cloud-based infrastructures such as AWS or GCP
Exposure to orchestration platforms such as Airflow or Kubeflow
Proven attention to detail, critical thinking, and the ability to work independently within a cross-functional team

What happens next?

If you are interested in this opportunity, please apply below, and we will review your application as soon as possible. You can update your resume or upload a cover letter at any time by accessing your candidate profile.

Condé Nast is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, age, familial status and other legally protected characteristics."
27,Data Engineer,Jeenie,United States,"Posted by
Maria Pulcini
Marketing Communications Specialist at Jeenie
Send InMail
POSITION OVERVIEW

Job Title: Data Engineer
Location: Remote, with travel up to 10% (as required). Candidate must be located in the United States.
Position Details: Full-Time, Exempt
Compensation: Salary commensurate with experience

Jeenie® is seeking a forward-thinking, passionate Data Engineer to join its growing engineering organization. This role reports directly to the Vice President of Engineering. The ideal candidate has strong technical and analytical capabilities, along with extensive experience with cloud native computing technologies.

You will be an integral part of the Jeenie Engineering Team, collaborating across the company to drive impact and growth. This position will support multiple business units in developing and implementing high-quality, data-driven solutions to advance effective and efficient business decisions.

RESPONSIBILITIES

The Data Engineer will collaborate across multiple teams to:

Analyze, architect, and implement solutions and/or tools that help support data and/or process needs in a secure cloud environment.
Design data visualizations, technical documentation, and non-technical presentation materials to support decision-making and business performance.
Design, develop, test, implement, and support distributed data processing pipelines and related infrastructure.
Analyze end-user automation requirements of data sources, and data types, with ability to identify integration challenges and propose solutions.
Collaborate with management on organizing and integrating multiple data sources into readily available formats, while maintaining existing structures and governing their use according to business requirements.
Monitor the performance, scalability, and security of data pipelines and related infrastructure.
Other duties as assigned.

QUALIFICATIONS
3+ years work experience
Bachelor’s Degree or higher in Computer Science or related engineering preferred
Experience with data modeling, and modern ETL data pipeline tools, including Airbyte, DBT, Google BigQuery, and Google DataStudio
Advanced knowledge of multiple database systems, including PostgreSQL, BigQuery, Redis
Experience in writing SQL scripts, functions, and stored procedures
Experience with programming languages such as Javascript, Golang, Python, R
Strong knowledge of GCP products and services
Experience in SaaS and early-to-mid stage technology companies preferred

TECHNICAL/WORKPLACE REQUIREMENTS
Computer with high resolution web camera for conference calls as required
Neutral, quiet, and professional work environment with appropriate lighting
Fast and stable internet connection

SKILLS AND COMPETENCIES
Application development including SQL, Javascript, Golang
Experience with Google Cloud Platform
Proficient with distributed data/computing tools, such as Airbyte, and DBT, or equivalent
Experience with containerized cloud environments, particular Kubernetes and Docker
Experience with Agile engineering practices
Ability to productively discuss technical requirements with non-technical team members
Excellent written and verbal communication skills
Superior organizational/follow-up skills"
28,Data Engineer ( Hydrid position) New york city only,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, AQUA Information Systems, Inc., is seeking the following. Apply via Dice today!

Job Title Junior Data Engineer - Data Integration (3+ Years Exp) Locations Location is important this time as the client expects all team members to be onsite 40 of the time, so they must be local or live near to NY. Job Description 0-3 years of experience Experience on building deploying and maintaining complex Cloud Data Architectures Expertise in building Cloud Data Warehouses in Snowflake Redshift BigQuery or analogous architectures Deep SQL expertise data modeling and experience with data governance in relational databases Experience with the practical application of data warehousing concepts methodologies and frameworks using traditional (Vertica Teradata etc.) and current (SparkSQL Hadoop Kafka) distributed technologies Refined skills using one or more scripting languages (e.g. Python bash etc.) Experience using ETLELT tools and technologies such as Talend Informatica SSIS a plus Embrace data platform thinking design and develop data pipelines keeping security scale uptime and reliability"
29,"Data Engineer II, Core Data",Vimeo,"New York, NY","Vimeo is searching for an experienced Python and SQL software engineer for its Core Data Engineering team. This is a chance to create and maintain a robust, scalable, and balanced enterprise data platform with other members of the Core Data Engineering team. The ideal candidate will enjoy working on a variety of projects at once, with lots of different software services and data sources, all orchestrated with Python code.

Our platform has robust analytics tools that can tell video creators where people are from, where they click, and even track where in a video someone stopped watching—giving you insight into creating engaging content in the future.

While working as a peer to both technical and non-technical staff throughout the company, you will drive the improvement process of both our products and our business operations. You will help define data access and discoverability requirements and work to build infrastructure and services that provide access to event streams.

What you'll do:
Ensure consistency to coding best practices and development of reusable code
Monitor data platform and make recommendations to improve system architecture
Work closely with other data engineers, analysts, and business partners to understand and plan technical requirements for projects
Provide decision analysis and decision support to business partners using BI and other data
Learn quickly and deeply about Vimeo’s software platform, desktop, and mobile applications
Contribute software designs, code, tooling, testing, and operational support to a multi-terabyte BI data platform
Collaborative work: iterative development, design, and code review sessions
Independent work: author and maintain tools for other developers
Skills and knowledge you should possess:
3+ years of engineering experience in a fast-paced environment; 2+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud
Deep understanding of distributed data processing architecture and tools such as Kafka and Spark
Solid understanding of design patterns and coding best practices
Experience with and understanding of data modeling concepts, techniques and best practices
Experience with Airflow, Celery, or other Python-based task processing systems
Proficiency in SQL
Proficiency in Python
Proficiency with modern source control systems, especially Git
Experience working with non-technical business stakeholders on technical projects
Bonus Points (Nice Skills to Have, but Not Needed):
Cloud-based DevOps: AWS or Google Cloud Platform
Relational database design
Snowflake, or other distributed columnar-store databases
Basic Linux/Unix system administration skills
Vimeo (NASDAQ: VMEO) is the world’s leading all-in-one video software solution. Our platform enables any professional, team, and organization to unlock the power of video to create, collaborate and communicate. We proudly serve our growing community of over 260 million users — from creatives to entrepreneurs to the world’s largest companies.

Vimeo is headquartered in New York City with offices around the world. At Vimeo, we believe our impact is greatest when our workforce of passionate, dedicated people, represents our diverse and global community. We’re proud to be an equal opportunity employer where diversity, equity, and inclusion is championed in how we build our products, develop our leaders, and strengthen our culture.

Learn more at www.vimeo.com

Learn more at www.vimeo.com/jobs"
30,DATA ENGINEER,Dice,"Jersey City, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Fourth Technologies, Inc., is seeking the following. Apply via Dice today!

DATA ENGINEER- REMOTE Data Engineer with focus on SQL and Python. Data integration experience. Understanding of Change Data Capture (CDC), ETL Concepts Most of the work they do enables Data Analysis Pyspark is a nice to have."
31,Data Engineer III (Store No.8 | Conversational Commerce),Walmart,"New York, NY","About Store No8

Position Summary... What you'll do...

We are the incubation arm of Walmart. Store No8 was formed in 2017 as part of Walmart’s larger innovation mission to shape the future of commerce. We pursue big ideas and take risks by stepping outside of Walmart’s core business to focus on leapfrog capabilities across conversational commerce, mixed reality, in-store digitization, and more. Our ultimate goal: fuel Walmart’s core business, create new operational efficiencies, and unlock amazing experiences for our customers in the long-term.

Conversational Commerce

This team within Store No8 is reimagining how families shop and manage their households. They are hard at work designing new products that simplify the shopping experience and integrate naturally into a customer’s routine.

About The Role

Walmart Conversational Commerce is a new conversational commerce shopping service that is focused on empowering our time-sensitive busy Walmart families to express their needs in any way and any moment so that they can focus on the things that matter. We believe that we will create a differentiated customer experience for Walmart customers, and our goal is to deeply enhance the retail relationship and optimize it for the digital world. We are building a hard-working passionate team to join us as we have fun delighting customers, changing the retail landscape, and driving technical innovation.

As a member of the Conversational Commerce team, you will help us bring in the data and insights that allow us to champion for the customer at every step of their product journey. This role will report into the Senior Technical Business Analyst.

What You’ll Do
Write and deploy fully optimized and documented ETL/ELT pipelines, as needed by Business Analysts and Product Managers. You should be equally comfortable communicating with technical and non-technical staff in scoping and executing engineering tasks.
Provide solutions for deploying machine learning models into production. This could include feature set generation; model artifact storage; prediction serving, explain ability, and monitoring; and/or configuring cloud hardware.
Help develop and create a robust platform for automating the deployment of analytics and data science pipelines to be used by other members of the data engineering team, as well as non-technical users.
Qualifications
Bachelor’s degree in Engineering, Computer Science, Math or a related technical field or equivalent relevant experience and 2+ years experience in a data engineering, ML engineering, or technical data analytics role
2+ years experience creating production-ready data pipelines in Python, or equivalent
Experience with SQL and ETL optimization techniques, especially within cloud-based data warehouses like Snowflake, BigQuery, and/or Redshift
Command line experience and familiarity with version control software (Git)
Experience with data pipeline management technologies with dependency checking, such as Airflow, as well as schema design and dimensional data modeling
Ability to leverage tools, business intuition, and attention to detail for data validation and QA
Excellent communication skills, particularly when explaining technical or complex matters to non-technical leaders and teammates
Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

Option 1: Bachelor’s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years’ experience in

software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, Master’s degree in Computer Science or related field and 2 years' experience in software engineering or related field Primary Location...250 Hudson St, NEW YORK, NY 10013-1006, United States of America"
32,"Data Engineer (AWS, SQL)",TransUnion,"White Plains, NY","TransUnion's Job Applicant Privacy Notice

What We'll Bring

At TransUnion, we strive to build an environment where our associates are in the driver’s seat of their professional development, while having access to help along the way. We encourage everyone to pursue passions and take ownership of their careers. With the support of colleagues and mentors, our associates are given the tools needed to get where they want to go. Regardless of job titles, our associates have the opportunity to learn new things and be a leader every day.

Come be a part of our team – you’ll work with great people, pioneering products and cutting-edge technology.

What You'll Bring
May/ June 2022 grad with an expected Bachelor’s or Masters degree in engineering, business, or an analytical field such as economics, statistics, etc.
Familiarity with SQL and AWS or other programing/data mining language (e.g. Python, VBA) is desired
A track record of outstanding academic performance
Excellent verbal, written and presentation skills
Ability to work in a team setting and independently
Strong analytical, communication, creative and interpersonal skills
Demonstrated problem solving and root cause analysis skills
Ability to work creatively and analytically in a problem-solving environment
Eagerness to contribute in a team-oriented environment
Must have passion for technology and automation
Impact You'll Make

We are seeking an Data Engineer to join Verisk Financial Services Development Operations Technology and Security (DOTS) organization to support our production platform on Amazon Web Services (AWS). This is an opportunity for May/ June 2022 graduates to begin their career. Candidates will apply business analysis to create advanced business intelligence solutions providing insight to clients. This role requires the candidate to use analytic tools and database skills to develop and evaluate a broad spectrum of analytics for large data sets. Your responsibilities may include loading client data, validating client data, processing client data and producing Business Intelligence reporting on client data. You will work with leading Database Technologies and Data Warehousing toolkits to master the complex nuances in financial data received at Argus. Some of the technologies that you will use are AWS, SQL Server, SQL server Reporting Services (SSRS), SQL Server Integration Services (SSIS), Visual Basic .Net , ASP, .NET, Python etc. Responsibilities:
Load, transform, validate, and report on client data
Analyze data to systematically apply in-house developed, complex analysis technology to test the data quality
Interface with team to assist with complex ad-hoc analysis on data; and analyze data to respond to client inquiries
Support Onboarding of new Financial Institution and product portfolio to the study
Develop data driven exploratory analysis to solve defined problems
Take a proactive role in solving complex technical challenges in the Data Warehousing operations
Document technical specification to manipulate data; execute and deliver analytic components of projects; produce graphs reports and summaries
Work on projects to enhance and improve the in-house technology suite for validating and reporting on new data
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.

During the COVID-19 pandemic, TransUnion has several safety protocols in place to protect associates, customers, and visitors. You may be required to be fully vaccinated against COVID-19 as a condition of employment and/or to participate in certain work-related activities. Exemption is available to qualified candidates as a reasonable accommodation.

TransUnion's Internal Job Title

Acquisition Exempt, Acquired Associate"
33,"Data Engineer (AWS, SQL)",TransUnion,"White Plains, NY","TransUnion's Job Applicant Privacy Notice

What We'll Bring

At TransUnion, we strive to build an environment where our associates are in the driver’s seat of their professional development, while having access to help along the way. We encourage everyone to pursue passions and take ownership of their careers. With the support of colleagues and mentors, our associates are given the tools needed to get where they want to go. Regardless of job titles, our associates have the opportunity to learn new things and be a leader every day.

Come be a part of our team – you’ll work with great people, pioneering products and cutting-edge technology.

What You'll Bring
May/ June 2022 grad with an expected Bachelor’s or Masters degree in engineering, business, or an analytical field such as economics, statistics, etc.
Familiarity with SQL and AWS or other programing/data mining language (e.g. Python, VBA) is desired
A track record of outstanding academic performance
Excellent verbal, written and presentation skills
Ability to work in a team setting and independently
Strong analytical, communication, creative and interpersonal skills
Demonstrated problem solving and root cause analysis skills
Ability to work creatively and analytically in a problem-solving environment
Eagerness to contribute in a team-oriented environment
Must have passion for technology and automation
Impact You'll Make

We are seeking an Data Engineer to join Verisk Financial Services Development Operations Technology and Security (DOTS) organization to support our production platform on Amazon Web Services (AWS). This is an opportunity for May/ June 2022 graduates to begin their career. Candidates will apply business analysis to create advanced business intelligence solutions providing insight to clients. This role requires the candidate to use analytic tools and database skills to develop and evaluate a broad spectrum of analytics for large data sets. Your responsibilities may include loading client data, validating client data, processing client data and producing Business Intelligence reporting on client data. You will work with leading Database Technologies and Data Warehousing toolkits to master the complex nuances in financial data received at Argus. Some of the technologies that you will use are AWS, SQL Server, SQL server Reporting Services (SSRS), SQL Server Integration Services (SSIS), Visual Basic .Net , ASP, .NET, Python etc. Responsibilities:
Load, transform, validate, and report on client data
Analyze data to systematically apply in-house developed, complex analysis technology to test the data quality
Interface with team to assist with complex ad-hoc analysis on data; and analyze data to respond to client inquiries
Support Onboarding of new Financial Institution and product portfolio to the study
Develop data driven exploratory analysis to solve defined problems
Take a proactive role in solving complex technical challenges in the Data Warehousing operations
Document technical specification to manipulate data; execute and deliver analytic components of projects; produce graphs reports and summaries
Work on projects to enhance and improve the in-house technology suite for validating and reporting on new data
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.

During the COVID-19 pandemic, TransUnion has several safety protocols in place to protect associates, customers, and visitors. You may be required to be fully vaccinated against COVID-19 as a condition of employment and/or to participate in certain work-related activities. Exemption is available to qualified candidates as a reasonable accommodation.

TransUnion's Internal Job Title

Acquisition Exempt, Acquired Associate"
34,Data Engineer,Robert Half,"Radnor, PA","Description

Are you a Data Engineer looking for an exciting opportunity in the Philadelphia area? Robert half is looking for a talented Engineer to join a team of financial software specialists responsible for the acquisition and management of financial market and related data required for research implementation of systematic trading strategies.

Primary Responsibilities
Implement robust solutions for adoption, storage and management of large volumes of data
Develop quality assurance systems in support of high integrity data sets
Assist quantitative analysts in crafting custom, bespoke data sets
Support timely and fault tolerant data systems in support of production trading algorithms
Meet tight deadlines in an efficient manner
Requirements Of The Candidate Include
Bachelor’s degree in Computer Science or applicable degree and very strong exposure to programming and computer systems
Experience with Python and C++
Experience with data storage and manipulation using approaches such as HDF5, Pandas, and RDBMS/SQL
2+ years professional experience in data management role
The desire to work in a fast-paced hardworking and committed environment with a talented team
Strong problem solving skills, critical thinking and clear written and verbal communications
Financial industry, market data and cloud data environment experience are beneficial
Requirements

Data Management, Data Warehousing, Quality Assurance Quality Control, Python, C++, University Degree, HDFS, Manipulate Data, RDBMS, SQL, Pandas, Critical Thinking, Communication Skills, Cloud Storage, Market Data, Quantitative Analysis, Confidentiality, Finance

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
35,"Senior Data Engineer II, Experimentation Platform",Etsy,"Brooklyn, NY","Company Description

Etsy is the global marketplace for unique and creative goods. We build, power, and evolve the tools and technologies that connect entrepreneurs with buyers around the world. Etsy, Inc.'s 'house of brands' portfolio has expanded to now include four individually distinct ecommerce brands -- Etsy, fashion resale marketplace Depop, musical instrument marketplace Reverb, and Brazil-based handmade goods marketplace Elo7. As an Etsy employee, you'll tackle unique problems alongside talented coworkers committed to Keeping Commerce Human. We're large enough that you'll focus on meaningful, complex challenges, but small enough that you can make a rewarding impact.

Job Description

What’s the role?

We are looking for a Senior Data Engineer to join the Core Experimentation Infrastructure squad within the Experimentation Platform and Science team to build and maintain a world class experiment infrastructure for Etsy’s internal experimentation platform. In doing so, you will enable and empower users to dig deeper through richer, more accessible data available in the platform! The platform is critical for the company’s product development success, and teams across Etsy use the platform to learn, make product decisions, and measure the impact of new features.

This is a full-time position working with Core Experimentation Infrastructure’s Engineering Manager. For this role, we are considering candidates based in the United States who are either remote, flex, or office-based. Etsy offers different work modes to meet the variety of needs and preferences of our team. Learn more about our flexible work options and vaccination policy here.

What’s this team like at Etsy?
On Core Experimentation Infrastructure, we work to accelerate Etsy’s product roadmap execution and to provide richer, more trustworthy, and accessible data.
We work closely with our partner teams under Experimentation Platform and Science as well as engineers and data scientists across various product and enablement teams
What does the day-to-day look like?
Implement improvements for Etsy’s internal experimentation platform related to metric calculations and definitions, audience segmentation, and data aggregation.
Scope and own work to incorporate analytical and statistical frameworks into an existing architecture from ideation through completion.
Optimize complex workflows, taking into account speed, reliability, monitoring, and dependency management.
Collaborate with team members to dig into issues with experiment data collection.
Act as the last line of defense for system failures owned by the team.
Mentor junior and senior engineers on the team.
Collaborate with external partners and customers throughout the company to develop innovative solutions to sophisticated business problems.
Of course, this is just a sample of the kinds of work this role will require! You should assume that your role will encompass other tasks, too, and that your job duties and responsibilities may change from time to time at Etsy's discretion, or otherwise applicable with local law.
Qualifications

Qualities that will help you thrive in this role are:
You have a passion for building data related products and tooling and understand good systems design.
You have experience working on large-scale data problems, ideally using SQL/BigQuery and Spark.
You have experience working on complex workflows, ideally using Airflow.
You gravitate towards hard problems. You are proactive about trying different approaches and use data to drive decision-making.
You write understandable, testable code by default with an eye toward maintainability.
You have excellent written and verbal communication skills.
You are excited about mentoring engineers.
You have experience with experimentation and/or analytics, either through building and supporting a platform or running experiments.
You keep learning and always seek to improve yourself. You are curious about new opportunities and act to explore them. Namely, you are familiar with the trending technology in data and software development.
Additional Information

What's Next

If you're interested in joining the team at Etsy, please send a cover letter and resume telling us why you'd be right for the position. As you've hopefully seen already, Etsy is a place that values individuality and variety. We don't want you to be like everyone else -- we want you to be like you! So write to us and tell us what you're all about.

Our Promise

At Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We encourage people from all backgrounds, ages, abilities, and experiences to apply. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment. While Etsy supports visa sponsorship, sponsorship opportunities may be limited to certain roles and skillsets.

For U.S. Roles Only

Many Etsy roles are open to remote candidates, and you'll be able to identify which ones within the location header of each job description. We're open to remote hires from all U.S. states except Hawaii and Alaska. For candidates who will work remotely from Colorado, visit this link for information related to Colorado's Equal Pay for Equal Work Act."
36,Data Engineer: Application Modernization,IBM,"Southbury, CT","529272BR

Introduction

Engineers at IBM are the backbone of our strategic initiatives to design, code, test, and provide industry-leading solutions that make the world run today - planes and trains take off on time, bank transactions complete in the blink of an eye and the world remains safe because of the work our software developers do. Whether you are working on projects internally or for a client, software development is critical to the success of IBM and our clients worldwide. At IBM, you will use the latest software development tools, techniques and approaches and work with leading minds in the industry to build solutions you can be proud of.

Your Role and Responsibilities

Application modernization is a new initiative in IBM, which is transforming the way IBM works. We are creating a better developer experience for all developers within IBM. Data Engineer’s primary responsibility is to drive the transformation of IBM applications to modern database architecture, to be deployed on Hybrid Cloud. We cultivate an open, healthy, diverse, and engaging work environment where team members are continuously gaining new skills which align with individual interests.

Responsibilities May Include

Ability to analyze data and existing database design with the goal of enacting improvements geared towards making processes more efficient.
Evaluating and selecting appropriate data modernization tools to enhance our teams’ productivity and user experience.
Participate in defining CIO’s data architecture framework, standards, and principles including data management, security, and infrastructure.
Leads multi-functional teams or conducts special projects.

You will be challenged to design and develop scalable systems, leveraging a wide variety of open source and commercial technologies, closely collaborating with IBM Research and Red Hat.

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

IBM will not be providing visa sponsorship for these positions now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Required Technical and Professional Expertise

Education

A combination of education, technical training, and/or work/military experience equivalent to a bachelor’s degree in computer science, IT, etc.

General

Excellent time management and interpersonal skills
Familiarity with Agile methodologies and principles, experience in an Agile team
Understanding of DevOps practices (Continuous Integration/Delivery/Monitoring/etc.)
Willingness to work with the entire stack (from the database to containers to monitoring and alerting)
Ability to write and review code and make decisions regarding technical implementations
Growth mindset

Technical Skills

5+ years’ experience Applied Knowledge of Software Development
3+ years’ experience in DB2 or other relational databases or NoSQL databases
Knowledge of ETL practices and methodologies
Application SQL development and SQL Performance tuning, where applicable

Preferred Technical And Professional Expertise

Previous experience as a Backend or Full Stack Developer
Automated testing frameworks
Experience in working with containers and container management systems including Docker, Kubernetes and/or OpenShift
Cloud computing knowledge

About Business Unit

The Office of the Chief Information Officer (CIO) owns IBM’s IT strategy and provides the tools, workstations, devices, and infrastructure that IBMers use to do their jobs every day. Put simply, our mission is to create a productive environment for IBM's 350,000 worldwide employees. Join us as we lead with design to drive simplicity and ease of use, engineering the systems that run the business, and innovating to transform the business.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
37,"Data Engineer, Snowflake",Deloitte,"Darien, CT","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
38,"Data Engineer, 1+ Years of Experience",Snap Inc.,"Los Angeles, CA","Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Snap Engineering teams build fun and technically sophisticated products that reach hundreds of millions of Snapchatters around the world, every day. We’re deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. We move fast, with precision, and always execute with privacy at the forefront.

Some core features we build and maintain include Snapchat’s Camera, Creative Tools, Maps, Chat, Memories, Stories, Discover, Games, and Minis. Our Infrastructure teams deliver an innovative and cost-efficient platform that ensures Snapchat is the fastest way to communicate with your friends, no matter where you are in the world. We have one of the fastest growing digital ad platforms, and our Monetization teams drive measurable returns for advertisers through novel ad formats like Augmented Reality. As a Snap Engineering team member, you’ll help us build the future of communication.

We’re looking for a Data Engineer to join Snap Inc!

What You’ll Do
Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner
Develop data pipelines adhering with privacy and governance principles
Become familiar with our data consumption portals and their capabilities
Build expertise and ownership of data quality for supported domains
Establish and implement data quality standards and controls
Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate
Drive adoption of the data sets you’ve produced
Knowledge, Skills & Abilities
Experience in building data pipelines to serve reporting needs
Experience owning all or part of a team roadmap
Ability to prioritize requests from multiple stakeholders in disparate domains
Ability to effectively communicate complex projects to non-technical stakeholders
Minimum Qualifications
BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field
1+ years experience in SQL or similar languages
1+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)
Experience in ETL / Data application development
Preferred Qualifications
Hands on experience with Google BigQuery
Experience in version control systems such as Git
Data architecture and warehousing experience
Experience leading a small team of data or software engineers
Experience with Airflow
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .

Our Benefits : Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid maternity & paternity leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!"
39,Senior Data Engineer,JPMorgan Chase & Co.,"Jersey City, NJ","As an experienced member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Expertise in application, data, and infrastructure architecture disciplines
Advanced knowledge of architecture and design across all systems
Proficiency in multiple modern programming languages
Knowledge of industry-wide technology trends and best practices
Keen understanding of financial control and budget management
Ability to work in large, collaborative teams to achieve organizational goals
Passionate about building an innovative culture
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
40,Data Engineer- Security Analytics & Reporting,IBM,"Raleigh, NC","526316BR

Introduction

As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities

Data Engineers work with Data Analysts and Data Scientists to improve the quality and accuracy of the information, enabling businesses to make more responsible and informed decisions.

We are looking for an experienced data engineer to join our team:

You will use various methods to transform raw data into useful data systems.
You will create data models to facilitate statistical analysis and optimal data retrieval.
You will strive for efficiency by aligning data systems with business goals.
You will have strong analytical skills and the ability to combine data from different sources creating effective data models that will provide accurate business answers.

Above all, we are looking for applicants who will thrive in an open, vibrant, flexible, fun-spirited, collaborative environment and desire creative freedom and an opportunity to work on high performing teams. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition communicating with internal customers
Work in an agile, collaborative environment
Collaborate with data analysts, data scientists and architects on several projects
Build algorithms and prototypes

About The Team

The SOS Security Analytics & Reporting Tribe

SYSTEMSPROUD

Required Technical and Professional Expertise

4+ years of experience developing and implementing data models
2+ years of experience writing and analysing complex SQL
1+ years Cognos (or similar) experience
1+ years of experience in data analytics, statistics, or similar field.
Exceptional technical knowledge in data modeling
Excellent technical understanding of databases and data storage systems
Excellent problem-solving skills
Excellent communication and collaboration skills

Preferred Technical And Professional Expertise

Understanding of security and compliance
Experience with dashboards and reports design
Experience with Python, R, Java or similar

About Business Unit

IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world’s most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfolio designed for cognitive business and optimized for cloud computing.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
41,Data Engineer- Security Analytics & Reporting,IBM,"Cambridge, MA","526316BR

Introduction

As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities

Data Engineers work with Data Analysts and Data Scientists to improve the quality and accuracy of the information, enabling businesses to make more responsible and informed decisions.

We are looking for an experienced data engineer to join our team:

You will use various methods to transform raw data into useful data systems.
You will create data models to facilitate statistical analysis and optimal data retrieval.
You will strive for efficiency by aligning data systems with business goals.
You will have strong analytical skills and the ability to combine data from different sources creating effective data models that will provide accurate business answers.

Above all, we are looking for applicants who will thrive in an open, vibrant, flexible, fun-spirited, collaborative environment and desire creative freedom and an opportunity to work on high performing teams. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties

Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition communicating with internal customers
Work in an agile, collaborative environment
Collaborate with data analysts, data scientists and architects on several projects
Build algorithms and prototypes

About The Team

The SOS Security Analytics & Reporting Tribe

SYSTEMSPROUD

Required Technical and Professional Expertise

4+ years of experience developing and implementing data models
2+ years of experience writing and analysing complex SQL
1+ years Cognos (or similar) experience
1+ years of experience in data analytics, statistics, or similar field.
Exceptional technical knowledge in data modeling
Excellent technical understanding of databases and data storage systems
Excellent problem-solving skills
Excellent communication and collaboration skills

Preferred Technical And Professional Expertise

Understanding of security and compliance
Experience with dashboards and reports design
Experience with Python, R, Java or similar

About Business Unit

IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world’s most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfolio designed for cognitive business and optimized for cloud computing.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
42,data engineer bo,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Nigel Frank International, is seeking the following. Apply via Dice today!

Job Description This is a full time hybrid position. The client is a leading provider of capital solutions to the private market ecosystem. Our flexible, consultative approach helps position us as the partner of choice for businesses seeking capital and liquidity solutions to support their sustained growth. With over 90 billion in assets under management, we seek to provide Institutional and Private Wealth clients access to differentiated, income generating investment opportunities with a focus on risk mitigation and downside management. Our platform unites established alternative asset managers, each with distinctive investment capabilities and proven track records of success, under one roof with the singular ambition of redefining private markets We are looking for a motivated Data Engineer to join our expanding team. The ideal candidate will have experince using Power BI and Excel. Role Responsibilities Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build analytics tools that utilize the data pipeline to provide actionable insights into client acquisition, operational efficiency and other key business performance metrics. Work with stakeholders including the Operations and Sales Management teams to assist with data-related technical issues and support their data infrastructure needs. Build automated reports and dashboards with the help of Power BI and other reporting tools. Understand business requirements to set functional specifications for reporting applications. Work with data and analytics experts to strive for greater functionality in our data Skills Qualifications Ability to communicate with business as well as technical teams. Comfortable using Power BI. Strong client management skills. Applying experience and knowledge to future solution considerations. Have an analytical and problem-solving mindset and approach. Documenting, designing, and modeling solutions and explaining, representing, and discussing the same with the team. Benefits Health, Vision, 401k, Dental, Life"
43,Data Engineer: Application Modernization,IBM,"Austin, TX","529272BR

Introduction

Engineers at IBM are the backbone of our strategic initiatives to design, code, test, and provide industry-leading solutions that make the world run today - planes and trains take off on time, bank transactions complete in the blink of an eye and the world remains safe because of the work our software developers do. Whether you are working on projects internally or for a client, software development is critical to the success of IBM and our clients worldwide. At IBM, you will use the latest software development tools, techniques and approaches and work with leading minds in the industry to build solutions you can be proud of.

Your Role and Responsibilities

Application modernization is a new initiative in IBM, which is transforming the way IBM works. We are creating a better developer experience for all developers within IBM. Data Engineer’s primary responsibility is to drive the transformation of IBM applications to modern database architecture, to be deployed on Hybrid Cloud. We cultivate an open, healthy, diverse, and engaging work environment where team members are continuously gaining new skills which align with individual interests.

Responsibilities May Include

Ability to analyze data and existing database design with the goal of enacting improvements geared towards making processes more efficient.
Evaluating and selecting appropriate data modernization tools to enhance our teams’ productivity and user experience.
Participate in defining CIO’s data architecture framework, standards, and principles including data management, security, and infrastructure.
Leads multi-functional teams or conducts special projects.

You will be challenged to design and develop scalable systems, leveraging a wide variety of open source and commercial technologies, closely collaborating with IBM Research and Red Hat.

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

IBM will not be providing visa sponsorship for these positions now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Required Technical and Professional Expertise

Education

A combination of education, technical training, and/or work/military experience equivalent to a bachelor’s degree in computer science, IT, etc.

General

Excellent time management and interpersonal skills
Familiarity with Agile methodologies and principles, experience in an Agile team
Understanding of DevOps practices (Continuous Integration/Delivery/Monitoring/etc.)
Willingness to work with the entire stack (from the database to containers to monitoring and alerting)
Ability to write and review code and make decisions regarding technical implementations
Growth mindset

Technical Skills

5+ years’ experience Applied Knowledge of Software Development
3+ years’ experience in DB2 or other relational databases or NoSQL databases
Knowledge of ETL practices and methodologies
Application SQL development and SQL Performance tuning, where applicable

Preferred Technical And Professional Expertise

Previous experience as a Backend or Full Stack Developer
Automated testing frameworks
Experience in working with containers and container management systems including Docker, Kubernetes and/or OpenShift
Cloud computing knowledge

About Business Unit

The Office of the Chief Information Officer (CIO) owns IBM’s IT strategy and provides the tools, workstations, devices, and infrastructure that IBMers use to do their jobs every day. Put simply, our mission is to create a productive environment for IBM's 350,000 worldwide employees. Join us as we lead with design to drive simplicity and ease of use, engineering the systems that run the business, and innovating to transform the business.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
44,Data Engineer,Credit Suisse,"Raleigh, NC","Your field of responsibility

Team member within Business Reporting & data driven RCA (Root Cause Analysis) team under the Performance & Control Function. This role features heavy involvement with the use of data lakes and streams, working closely with partners in technology and the business to test and challenge current and proposed solutions. This person will identify issues within data sets and form a picture around the process that caused the problem system to system; Find opportunities for improvement supported by strong data analysis and visualization. This candidate will work with key partners across the business to trace issues end-to-end to implement a remediation strategy. This person should develop a strong network of working relationships outside of the immediate team and across the wider business. Be responsible to provide their manager with regular updates on progress and demonstrate how that has been achieved.

Your future colleagues

A versatile team with a number of different skills. The team is ambitious and strive to make a difference within Global Operations and the Firm overall. Phenomenal atmosphere within the team and excellent ethos We are a department which values Diversity and Inclusion (D&I) and is committed to realizing the firm’s D&I ambition which is an integral part of our global cultural values

Your skills and experience

Skill and experience required for Data Engineer role within the Root Cause Analytics Team.

We are looking for applicants with a relevant degree, Science, Engineering, Math or Computer Science along with an equivalent experience using data lakes and understanding of big data principles (Cloudera).
Excellent knowledge of SQL and database management
Very good standard of data visualization (Tableau)
Good proficiency in Python (Pandas, Plotly, Matplotlib)
Good knowledge of Linux systems and experience with shell automation (cron)
Experience data flows and ETL (batch, incremental loads)
Ability to communicate and partner management
Keen attention to detail and curiosity about root cause analysis, curiously minded with the ability to ask probing questions.
Keen to learn new technologies and data methodologies
Knowledge of investment banking (settlements) and financial products
Understanding of data standardization (data fabric)
Preferred
Alteryx
Awareness of predictive analytics and machine learning
PySpark (MapReduce principles)
Investment Banking experience in the Trade Lifecycle is advantageous, but not essential
Dedication to fostering an inclusive culture and value varied perspectives.
United States-NC-Raleigh"
45,AWS Data Engineer,Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
46,Senior Data Engineer,Grindr,"Brooklyn, NY","We’re looking to add a Senior Data Engineer to our Data Platform team. We’re in the process of modernizing our Data Platform with the goal of making data a strategic asset to all aspects of Grindr’s business. We recently finished a lift-and-shift of our PB-scale Data Lake to Snowflake and we’re beginning a next phase of redesigning our pipelines and data models. Goals broadly include near-real time product analytics, ML feature engineering, automated data quality alerts, a common event taxonomy and AB testing. Along the way we’re eliminating redundant code and streamlining processes.

Data Engineering has visibility and exposure to a broad set of crucial business functions at Grindr. The Data Platform team supports Data Engineering for all of Grindr by collaborating with product, engineering, and embedded analytics teams while collaborating with a core analytics team on the Data Platform.

The Data Platform is composed of 3 pods generally: Data Engineering and Core Product Analytics, Data Applications, and Data Science / ML Ops. Data Engineers coordinate across these pods. For example, help us upgrade our event firehose by migrating our 100K events-per-second firehose to a Kafka-based system. Build deep expertise in Airflow and Snowflake tasks as you write SQL that mutates trillions of records. Help develop and curate a new Data Catalog.

What’s the job?
Design, develop and deliver data pipelines and datasets to production, considering internal data governance, security and scale
Develop near real-time data pipelines using change-data-capture and event-based processing.
Maintain and enforce the business contracts on how data should be represented and stored.
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What We’ll Love About You
Significant experience with dimensional data modeling for a Data Lake / Warehouse
Expert in ETL and ELT (managing high-quality reliable ETL pipelines)
Experience with relational databases and expert in SQL development
5+ years of experience working with data at scale or complex data analysis
7+ years experience with Python or equivalent
Experience with agile engineering practices such as TDD, Continuous Integration, automated testing, and deployment.
Familiarity with legal compliance (with data management tools) data classification, and retention.

We’ll really swoon if you have
Experience as a Data Engineer using Snowflake or Data Bricks
Previous business experience working with customers around data-centric processes and analysis
Experience with complex business change management and process implementations

What You’ll Love About Us
Global impact. Grindr is the world leading LGTBQ+ social networking service. Your role will impact the lives of millions of LGTBQ+ people around the world.
High Growth. Like the company itself, this role offers significant room for growth and development
Remote First: We have offices in LA, NYC, SF, Chicago, Toronto and Taipei, though the company is set-up not just as remote-friendly, but remote-first. More than 30% of our employees work outside the cities where we have offices. Today, we are set-up to hire easily in the US, Canada and Taiwan. If you live outside one of these countries, just let us know so we can make sure we can support you.
Comp: Industry competitive compensation, including equity for all employees
Benefits: Unlimited vacation policy
Family insurance: We provide great insurance coverage for health, dental and vision for you and your family.
Retirement Savings: Generous 401(k) plan
Other Perks: home-office stipend, many company-sponsored events

About Grindr

Since launching in 2009, Grindr has grown into the world’s largest social networking app for gay, bi, trans, and queer people. We have millions of daily users who use our location-based technology in almost every country in every corner of the planet.

Today, Grindr proudly represents a modern LGBTQ+ lifestyle that expands into new platforms. From social issues to original content, we continue to blaze innovative paths with a meaningful impact for our community. At Grindr, we create a safe space where you can discover, navigate, and get zero feet away from the queer world around you.

As of June 2020, Grindr has new owners with a track record of multiple successful Bay Area start-ups. The new leadership is demonstrating a renewed commitment to creating an experience for users that is safe, fun, and productive, as well as a positive & uplifting company culture in which everyone can be their best selves. At the heart of Grindr’s mission in this new chapter is a shared set of core values including transparency, accountability, experimentation (failing fast), and strong allegiance to the LGBTQ+ community.

Grindr is an equal opportunity employer"
47,"Data Engineer, Integrations & Scraping",D1 Brands,United States,"A Little Bit About D1

D1 Brands is a global operator and acquirer of 3rd-party consumer products brands. Starting from a small group of Amazon-native sellers, we’ve built D1 into a global consumer brands platform with a vision to define the future of 21st century retail. Since our founding in late 2020, we’ve expanded the D1 Brands family into more than 120 talented professionals located around the globe; we’ve raised more capital in our first year of business than most start-ups raise in a lifetime; and we’ve partnered with dozens of exceptional entrepreneurs to help protect and grow the brands they’ve built. But, as our namesake indicates, we believe it is always “Day 1”, so if you’re interested in helping us build the future of digital commerce, we’d love to meet you.

Here Are a Few

The D1 team comes from a wide variety of professional backgrounds and countries, but we all share a few core beliefs.
We are fanatical about creating value for our brand sellers, customers, and D1 team

members
We’d rather write the rules than follow someone else’s – there is always a better way

to do things
When it comes to building a world-class business, we shouldn’t compromise between

hard work and enjoying the journey – if we’re not having fun, we’re failing

The Role

You will build systems to integrate data from our 3rd party data vendors into our data warehouse. We run our services on AWS and use Snowflake as a data warehouse. We develop software using Git and GitHub. We use modern software technologies for deployment and infrastructure including Docker, ECS, and Terraform.

Key Responsibilities & Duties
Intermediate knowledge of a programming language like Python or Go.
Comfortable learning new APIs and understanding their data models, semantics, and limitations.
Familiar with basic AWS services like S3 and SQS, which you may need to interact with programmatically.
Capable of operating in a fully remote environment. Self aware enough to know when you are blocked and need help.
Comfortable asking for help, clarifying ambiguous requirements, and questioning requirements which greatly increase complexity or cost.
Strong written communication skills. Capable of reading, writing technical documents. And participating in a primarily asynchronous, written design process.
Understands data processing techniques including batching, streaming, event sourcing, snapshotting, idempotency, and deduplication.
Familiar with software development tools and practices. Git, GitHub, feature branching, pull requests, code review, testing, and continuous integration.

Preferred Background & Capabilities
Minimum experience of 1-3 years in related field.
Must be a strong programmer, familiarity with Python and/or Go.
Experience building data integrations and/or web scraping.
Familiarity with Docker is a bonus, but no DevOps experience is required.
Able to quickly understand the semantics and data models of an API

What else does D1 have to offer?

Do you offer any benefits to your employees?

We do offer medical, vision, dental, short term disability and life insurance benefits for U.S. employees. We offer medical stipends for our international staff. We also offer unlimited PTO, a generous parental leave, and remote work options.

Some Other Fun Perks
No-meeting Thursday's
Monthly company socials to get to know your fellow D1ers
An in-house resource library
Opportunities for professional development

What if I don't see a job description that fits my background?

If this position isn't a fit for your background, but you are still interested in D1 Brands please consider dropping your resume to our General Consideration posting. Our company is growing fast and our Talent Acquisition Team does review all resumes.

If you are at least a little intrigued at the potential of working at D1, drop your resume!

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law."
48,Junior Data Engineer,IBM,"Baton Rouge, LA","490551BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

The position of the Data Engineer plays a key role in the development and deployment of innovative big data platforms for advanced analytics and data processing. The Data Engineer defines and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Develops Big Data and Cognitive technologies including API development. Expected to have experience with ETL tools & Data warehouses. Strong technical abilities to understand, design, write and debug complex code.

Required Technical and Professional Expertise

Minimum 2 years of hands-on coding experience in Java
Minimum 1 years of hands-on coding experience in SCALA
Minimum 1 years of hands-on experience working with Kafka
Minimum 1 years of experience in Big Data technologies (Hadoop, Spark)
Minimum 1 years of hands-on experience with Spring Boot or Spring Cloud
Minimum 1 years of experience using SQL and good RDBMS conceptual knowledge
Minimum 1 years of experience with ETL tools

Experience with ADO or Jenkins for CI/CD

Preferred Technical And Professional Expertise

Experience in Kubernetes and Docker
Experience coding in Python
Big Data Certifications on Cloudera/Hortonworks/AWS/GCP/Azure
Certification on Snowflake Pro

Experience in Azure, AWS, GWP, IBM cloud etc.

About Business Unit

IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
49,Data Engineer - FinTech,Amazon,"Jersey City, NJ","Description

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build big-data solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access? If so, Amazon Finance Technology (FinTech) is for you!

Amazon FinTech is an organization where people, technology and innovation come together to build products and solve problems for Amazon. The technology solutions and services we build enable Amazon’s new business growth, provide operational efficiency through automation, compliance with law and analysis of our financial data. Through our products, we aim to provide Amazon competitive advantage for running its business and insights for our customers using state of the art technologies.

As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The candidate is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
3+ years experience with Java, Scala, or Python
Experience with Redshift, Oracle, etc.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience in working and delivering end-to-end projects
Knowledge of distributed systems as it pertains to data storage and computing
Ability to work on a diverse team
Preferred Qualifications
3+ years experience with AWS services including S3, Redshift, EMR and RDS.
5+ Years experience with SQL or other query languages
Ability to distill ambiguous customer requirements into a technical design
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.
Experience providing technical leadership and educating other engineers for best practices on data engineering.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1754654"
50,Data Engineer (Remote),Signify Health,"Bridgeport, CT","What We Do

At Signify Health, we provide care for those who need it most. We understand the benefits of a personalized approach to clinical encounters and the importance of conversation, understanding, and sitting knee-to-knee with the patient in the best setting for care delivery: the home.

A Software Engineer focusing on Data Engineering develops systems to manage data flow throughout Signify Health’s infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data.

What will you do?
Develop readable, well-tested applications, APIs, and libraries
Work with internal and external APIs for applications and data sources
Utilize cloud infrastructure and collaborate with SREs to build scalable systems
Implement application observability in the form of metrics, logging, and monitoring
Work with engineers across the organization
Collaborate closely with team members and product stakeholders
Requirements
4+ years of relevant software engineering work experience
Prior work with cloud-based systems
Expansive knowledge of RESTful API design and use
Familiarity with observability concepts and tooling
Meaningful experience with relational databases and SQL
Nice-to-have
Experience with Go or Python
Experience with Amazon AWS services
Experience with non-relational databases
Bachelor's degree in Computer Science or a related field
About Us

Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals’ clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we’re able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved."
51,Data Engineer,The Brandtech Group,"New York, NY","Posted by
Paulette L. Forte
Chief People Officer at The Brandtech Group
Send InMail
The Brandtech Group is building a platform to integrate workflows and data across our content creation and media activation partners. With clients including some of the biggest brands on the planet (Unilever, Amazon, Google, Adidas, Marriott, Microsoft, many more) and a roadmap spanning serverless ETL, AI/ML, NFT creation and tracking, smart contracts and metaverse asset management this is a dream job for the right candidate. We have the size, greenfield and upside of a startup while being backed by one of the biggest brand service companies on earth.
Come build cool stuff with us.
More About the Opportunity
We're looking for an experienced data engineer to help build out the current offering into a suite of enterprise products with a focus on asset tagging and performance as well as data collection, transformation, analysis and visualization. As the team grows you will provide technical leadership, guidance, and mentoring on design, code and strong engineering practices. You must be comfortable working in diverse development ecosystems and have worked independently in a rapidly scaling startup. You’ll also be working on a modern stack: Python, Data Fusion, Dataflow, AutoML, Docker/Kubernetes, BigQuery, Postgres, Jupyter, Polygon/OpenZeppelin and GCP/AWS.
What you'll do
● Build the Brandtech Cloud: this is a greenfield effort and a unique opportunity to participate in building a new product from the ground-up
● Work with ML: pull data and media assets from all Brandtech Group partner companies and apply AI/ML to enrich BI to make content more engaging and effective
● Work with automated ETL: Use Google’s Dataflow and BigQuery to extract and transform 100Ms of rows per day from partner companies as the foundation for the Brandtech Cloud
● Effectively communicate and collaborate with remote / distributed teams
● Help shape the product & technical strategy for the Brandtech ecosystem

What we're looking for
● 3-4 years experience with ETL and ELT technologies such as AWS Glue, Airflow, DataFlow etc.
● 2-4 years of experience EMR/Hadoop/Spark or platform Databricks/Snowflakes
● Good understanding and comfort with storage solutions ranging from RDBMS databases (e.g. MySQL, Postgres) to OLAP stores (e.g. Snowflake, BigQuery) to in-memory caches (e.g. Redis, Memcache)
● 2-4 years of experience working on production services, with CI/CD tools and with cloud providers like AWS and GCP
● Bonus, but not required: Experience with web3 and the metaverse or a deep curiosity of those landscapes
We’d love to hear from you even if you don’t have experience or interest in every bullet. There’s no perfect candidate and we want to find the right fit, even if it’s different than we imagine. We especially would like to meet underrepresented/underestimated candidates.
What we'll offer
● Competitive Salary
● Family-Friendly Health Benefits
● Pre-tax commuter benefits
● Your choice of technical setup and equipment
About the location
Culture is a critical focus for our company and much of that comes from in-person collaboration and celebrating wins together. While we’re open to this role being fully remote, we highly encourage you to establish a home base at one of our offices in NYC or Philadelphia (currently open). Collaboration and teamwork should be able to happen regardless of where your desk is, and we’ve got the tools to help make that happen.
Apply
Send your resume to joinus@thebrandtechgroup.com
About The Brandtech Group
The Brandtech Group’s mission is to help businesses do their marketing better, faster and cheaper using technology.
It was founded in June 2015 by former Havas Global CEO and Facebook Client Council founding member, David Jones. The Brandtech Group sits at the intersection of the dramatic growth in mobile, a tech revolution that has empowered people to create, produce and share unprecedented amounts of content, the impact of AI, AR, and blockchain, and frustrated global companies looking for brand- and tech-literate partners.
Group clients include Unilever, Accenture, Google, Adidas, Marriott, Microsoft, Reebok, PayPal, Hertz, LVMH, Nissan Renault, Danone, Uber and Reckitt.
The Brandtech Group is headquartered in New York and has offices in 40 countries & over 50 cities including Amsterdam, Bangalore, Bangkok, Bogota, Boston, Cape Town, Dubai, Geneva, Hong Kong, Istanbul, Jakarta, London, Los Angeles, Manila, Mexico City, Mumbai, Paris, San Francisco, São Paulo, Seoul, Shanghai, Shenzhen, Singapore, Sydney, Tokyo and Toronto.
In support of our mission to help businesses build brands better, faster and cheaper using technology, we focus our operations on achieving excellence, speed and cost-efficiency. We hold very high standards, and operate under US GAAP, working with top-tier legal and audit advisors. Our partners and team members and our operating companies all operate with speed and efficiency enabled by technology.
The Brandtech Group provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training."
52,"Data Engineer, 1+ Years of Experience",Snap Inc.,"Seattle, WA","Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Snap Engineering teams build fun and technically sophisticated products that reach hundreds of millions of Snapchatters around the world, every day. We’re deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. We move fast, with precision, and always execute with privacy at the forefront.

Some core features we build and maintain include Snapchat’s Camera, Creative Tools, Maps, Chat, Memories, Stories, Discover, Games, and Minis. Our Infrastructure teams deliver an innovative and cost-efficient platform that ensures Snapchat is the fastest way to communicate with your friends, no matter where you are in the world. We have one of the fastest growing digital ad platforms, and our Monetization teams drive measurable returns for advertisers through novel ad formats like Augmented Reality. As a Snap Engineering team member, you’ll help us build the future of communication.

We’re looking for a Data Engineer to join Snap Inc!

What You’ll Do
Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner
Develop data pipelines adhering with privacy and governance principles
Become familiar with our data consumption portals and their capabilities
Build expertise and ownership of data quality for supported domains
Establish and implement data quality standards and controls
Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate
Drive adoption of the data sets you’ve produced
Knowledge, Skills & Abilities
Experience in building data pipelines to serve reporting needs
Experience owning all or part of a team roadmap
Ability to prioritize requests from multiple stakeholders in disparate domains
Ability to effectively communicate complex projects to non-technical stakeholders
Minimum Qualifications
BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field
1+ years experience in SQL or similar languages
1+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)
Experience in ETL / Data application development
Preferred Qualifications
Hands on experience with Google BigQuery
Experience in version control systems such as Git
Data architecture and warehousing experience
Experience leading a small team of data or software engineers
Experience with Airflow
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .

Our Benefits : Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid maternity & paternity leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!"
53,"Senior Data Engineer, Insights & Intelligence Team",Catalyst Software,New York City Metropolitan Area,"Posted by
Lana G.
Senior Technical Recruiter at Catalyst Software
Send InMail
Catalyst Overview
Catalyst is the world’s most intuitive Customer Success Platform (CSP), and was built by an experienced group of industry leaders. Our software integrates with all of the tools that CS teams are already using to provide one centralized view of customer data. Customer Success Managers can subsequently take the right actions to prevent churn, increase product adoption, and align the entire organization on a unified workflow to manage customers throughout their journey. Catalyst helps organizations turn Customer Success into a company-wide mission.
Position Overview
Catalyst empowers its users to understand the state of their customers across many different data sources, including SaaS, data warehouses, and product usage data. As a Senior Data Engineer on our Insights and Intelligence team, you will be responsible for designing and implementing the architecture, modeling, and pipelining for systems that will need to scale into 100s of TB of data.
What You’ll Do
Contribute to and evolve our data models and architecture
Execute engineering tasks with maturity in a variety of languages including primarily SQL and Python and to a lesser extent Golang and Ruby
Lead data engineering projects and the development of customer facing data driven application features
Set and implement data governance standards
Architect and drive implementation for self-serve data processing throughout our business
Set the standards, guidelines, and tooling for data engineering work within engineering
Work with a variety of open source, AWS, and GCP technologies
Build and optimize the performance of batch, stream, and queue-based solutions including Kafka and Apache Spark
Understand and extend our current warehousing strategies
Mentor more junior engineers
Advocate for data quality, cost effective scalability, and distributed system reliability and establish automated mechanisms to improve these
Work cross functionally with application engineers, SRE, product, data analysts, data scientists, and ML engineers
In future, work with data scientists and ml engineers to implement and productionize machine learning models

What You’ll Need
5+ years of experience successfully implementing modern data architectures
Strong Project Management skills
Demonstrated experience implementing ETL pipelines preferably with Apache Spark in Python and SQL
Python or other language proficiency
Deep understanding of SQL with relational data stores such as Postgres or Mysql
A strong desire to show ownership of problems you identify, and proven ability to empower others to get more done
Experience with Data Warehouses and Lakes such as Redshift, Snowflake, and Databricks Delta Lake
Experience with distributed streaming tools like Kafka and Spark Structured Streaming
Familiarity with workflow tools such as Airflow, dbt, and Delta Live tables
Experience with automated testing for distributed systems (unit testing, E2E testing, QA, CI/CD, data expectation monitoring)
Experience working with application engineers, product, and data scientists
Experience leading projects
Experience with additional data stores, preferably ElasticSearch
Why You’ll Love Working Here!
Highly competitive compensation package, including equity - everyone has a stake in our growth
Monthly mental health days and two full mental health weeks per year
Comprehensive benefits, including up to 100% paid medical, dental, & vision insurance coverage for you & your loved ones
Open vacation policy, encouraging you to take the time you need - we trust you to strike the right work/life balance
Annual education stipend, to ensure that you're continuously expanding your skill set
Monthly wellness stipend, to ensure that you’re taking care of both your physical & mental health
Monthly remote team-building events, including game nights, trivia, cooking/mixology classes, and more!
Catalyst is an equal opportunity employer, meaning that we do not discriminate based upon race, religion, national origin, gender identity, age, sexual orientation, or any other protected class. We believe that diversity is more than just good intentions, and we are committed to creating an inclusive environment for all employees."
54,Data Engineer,NYC Department of Parks & Recreation,New York City Metropolitan Area,"NYC Parks is an award-winning city agency that designs, builds, and preserves public spaces for New Yorkers. NYC Parks cares for 14% of the land in New York City, diverse array of structures sits on these public spaces including recreation centers, sporting facilities, historic houses, pool facilities, nature centers, marinas, and comfort stations.
Data Engineers act as a liaison between the Information Technology & Telecommunications (ITT) and Information & Performance Management (IPM) departments, working collaboratively with both teams to provide support for Parks’ data-centric needs, including updating and improving data flow, data infrastructure, and overall data management.

NYC Parks offers competitive pay and a generous benefits package, including pension, excellent health benefits, competitive vacation/sick days and a healthy work-life balance.
THE CITY OF NEW YORK IS AN INCLUSIVE EQUAL OPPORTUNITY EMPLOYER COMMITTED TO RECRUITING AND RETAINING A DIVERSE WORKFORCE AND PROVIDING A WORK ENVIRONMENT THAT IS FREE FROM DISCRIMINATION AND HARASSMENT BASED UPON ANY LEGALLY PROTECTED STATUS OR PROTECTED CHARACTERISTIC, INCLUDING BUT NOT LIMITED TO AN INDIVIDUAL'S SEX, RACE, COLOR, ETHNICITY, NATIONAL ORIGIN, AGE, RELIGION, DISABILITY, SEXUAL ORIENTATION, VETERAN STATUS, GENDER IDENTITY, OR PREGNANCY.
Major Responsibilities
· Maintain, improve, clean, and manipulate data in agency operational and analytics databases.
· Analyze complex data elements and systems, data flow, dependencies, and relationships to contribute to conceptual physical and logical data models.
· Lead efforts to standardize data documentation and metadata; identify, promote, and enforce best practices across data types, systems and uses. Troubleshoot data issues.
· Define and help build data pipelines that will enable faster, better, data-informed decision-making within the agency.
· Ensure proper data governance and quality across the agency.
· Proactively analyze and evaluate the agency’s data infrastructure to identify and recommend improvements and optimization.
· Design and develop scalable Extract-Transform-Load (ETL) packages from the business source systems and the development of ETL routines to populate databases from sources and to create aggregates.
· Perform thorough testing and validation to support the accuracy of data transformations and data verification used in analyses.
· Play a collaborative role with ITT and IPM to develop and implement scripts that automate database maintenance, monitoring and performance tuning to be applied across the agency. Find ways to deploy new solutions and cutting-edge technology practices while ensuring interoperability with existing legacy systems.
· Keep up with industry trends and best practices, advising senior management on new and improved data engineering strategies that will drive departmental performance, reduce data and knowledge silos, ultimately improving overall agency performance.
Qualification Requirements
1. A doctorate degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental, or social science and one year of full-time experience in a responsible supervisory, administrative or research capacity in the appropriate field of specialization; or
2. A master’s degree from an accredited college or university with specialization in an appropriate field of physical, biological, environmental or social science and three years of responsible full-time experience in the appropriate field of specialization, including one year of full-time experience in a responsible supervisory, administrative or research capacity in the appropriate field of specialization; or
3. Education and/or experience which is equivalent to “1” or “2” above. However, all candidates must have a master’s degree in an appropriate field of specialization and one year of full-time experience in a responsible supervisory, administrative, or research capacity as described in “2” above.
Residency in New York City, Nassau, Orange, Rockland, Suffolk, Putnam, or Westchester counties required for employees with over two years of city service. New York City residency required within 90 days of hire for all other candidates.
Preferred Skills/Qualifications
1. Outstanding analytical skills across traditional knowledge domains, paired with a well-developed sense of management priorities. Previous team leader/management experience.
2. Demonstrated project management, technology implementation, supervisory, writing, administrative and interpersonal skills.
3. Track record of fostering innovation, prioritizing initiatives, and coordinating the deployment of complex projects across departments and different groups of stakeholders.
4. Strong communication skills to inspire and motivate staff at all levels and develop and maintain effective working relationships with diverse sets of partners within the technology and user community.
5. Experience analyzing and identifying appropriate business solutions to meet operational needs; extensive project management experience, helping develop and implement operations projects from start to finish.
6. Master’s degree or equivalent; at least 10 years of relevant experience.
7. Demonstrated leadership in government innovation. Prior experience working in government and with the operations sector preferred.
Vaccination Requirement: As of August 2, 2021, all new hires must be vaccinated against the COVID-19 virus, unless they have been granted a reasonable accommodation for religion or disability. If you are offered city employment, this requirement must be met by your date of hire, unless a reasonable accommodation for exemption is received and approved by the hiring agency.
* Please note that the Data Engineer will be required to work at both the Arsenal and Arsenal West in Manhattan.

Salary: $75, 504 - $86,380 plus excellent benefits
Open Until Filled
Work Location: Arsenal, Manhattan
For external applicants, please apply through www.nyc.gov/careers
Go to nyc.gov/careers/search and search for Job ID# 528038.
For details about NYC Parks: www.nyc.gov/parks
References will be required upon request."
55,Data Engineer,JPMorgan Chase & Co.,"Jersey City, NJ","We look for people who are passionate around solving business problems through innovation & engineering practices. You will be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals.

J.P. Morgan Asset Management is a global institution that prides itself in the power of scale - offering first class financial products and services to its clients (and its clients' clients) across the spectrum of client needs.

The Business Intelligence and Analytics team's mission is to leverage large datasets to power cutting-edge applications and analytical capabilities within the Asset Management Distribution Business. As a Software Engineer, you will work at the intersection of business analytics, software engineering, and data warehousing and will be responsible for building the foundational data platform critical to the success of Business Intelligence and Analytics

Responsibilities
Build the core platform and frameworks for large-scale batch, ETL and real-time data pipelines using cloud and on-premises data technologies, such as Redshift, Python, Spark, PySpark, and Apache Kafka
Use infrastructure as code to build applications to orchestrate and monitor the data pipelines, create and manage on-demand compute resources on cloud programmatically, create frameworks to ingest and distribute data at scale
Design best practices for data processing, data modeling and warehouse development throughout our team and group
Develop strategy to provide proactive solutions and enable stakeholders to extract insights and value from data
Understand end to end data interactions and dependencies across complex data pipelines and data transformation and how they impact business decisions.
Preferred Skills
Expertise in application, data, and infrastructure architecture disciplines
Advanced knowledge of architecture and design across all systems
Proficiency in multiple modern programming languages
Expert level skills in Python, its standard library and its package
Hands-on experience building a data warehouse and data pipelines using Java, Python or Scala in a data intensive engineering role
Hands-on experience with data warehouse / data lake architectures based on Hadoop, Redshift or Snowflake
Advanced level skills in SQL, data integration, data modeling and data architecture
Experience with workflow orchestration tools such as Apache Airflow, Autosys
Familiarity with data transformation and collection tools such as Pentaho, Informatica
Experience with stream processing platforms such as Kafka or Dataflow
Knowledge of data columnar and serialization formats such as JSON, XML, Parquet, Avro
Experience with container technologies such as Docker and Kubernetes
Experience with CI/CD systems e.g. Jenkins and automation / DevOps best practices
Familiarity with AWS ecosystem including S3, Glue, Redshift, Kinesis, EMR, EC2, SQS
Familiarity of micro-services stack based on AWS Lambdas. Elastic Search, Spring Boot, NodeJS
Knowledge of industry-wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals
Passionate about building an innovative culture
Minimum Requirements
BS/BA degree or equivalent experience in computer science or engineering
Expertise in application, data, and infrastructure architecture disciplines
Advanced knowledge of architecture and design across all systems
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
56,Data Engineer - FinTech,Amazon,"Jersey City, NJ","Description

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access? If so, Amazon Finance Technology (FinTech) is for you!

FinTech is seeking a Data Engineer to join the team consisting of software engineers and data engineers, and shaping the future of the finance data platform. We are committed to building the data stores, and enabling machine learning-based Forecasting applications and Risk Management capabilities for Amazon's rapidly growing and dynamic businesses. Our data platform is used to power data-driven decision-making in Finance and Risk Management through a timely, accurate, and actionable manner.

As a Data Engineer, you are an expert in designing data models and data pipelines at scale. You have deep understanding of wide-variety options building datalake, data pipeline, and data workflow orchestration. You enjoy diving into datasets and learning about the data classification, data governance, consumption patterns, availability requirements, use cases while making design tradeoffs. You have strong analytical skills and are capable of communicating with stakeholders and customers to understand their needs and develop the best solution answering their questions with data. You will be building scalable data ingestion and transformation pipeline, delivering self-service solutions to our customers, allowing them to be self-sufficient with analytics. You are enthusiastic about learning new technologies and eager to adopt them to delight our customers.

Ideal candidates thrive in this fast-paced environment and relish working with large volumes, enjoy the challenge of highly complex business contexts, and, is a passionate about data and analytics. In this role you will take a part of Amazon's journey to Work Hard, Have Fun and Create history!

Responsibilities
Design and develop scalable and maintainable data products to democratize the data for data science and analytics.
Build efficient and high performance data models to help the customers unlock data driven business decisions.
Interface with cross functional teams to gather the requirements and deliver end to end products.
Develop subject matter expertise in the FinTech data and build solutions to help discover the datasets.
Implement and simplify self-service data query and analysis capabilities of the BI platform to Customers.
Stay up to date on emerging technologies, experimenting and implementing where appropriate.
Triage possible courses of action in a high-ambiguity environment, and adopt quantitative analysis and business judgment when making design decisions.
Implement and simplify self-service data query and analysis capabilities of the BI platform to Customers.

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in Computer Science or related technical field.
3+ years experience using at least one of the Big Data technologies (Parquet, Spark, Hadoop, Presto, EMR, etc.)
Proficiency in at least one modern programming language such as Scala, or Python
Preferred Qualifications
3+ years experience with AWS services including S3, Redshift, EMR and RDS.
5+ Years experience with SQL or other query languages
Ability to distill ambiguous customer requirements into a technical design
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.
Experience providing technical leadership and educating other engineers for best practices on data engineering.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1719625"
57,Python / Data Engineer [REMOTE],Braintrust,United States,"ABOUT US:

Braintrust is the only network that gives in-demand talent all the freedom of freelance with all the benefits, community and stability of a full-time role. As the first decentralized talent network, our revolutionary Web3 model ensures the community that relies on Braintrust to find work are the same people who own and build it through the blockchain token, BTRST. So unlike other marketplaces that take 20% to 50% of talent earnings, Braintrust allows talent to keep 100% of earnings and to vote on key changes to improve the network. Braintrust is working to change the way freelance works – for good.

JOB TYPE: Freelance, Contract Position - No agencies (See notes below)

LOCATION: Remote - Open to US/India Only

HOURLY RANGE: Our client is looking to pay $60-$100 /hr

ESTIMATED DURATION: 40Hrs/Week - Long Term, 6-month project

About The Role
Must be based in the US or India
Fluency developing tools, applications and backend services in Python
Command of Python, Spark, Scala or Java for ETL or ELT processes
Strong experience with RDBMS database like MySQL or Postgres (preferred) including writing complex queries and performance tuning
Good understanding of the challenges of data quality and data testing and how to mitigate them
Cloud platform experience with GCP (preferred), AWS or Azure
Proficiency using source control tools like Github and process tools like Jira/Confluence
Education in Computer Science, Engineering or training in a related field
Knowledge of Pub/Sub architectures or other messaging / streaming technologies is a plus
Experience with NoSQL data stores like MongoDB, BigTable, DynamoDB and analytics warehouses like BigQuery or Redshift is a plus
Familiarity with data-warehouse-as-a-service offerings like Databricks or Snowflake is a plus
Experience working with large data sets using open source technologies such as Spark is a plus

Apply Now!

ABOUT THE HIRING PROCESS:

Qualified candidates will be invited to do a screening interview with the Braintrust staff. We will answer your questions about the project, and our platform. If we determine it is the right fit for both parties, we'll invite you to join the platform and create a profile to apply directly for this project.

C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
58,"Data Engineer, 1+ Years of Experience",Snap Inc.,"San Francisco, CA","Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Snap Engineering teams build fun and technically sophisticated products that reach hundreds of millions of Snapchatters around the world, every day. We’re deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. We move fast, with precision, and always execute with privacy at the forefront.

Some core features we build and maintain include Snapchat’s Camera, Creative Tools, Maps, Chat, Memories, Stories, Discover, Games, and Minis. Our Infrastructure teams deliver an innovative and cost-efficient platform that ensures Snapchat is the fastest way to communicate with your friends, no matter where you are in the world. We have one of the fastest growing digital ad platforms, and our Monetization teams drive measurable returns for advertisers through novel ad formats like Augmented Reality. As a Snap Engineering team member, you’ll help us build the future of communication.

We’re looking for a Data Engineer to join Snap Inc!

What You’ll Do
Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner
Develop data pipelines adhering with privacy and governance principles
Become familiar with our data consumption portals and their capabilities
Build expertise and ownership of data quality for supported domains
Establish and implement data quality standards and controls
Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate
Drive adoption of the data sets you’ve produced
Knowledge, Skills & Abilities
Experience in building data pipelines to serve reporting needs
Experience owning all or part of a team roadmap
Ability to prioritize requests from multiple stakeholders in disparate domains
Ability to effectively communicate complex projects to non-technical stakeholders
Minimum Qualifications
BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field
1+ years experience in SQL or similar languages
1+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)
Experience in ETL / Data application development
Preferred Qualifications
Hands on experience with Google BigQuery
Experience in version control systems such as Git
Data architecture and warehousing experience
Experience leading a small team of data or software engineers
Experience with Airflow
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .

Our Benefits : Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid maternity & paternity leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!"
59,"Data Engineer, 1+ Years of Experience",Snap Inc.,"Mountain View, CA","Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Snap Engineering teams build fun and technically sophisticated products that reach hundreds of millions of Snapchatters around the world, every day. We’re deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. We move fast, with precision, and always execute with privacy at the forefront.

Some core features we build and maintain include Snapchat’s Camera, Creative Tools, Maps, Chat, Memories, Stories, Discover, Games, and Minis. Our Infrastructure teams deliver an innovative and cost-efficient platform that ensures Snapchat is the fastest way to communicate with your friends, no matter where you are in the world. We have one of the fastest growing digital ad platforms, and our Monetization teams drive measurable returns for advertisers through novel ad formats like Augmented Reality. As a Snap Engineering team member, you’ll help us build the future of communication.

We’re looking for a Data Engineer to join Snap Inc!

What You’ll Do
Work closely with stakeholders in engineering, finance, sales, marketing, strategy, and governance to make high quality datasets available to consumers in a timely manner
Develop data pipelines adhering with privacy and governance principles
Become familiar with our data consumption portals and their capabilities
Build expertise and ownership of data quality for supported domains
Establish and implement data quality standards and controls
Build tooling and implement systems to overcome limitations of the data consumption portals when appropriate
Drive adoption of the data sets you’ve produced
Knowledge, Skills & Abilities
Experience in building data pipelines to serve reporting needs
Experience owning all or part of a team roadmap
Ability to prioritize requests from multiple stakeholders in disparate domains
Ability to effectively communicate complex projects to non-technical stakeholders
Minimum Qualifications
BS/BA degree in Computer Science, Math, Physics, or a related field, or equivalent years of experience in a relevant field
1+ years experience in SQL or similar languages
1+ years development experience in at least one object-oriented or scripting language (Python, Java, Scala, etc)
Experience in ETL / Data application development
Preferred Qualifications
Hands on experience with Google BigQuery
Experience in version control systems such as Git
Data architecture and warehousing experience
Experience leading a small team of data or software engineers
Experience with Airflow
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. EOE, including disability/vets. If you have a disability or special need that requires accommodation, please don’t be shy and contact us at accommodations-ext@snap.com .

Our Benefits : Snap Inc. is its own community, so we’ve got your back! We do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. Our benefits are built around your needs and include paid maternity & paternity leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in Snap’s long-term success!"
60,Data Engineer,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Global Data Management Inc, is seeking the following. Apply via Dice today!

Data Engineer ChicagoNYC Primary Responsibilities Implement the services defined in the blueprint ultimately developed by our solution architect Partner with our AWS and IT teams to design and deploy a cloud big data platform that is fit for purpose and in line with business goals and objectives Develop and support features for platforms such as AWS S3 and Redshift Contribute towards the definition and SDLC adoption of strategic enterprise platform architecture standards including partnering with Data Architecture on naming and modelling standards supporting data at rest (logical and physical models) and data in motion (messaging, APIrsquos) Develop platform standards for how integrations are developed, and how features are developed within the ecosystem Provide design and technical support for application teams that are re-architecting legacybatch data solutions into more modern, event-based technical solutions for on-prem and the public cloud (AWS) Provide support to enterprise application teams to define re-usable critical cloud technologies Qualifications 10+ years experience as a cloud database engineer with a proven track record of delivery Expertise with AWS infrastructure such as S3, Redshift, Glue, and Athena required Hands-on experience as a database developer andor administrator is a must ndash from initial logical design to physical deployment, for both new build and change management Working experience with python or ansible for the purposes of developing a software defined infrastructure Experience engineering and supporting enterprise-level big data platforms is required Self-starter with a willingness to step outside normal boundaries given the startup nature of this role Team-oriented, with proven ability to work well and collaborate globally across multiple and disparate teams Bachelorrsquos or Masterrsquos degree in Systems Engineering, Computer Science or similar technical discipline"
61,Data Engineer II,Federal Reserve Bank of Dallas,"Dallas, TX","Company

Federal Reserve Bank of Dallas

A requirement of this position is that you must be fully vaccinated against COVID-19 or qualify for an accommodation from the Bank's vaccination policy; the Bank will provide accommodations as needed by law for individuals unable to be vaccinated because of medical condition or sincerely held religious belief.

Location:

We are dedicated to serving the public by promoting a strong financial system and a healthy economy for all. These efforts take a team of dedicated individuals doing many different jobs. Together we’re creating a workplace where talented people can thrive, and we welcome your unique background and perspective to help present the best possible solutions for our partners.

About the Role:

The Federal Reserve Bank of Dallas is seeking a Data Engineer II who will serve as an integral member of a national team responsible for supporting the core functions of the Credit Risk Management business line. This person provides data and analytics development services and technical support. The Data Engineer II develops, tests and maintains data and analytics solutions (data warehouse/mart/stores/data lake/reporting/analytics) using tools and programming languages.

The Data Engineer II works under the guidance of senior team members, operating under a clear framework of accountability with substantial autonomy. The successful candidate will develop a strong domain knowledge of the relevant business area/s, managing key responsibilities to include requirement analysis, code, test, debug, document, implement and maintain data and analytics solutions.

You Will:
Develop data and analytics solutions
Perform analysis on data sets
Monitor, maintain and enhance data processes
Analyze and resolve customer requests and data issues
Proactively identify and implement process improvements
Participate in planning, conduct stakeholder meetings, requirements, design, development, testing and implementation
Develop in-depth understanding of Credit Risk Management business function

You Have:
Working experience in data analytics, data integration and data science
Experience building dashboards, and reports with BI tools
Strong Experience with scripting languages (Unix/Windows/Python/R)
Ability to develop data driven recommendations with actionable insights using predictive models
Understanding of ETL (Extract Transform Load) Data Pipelines and relational databases
Understanding of data warehouse concepts and frameworks
Familiarity to work with AWS data technologies
Experience with working within an Agile environment
Strong customer focus and business acumen
Extremely organized with strong time-management skills
Capable of adapting to changing requirements and responsibilities
Other skills: problem solving, analysis, excellent communication skills
Equivalent education and/or experience may be substituted for any of the above

Our Benefits:

Our total rewards program offers benefits that are the best fit for you at every stage of your career:
Comprehensive healthcare options (Medical, Dental, and Vision)
401K match, and a fully funded pension plan
Paid vacation, holidays, and volunteer hours; flexible work environment
Generously subsidized public transportation and free parking
Annual tuition reimbursement
Professional development programs, training and conferences
And more…

Notes:

This position may be filled at various levels based on candidate's qualifications as determined by the department.

This position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are not U.S. citizens or U.S. permanent residents may be eligible for the information access required for this position and sponsorship for a work visa, and subsequently for permanent residence, if they sign a declaration of intent to become a U.S. citizen and meet other eligibility requirements.

In addition, all candidates must undergo an enhanced background check and comply with all applicable information handling rules, and all non-U.S. citizens must sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship.

The Federal Reserve Bank of Dallas is proud to be an Equal Opportunity Employer that believes in the diversity of our people, ideas and experiences, and we are committed to building an inclusive culture that represents the communities we serve.

If you need assistance or an accommodation due to a disability, please notify your Talent Acquisition Consultant.

Full Time / Part Time

Full time

Regular / Temporary

Regular

Job Exempt (Yes / No)

Yes

Job Category

Work Shift

First (United States of America)

The Federal Reserve Banks believe that diversity and inclusion among our employees is critical to our success as an organization, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. The Federal Reserve Banks are committed to equal employment opportunity for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.

Privacy Notice"
62,Data Engineer,Carta Healthcare,"New York, United States","Posted by
Muhammad Qasim
Recruiting A-Listers | Remote Talent Acquisition Specialist | Technical Recruiter | International Hiring | Recruiting Across 6 Time Zones | Recruiting for Tech
Send InMail
At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare
We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.
Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.
You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.
Responsibilities:
Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.
What you’ll need:
Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends
Why we love Carta Healthcare, and why you will too!
Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
63,"Big Data Engineer - Data Modeling (Full-time, Direct hire, Fully Remote)",Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Dawar Consulting, is seeking the following. Apply via Dice today!

Fulltime, Direct hire, Remote (Work in PST hours) Client Startup (Pre-IPO), San Francisco, CA Salary Range 140k - 200k+ (DOE) + Full Benefits Perks + Sign-in bonus Our client is building industryrsquos most trusted mobile data and analytics platform. They are looking for a ldquoBig Data Engineerrdquo who can create innovative new products in the analytics and data space. You will participate in the development that creates the world's 1 app stores analytics service. Together with the team you will build out new product features and applications using agile methodologies and open source technologies. You will work directly with Product Managers, Software Architects, and will be on the front lines of coding new and exciting analytics and data mining products. You should be passionate about what you do and excited to join an entrepreneurial start-up. Roles Responsibilities As a Big Data Engineer, we will need you to be in charge of model implementation and maintenance, and to build clean, robust and maintainable data processing program that can support these projects on huge amount of data, this includes Able to design and implement complex product components based on requirements with possible technical solutions. Write data programs using pyspark with a commitment to maintaining high quality work while being confident in dealing with data mining challenges. Discover any feasible new technologies lying in the Big Data ecosystem, share them to team with your professional perspectives. Get up to speed in the machine learning domain, implementing analysis components in a distributed computing environment with instruction from Data Scientists. Be comfortable conducting detailed discussions with Data Scientists regarding specific questions related to specific data models. You should be a strong problem solver with proven experience in big data. Qualifications Skills Hands-on experience and deep knowledge of Hadoop ecosystem Must PySpark, Mapreduce, HDFS Plus Storm, Kafka Must have 2+ years Linux environment development experience. Proficient with programming in Python, experience in Pandas, Sklearn or Other data science and data analysis toolset is a big plus. Having a background of data mining and machine learning domain, familiar with common algorithms and libs is a plus. Passion for cloud computing (AWS in particular) and distributed systems. You must be a great problem solver with the ability to dive deeply into complex problems and emerge with clear and pragmatic solutions. Good communication, and cooperation globally. Please respond with your updated resume and contact information. Best Regards, Dawar Consulting Inc.,"
64,"Data Engineer, DAC Economics",Amazon,"Culver City, CA","Job Summary

DESCRIPTION

Cutting edge big data technology with Spark, EMR, Glue, SageMaker, and Airflow? Check. Deep involvement with business strategy decisions? Check. Work across one of the world's largest and most complex data environments? Check.

Are you excited about the idea to work with big data resources and technologies? Are you up to the challenge of working with top PhD economics and data science researchers to bring front tier economic theories to production? Are you ready to build data pipelines that inform billion dollar business decisions across Amazon? If your answer is yes, come join us!

We are a hyrbid research + engineering team that brings disruptive econometric models to life in the media entertainment and advertising domains. As a Data Engineer, you will collaborate with research scientists, economists, and software engineers across the company to develop, test and deploy a wide range of econometric and ML models. You will face our ever-growing information challenges and provide solution to our analytics and research science teams with the right data pipelines. You’ll build and support data gathering and validation systems at the forefront of the ML and Big Data revolution that are used for analytics, machine learning, and econometrics at scale.

You should be experienced in the architecture of data solutions for the Enterprise environment, such as ETL, Redshift, Spark, etc. You should excel in the design, creation, management, and business use of large (10 TB+) datasets. You should have excellent business and communication skills for working with scientists and project owners to build data sets that answer business questions. But above all, you should be passionate about working with huge data sets to answer hard business questions and drive disruptive change.

Key job responsibilities
Create and support AWS based data platforms that feed input to and serve output from machine learning / econometric models
Develop and extend reusable libraries for quickly rolling out new data pipelines
Establish key relationships which span Amazon business units and Business Intelligence teams.
Create and drive data governance strategies for disparate data sources across the company
Implement standardized, automated operational and quality control processes to deliver accurate and timely data to meet or exceed SLAs.
Create and review technical requirement and design documents, working backward from customer needs.
A day in the life

You'll spend most the day coding, possibly writing or testing Spark jobs, updating our reusable libraries, or tweaking an Apache Airflow graph. You'll join a daily standup and sync with your team mates, and maybe perform a code review or two. Depending on the project phase you may be finishing up your design document for review with your fellow engineers or scientists. Finally you'll probably join a couple of meetings, either related to your project, Sprint Planning, or maybe even a fun team event!

About The Team

We're a pretty atypical team at Amazon, with engineers outnumbered by scientists 3 to 1. Our mission is to 1) productionize science prototypes into well tuned, scaled systems and 2) Support and improve data infrastructure for scientists quickly prototype new ideas and models. You'll be working closely on your projects not just with other data or software engineers, but with Business Intelligence Engineers, Economists, and Data Scientists. Your engineering background and experience is critical to building reliable, cost effective products for our business partners and Amazon customers.


Basic Qualifications
Bachelors or Masters Degree in Computer Science, Information Systems or related field
2+ years experience in Data Engineering or Business Intelligence roles working with ETL, Data Modeling, and Data Architecture
ETL design and SQL skills, knowledge of industry best practices, and an understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment.
Experience with Big Data technologies such as Pig/Hive/Spark
Proficiency in at least one programing or scripting language - Java, Scala, Python, Ruby, linux shell, or similar
Preferred Qualifications
Experienced with setting up end to end Data pipelines in an Enterprise environment
Proficient in performance optimizing Spark queries and jobs
Experiences with Amazon Web Services tools such as Redshift, EMR, SageMaker, Step Functions, Managed Airflow, or other similar platforms
Experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements
Excellent communication skills with both technical and non technical users
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1746668"
65,Data Analytics Engineer,Scientific American,"New York, NY","Posted by
Holly E. Ciccarella
US Recruitment Manager at Springer Nature
Send InMail
Job description:



Springer Nature is seeking a Data Analytics Engineer for its New York based Scientific American Team. The team develops new data products for the research community. This is an exciting opportunity for the Data Analytics Engineer, expanding from strong foundations to build new solutions and services. We are looking for someone who is able to deliver solutions and work independently, with support from the wider team where necessary.



As a Data Analytics Engineer, you will be responsible for ensuring continuous flow of data with minimum latency between data sources. You will be developing, testing and deploying data pipelines into the production environment. You’ll be responsible for implementation of Google Analytics 4 server side tracking, following the existing solution that will be provided. You’ll be analyzing big data from very highly trafficked websites and content. You will provide actionable analysis and insight into the behavior of users. Driving change that improves their experience with SpingerNature and our customers, contributing to our purpose to advance discovery with some of the most interesting datasets available.



You’ll be working in close partnership with data analysts, scientists and engineers and researchers from Springer Nature. You’ll be working with the latest data and analytics technologies including graph databases, Google Analytics, Google Tag Manager, BigQuery, Looker and Plotly Dash as well as previewing solutions from Google and other partners.





Role responsibilities will include:

Build streaming/batch Data pipelines for extraction/loading/transforming data between various data sources at scale in different formats.
Work closely with Data Scientists /Analysts to understand the requirements and develop the data solutions in line with the business requirements.
Maintain the current cloud infrastructure and help onboard the new applications.
Developing sophisticated segmentation, analysis and dashboards that support Springer Nature and its customers to advance discovery.
Implementing cutting edge analytics and data science solutions in partnership with the wider teams
Consulting with stakeholders throughout the business to shape and implement solutions in support of business objectives.
Play a key role in shaping solutions, creating the right governance and safety checks to maintain tracking accuracy.


Role requirements:

BA degree with a strong analytical/quantitative background or equivalent experience (e.g. Data Science, Statistics, Mathematics, Econometrics, Physics, Computer Science etc.)
Strong working knowledge of SQL, Google Analytics, Google Tag Manager and Python
Excellent problem solving capabilities
Knowledge of Machine Learning concepts is beneficial but not essential as training will be provided
Prior experience with schema designing data modeling
Familiarity with Google Cloud products (BigQuery, Colab, Data Studio, Looker, Dataform, Google Analytics) or other cloud data platforms is beneficial but not essential
Well organized and accurate with good time management





Desired Skills and Experience
N/A"
66,"Data Engineer (AWS, SQL)",TransUnion,"White Plains, NY","TransUnion's Job Applicant Privacy Notice

What We'll Bring

At TransUnion, we strive to build an environment where our associates are in the driver’s seat of their professional development, while having access to help along the way. We encourage everyone to pursue passions and take ownership of their careers. With the support of colleagues and mentors, our associates are given the tools needed to get where they want to go. Regardless of job titles, our associates have the opportunity to learn new things and be a leader every day.

Come be a part of our team – you’ll work with great people, pioneering products and cutting-edge technology.

What You'll Bring
May/ June 2022 grad with an expected Bachelor’s or Masters degree in engineering, business, or an analytical field such as economics, statistics, etc.
Familiarity with SQL and AWS or other programing/data mining language (e.g. Python, VBA) is desired
A track record of outstanding academic performance
Excellent verbal, written and presentation skills
Ability to work in a team setting and independently
Strong analytical, communication, creative and interpersonal skills
Demonstrated problem solving and root cause analysis skills
Ability to work creatively and analytically in a problem-solving environment
Eagerness to contribute in a team-oriented environment
Must have passion for technology and automation
Impact You'll Make

We are seeking an Data Engineer to join Verisk Financial Services Development Operations Technology and Security (DOTS) organization to support our production platform on Amazon Web Services (AWS). This is an opportunity for May/ June 2022 graduates to begin their career. Candidates will apply business analysis to create advanced business intelligence solutions providing insight to clients. This role requires the candidate to use analytic tools and database skills to develop and evaluate a broad spectrum of analytics for large data sets. Your responsibilities may include loading client data, validating client data, processing client data and producing Business Intelligence reporting on client data. You will work with leading Database Technologies and Data Warehousing toolkits to master the complex nuances in financial data received at Argus. Some of the technologies that you will use are AWS, SQL Server, SQL server Reporting Services (SSRS), SQL Server Integration Services (SSIS), Visual Basic .Net , ASP, .NET, Python etc. Responsibilities:
Load, transform, validate, and report on client data
Analyze data to systematically apply in-house developed, complex analysis technology to test the data quality
Interface with team to assist with complex ad-hoc analysis on data; and analyze data to respond to client inquiries
Support Onboarding of new Financial Institution and product portfolio to the study
Develop data driven exploratory analysis to solve defined problems
Take a proactive role in solving complex technical challenges in the Data Warehousing operations
Document technical specification to manipulate data; execute and deliver analytic components of projects; produce graphs reports and summaries
Work on projects to enhance and improve the in-house technology suite for validating and reporting on new data
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, veteran status, marital status, citizenship status, sexual orientation, gender identity or any other characteristic protected by law.

During the COVID-19 pandemic, TransUnion has several safety protocols in place to protect associates, customers, and visitors. You may be required to be fully vaccinated against COVID-19 as a condition of employment and/or to participate in certain work-related activities. Exemption is available to qualified candidates as a reasonable accommodation.

TransUnion's Internal Job Title

Acquisition Exempt, Acquired Associate"
67,Data Engineer - Site Reliability,Morgan Stanley,"Alpharetta, GA","Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management and wealth management services. The Firm's employees serve clients worldwide including corporations, governments and individuals from more than 1,200 offices in 43 countries. As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence and strong team ethic. Morgan Stanley can provide a superior foundation for building a professional career - a place for people to learn, to achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.

Technology works as a strategic partner with Morgan Stanley business units and the world's leading technology companies to redefine how we do business in ever more global, complex, and dynamic financial markets. Morgan Stanley's sizeable investment in technology results in quantitative trading systems, cutting-edge modeling and simulation software, comprehensive risk and security systems, and robust client-relationship capabilities, plus the worldwide infrastructure that forms the backbone of these systems and tools. Our insights, our applications and infrastructure give a competitive edge to clients' businesses—and to our own.

The Data Engineering group, RTOI is responsible for design and support for continuous data streams from disparte sources into a variety of targets. Includes design and development of applications to consolidate data from various sources (Internal + External), integration and enrichment with other sources of data with central repositories, applications to manage data flows between heterogeneous sources and applications to uniformly distribute data for downstream consumption. Working in a highly collaborative and dynamic environment to engineer, develop and integrate a variety of systems and applications through multiple environments.

Tasks Include, But Are Not Limited To
Design and support Data pipeline initiatives
System design, site reliability, administration and performance tuning. Engineer optimizations and solutions for real time streaming applications.
Ensure performance, availability and scalability of data solutions including Kafka and Elastic Search (ELK)
Skills
Strong Unix / Linux skills
Apache Kafka
Elastic stack
Working knowledge of scripting languages (e..g, Shell + python)
Understand continuous data stream concepts
Working knowledge of XML
Strong troubleshooting skills
Database (eg: SQL, Sybase, DB2)
Team oriented and can work well within a global collaborative model
Be comfortable expressing your ideas in meetings, design sessions, etc.
Good analytical and problem solving skills that are coupled with strong communication
Self-sufficient and show ability to lead given the opportunity
Posting Date

Nov 22, 2021

Primary Location

Americas-United States of America-Georgia-Alpharetta

Education Level

Bachelor's Degree

Job

Engineering

Employment Type

Full Time

Job Level

Associate"
68,Data Engineer,Robert Half,"Princeton, NJ","Description

Responsibilities Responsibilities for this opportunity include, but are not limited to:
Interpret data, analyze results using statistical techniques and provide ongoing reports.
Identify, analyze, and interpret trends or patterns in complex data sets.
Implement custom processes for data cleansing and transformation.
Utilize multiple tools and programs to analyze and convert source data to a standardized format.
Build and maintain documentation for data conversion processes while seeking out process improvements.
Perform data quality control reviews and data assessments to identify trends and anomalies.
Research new techniques and best practices within the industry.
Work with product and engineering to define objectives and develop analytic tools and products.
Requirements

Qualifications In addition, our ""ideal candidate"" has the following skills & experience:
Bachelor’Microsoft degree in CS, Engineering, Mathematics, Economics, Statistics, or related field.
You are an analytical, result driven individual with high attention to detail.
You are a self-starter, quick learner, problem-solver and someone who relishes challenges.
3+ years using SQL or other query and scripting languages to aggregate, gather, and manipulate data.
2+ years using scripting languages such as Python or Bash.
Deep understanding of relational databases, ETL tools, data conversion and data cleansing methodologies
Experience with (or at least exposure to) cloud-based databases like Redshift, Snowflake, or BigQuery.
Your verbal and written communication skills are excellent.
You have a love for all things data and data-driven solutions.
Pride in your work, and high standards for yourself and the company you work for.
Strong communication, analytical and collaborative problem-solving skills.
Proven ability to learn quickly, work independently, and adapt to change in a fast-paced environment.
Experience with Big Data technologies such as Spark and Hadoop
Experience with Cloud Platforms such as AWS or Google Cloud
Previous AdTech or MarTech industry experience
Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
69,Data Engineer,Robert Half,"Princeton, NJ","Description

Responsibilities

Responsibilities for this opportunity include, but are not limited to:
Interpret data, analyze results using statistical techniques and provide ongoing reports.
Identify, analyze, and interpret trends or patterns in complex data sets.
Implement custom processes for data cleansing and transformation.
Utilize multiple tools and programs to analyze and convert source data to a standardized format.
Build and maintain documentation for data conversion processes while seeking out process improvements.
Perform data quality control reviews and data assessments to identify trends and anomalies.
Research new techniques and best practices within the industry.
Work with product and engineering to define objectives and develop analytic tools and products.
Requirements

Qualifications

In addition, our ""ideal candidate"" has the following skills & experience:
Bachelor’s degree in CS, Engineering, Mathematics, Economics, Statistics, or related field.
You are an analytical, result driven individual with high attention to detail.
You are a self-starter, quick learner, problem-solver and someone who relishes challenges.
3+ years using SQL or other query and scripting languages to aggregate, gather, and manipulate data.
2+ years using scripting languages such as Python or Bash.
Deep understanding of relational databases, ETL tools, data conversion and data cleansing methodologies
Experience with (or at least exposure to) cloud-based databases like Redshift, Snowflake, or BigQuery.
Your verbal and written communication skills are excellent.
You have a love for all things data and data-driven solutions.
Pride in your work, and high standards for yourself and the company you work for.
Strong communication, analytical and collaborative problem-solving skills.
Proven ability to learn quickly, work independently, and adapt to change in a fast-paced environment.
Experience with Big Data technologies such as Spark and Hadoop
Experience with Cloud Platforms such as AWS or Google Cloud
Previous AdTech or MarTech industry experience
Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
70,Big Data Engineer,Dice,"Philadelphia, PA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Pyramid Consulting, Inc., is seeking the following. Apply via Dice today!

Job Position Big Data Engineer Job Location Philadelphia, PA (Initial Remote) Job Type Contract Job Description Excellent 5+ years of hands on hard core data engineering experience on Scala, Hadoop, EMR, spark, kafka Good knowledge in AWS, Kubernetes and production support troubleshooting. Experience with AWS cloud services Ec2, EMR, RDS and Redshift"
71,"Data Engineer, Snowflake",Deloitte,"Parsippany, NJ","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
72,"Data Engineer, Spark",Deloitte,"Miami, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
73,"Data Engineer, CTPS",Amazon,"San Diego, CA","Job Summary

DESCRIPTION

Job Summary

Have you ever thought about what it takes to detect and prevent fraudulent purchases among hundreds of millions of ecommerce transactions in six countries? What would you do to create a trusted marketplace where millions of buyers and sellers can safely transact online? What kinds of processes and systems would you build to maximize customer satisfaction?

Our mission in Payments Risk is to make Amazon.com the safest place to shop online. The Payments Risk team safeguards the order pipelines; monitoring, tracking, and managing risk to ensure long-term buyer satisfaction. The Payments Risk group designs and builds the software systems, risk models and operational processes that minimize risk and maximize trust in Amazon.com. In addition to this, we evaluate new business opportunities from across the company to determine how we can minimize the risk associated with these launches.

Amazon.com is seeking an outstanding Data Engineer to join the Payments Risk Analytics team. Amazon.com has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the Amazon.com Payments Risk team your work will have an immediate influence on day-to-day decision making at Amazon.com and drive the adoption of new Business Intelligence technologies.

Are you passionate about turning large amounts of information into knowledge? Are you passionate about building and leading data teams? Can you architect large data systems? Can you work with business partners and across technical teams?

A key responsibility of this role is to build large data systems that help us drive advanced analytics for Payments Risk. A successful candidate will be uncompromisingly detail oriented, efficient, and customer obsessed. Your work must be accurate, timely, and insightful. You must be a self-starter who is able to think big and work in a fast-paced and ever-changing environment. You will build sustainable and scalable analytics processes, and data systems that can be leveraged across Payments Risk.

As a Data Engineer on the Analytics team, you’ll have huge impact on how customers engage with Amazon through building infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, and recent advances in distributed systems (i.e. MapReduce, noSQL databases). You will work with passionate scientists, business intelligence engineers, software development engineers and product managers, to deliver a variety of stable and performant data feeds used for developing business insights as well as offline machine learning use cases.

We love to work with smart people who have a strong sense of ownership and strong engineering mindset. You are a technical leader for your team and a great mentor. You provide perspective and context for technology choices. You’re up to the challenge of real-time notification strategies, latency, TPS and building an end-to-end platform that internal Amazon teams integrate with. You motivate your team to pursue ambiguous situations and rapidly produce prototypes for a more personalized experience. You outline paths from prototype to product. You deeply invest in each colleague's career growth, improving their technical knowledge, and defining your team's operational metrics.


Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Experience working with AWS big data technologies (EMR, Redshift, S3, AWS Glue, Kinesis and Lambda for Serverless ETL)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Hands-on experience and advanced knowledge of SQL
Basic scripting skills using Python and Scala
Basic understanding of Machine Learning
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using business intelligence reporting tools (Quicksight, Tableau etc.)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Mindset and analytical skills to towards continuous improvement and have an edge to always research on latest technologies
Passion for building great notification experiences which directly impacts our customers
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2013918"
74,Data Engineer: Application Modernization,IBM,"Atlanta, GA","529272BR

Introduction

Engineers at IBM are the backbone of our strategic initiatives to design, code, test, and provide industry-leading solutions that make the world run today - planes and trains take off on time, bank transactions complete in the blink of an eye and the world remains safe because of the work our software developers do. Whether you are working on projects internally or for a client, software development is critical to the success of IBM and our clients worldwide. At IBM, you will use the latest software development tools, techniques and approaches and work with leading minds in the industry to build solutions you can be proud of.

Your Role and Responsibilities

Application modernization is a new initiative in IBM, which is transforming the way IBM works. We are creating a better developer experience for all developers within IBM. Data Engineer’s primary responsibility is to drive the transformation of IBM applications to modern database architecture, to be deployed on Hybrid Cloud. We cultivate an open, healthy, diverse, and engaging work environment where team members are continuously gaining new skills which align with individual interests.

Responsibilities May Include

Ability to analyze data and existing database design with the goal of enacting improvements geared towards making processes more efficient.
Evaluating and selecting appropriate data modernization tools to enhance our teams’ productivity and user experience.
Participate in defining CIO’s data architecture framework, standards, and principles including data management, security, and infrastructure.
Leads multi-functional teams or conducts special projects.

You will be challenged to design and develop scalable systems, leveraging a wide variety of open source and commercial technologies, closely collaborating with IBM Research and Red Hat.

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

IBM will not be providing visa sponsorship for these positions now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Required Technical and Professional Expertise

Education

A combination of education, technical training, and/or work/military experience equivalent to a bachelor’s degree in computer science, IT, etc.

General

Excellent time management and interpersonal skills
Familiarity with Agile methodologies and principles, experience in an Agile team
Understanding of DevOps practices (Continuous Integration/Delivery/Monitoring/etc.)
Willingness to work with the entire stack (from the database to containers to monitoring and alerting)
Ability to write and review code and make decisions regarding technical implementations
Growth mindset

Technical Skills

5+ years’ experience Applied Knowledge of Software Development
3+ years’ experience in DB2 or other relational databases or NoSQL databases
Knowledge of ETL practices and methodologies
Application SQL development and SQL Performance tuning, where applicable

Preferred Technical And Professional Expertise

Previous experience as a Backend or Full Stack Developer
Automated testing frameworks
Experience in working with containers and container management systems including Docker, Kubernetes and/or OpenShift
Cloud computing knowledge

About Business Unit

The Office of the Chief Information Officer (CIO) owns IBM’s IT strategy and provides the tools, workstations, devices, and infrastructure that IBMers use to do their jobs every day. Put simply, our mission is to create a productive environment for IBM's 350,000 worldwide employees. Join us as we lead with design to drive simplicity and ease of use, engineering the systems that run the business, and innovating to transform the business.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
75,Data Engineer August 2022 Start Date,Capgemini,"New York, NY","Job Title: Data Engineer

Location: New York, NY/Chicago, IL

Travel/Relocation/Remote: Must be open

Job Description

We offer you training and a career path to grow your skills in data, consulting, analysis, problem solving, data visualization and cloud. Our Associates work in salaried consulting roles as Data Engineers, Software Engineers, or Data Analysts towards developing, testing and maintaining business applications. Upon completing our initial multi-week training program to enhance your programming, delivery and consultative skills, you will be ready to begin your first project. Our project roles are aligned with our financial services clients on innovative Data, AI, Cloud projects with our customers in banking and insurance. You’ll work closely with our management team who will provide you guidance, growth opportunities, tools, and genuine feedback. Capgemini provides an ideal opportunity for bright engineering-savvy minds to apply and grow their talents in real-world applications through modern business solutions.

Qualifications
Bachelor’s degree in Computer Science, Computer Engineering, Information Systems, Data Science or related field
Learning mindset with a willingness to embrace new technologies quickly
Experience working with and developing big data solutions
Programming experience in at least one of the following languages: Scala, Java, C/C++, or Python
Excellent verbal and written communication skills, as well as the willingness to collaborate across teams of internal and external technical staff, business analysts, software support and operations staff.
US Citizen or Permanent Resident
Preferred Qualifications
Experience in developing Hive, Sqoop, Spark, Kafka, HBase on Hadoop and cloud technologies
Familiarity with ETL tools like Informatica, Ab Initio, Hortonworks, Zookeeper, and Oozie is a plus
Exposure to cloud technologies
Disclaimer

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status, or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory, or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant - http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini."
76,Data Engineer,Deloitte,"Arlington, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
77,Data Engineer,Carta Healthcare,"Seattle, WA","Posted by
Muhammad Qasim
Recruiting A-Listers | Remote Talent Acquisition Specialist | Technical Recruiter | International Hiring | Recruiting Across 6 Time Zones | Recruiting for Tech
Send InMail
At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare
We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.
Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.
You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.
Responsibilities:
Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.
What you’ll need:
Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends
Why we love Carta Healthcare, and why you will too!
Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
78,Data Engineer,FORME,"San Francisco, CA","Job Description

We are looking for a highly motivated software engineer with a strong emphasis on Machine learning and data mining experience. As a member of the Data Engineering team, you will have significant responsibility to help build large scale cloud-based data and analytics platforms which will power FORME's Recommendation engine, data analytics and strength algorithms to better predict & recommend exercise content. This role is inherently multi-functional, and the ideal candidate will work across teams (Product, Device engineering and cloud teams). The position requires ability to own things, come up with innovative solutions, try new tools, technologies, and entrepreneurial approaches.

Responsibilities:
Build fault tolerant, scalable, quality data pipelines using cloud- based tools.
Innovative solutions to help broader organizations take crucial actions fast and efficiently.
Be a data steward. Develop deep organization wide data understanding.
Chip in to data engineering frameworks, tools, and processes
Implement discipline around data operations and implement best practices to use resources in an optimum way.
Architect data ingestion, data transformation, data consumption, data governance frameworks.
Work in a collaborative environment and contribute to the team as well as the organization's success.
Experience:
BS in Computer Science with good work experience. MS preferable.
4+ years of experience as a data engineer.
3+ years proven ability in distributed data technologies e.g., Hadoop, Hive, Presto, Spark etc.
2+ years of experience with Cloud based technologies - Databricks, AWS EMR, S3, Azure Blob Storage, Notebooks etc. Familiarity and usage of different file formats in batch/streaming processing i.e., Delta/Parquet/Avro/ORC etc.
Best in class SQL experience. Ability to write sophisticated SQLs across platforms.
Proven hands - on experience in Python/PySpark/Scala and ability to manipulate data using Pandas, NumPy, Koalas etc. and using APIs to transfer data.
Experience working as an architect to design large scale distributed data platforms.
Experience with development tools such as Jenkins, GitHub etc.
Working experience with Open- source orchestration tools i.e., Apache Airflow/ Azkaban etc.
A teammate with excellent communication/collaboration skills when it comes to closely working with data scientists and machine learning engineers daily.
Familiarity with Machine Learning Algorithms.
What we Offer:
Competitive Compensation Package
100% employer-paid health coverage* (medical, dental, vision, disability) for employees
FSA/HSA & Commuter benefits
The opportunity to make a tangible impact
The chance to design a truly holistic experience for our customers
The opportunity to be a part of a growing company, building from the ground up"
79,Data Engineer: Application Modernization,IBM,"Raleigh, NC","529272BR

Introduction

Engineers at IBM are the backbone of our strategic initiatives to design, code, test, and provide industry-leading solutions that make the world run today - planes and trains take off on time, bank transactions complete in the blink of an eye and the world remains safe because of the work our software developers do. Whether you are working on projects internally or for a client, software development is critical to the success of IBM and our clients worldwide. At IBM, you will use the latest software development tools, techniques and approaches and work with leading minds in the industry to build solutions you can be proud of.

Your Role and Responsibilities

Application modernization is a new initiative in IBM, which is transforming the way IBM works. We are creating a better developer experience for all developers within IBM. Data Engineer’s primary responsibility is to drive the transformation of IBM applications to modern database architecture, to be deployed on Hybrid Cloud. We cultivate an open, healthy, diverse, and engaging work environment where team members are continuously gaining new skills which align with individual interests.

Responsibilities May Include

Ability to analyze data and existing database design with the goal of enacting improvements geared towards making processes more efficient.
Evaluating and selecting appropriate data modernization tools to enhance our teams’ productivity and user experience.
Participate in defining CIO’s data architecture framework, standards, and principles including data management, security, and infrastructure.
Leads multi-functional teams or conducts special projects.

You will be challenged to design and develop scalable systems, leveraging a wide variety of open source and commercial technologies, closely collaborating with IBM Research and Red Hat.

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

IBM will not be providing visa sponsorship for these positions now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Required Technical and Professional Expertise

Education

A combination of education, technical training, and/or work/military experience equivalent to a bachelor’s degree in computer science, IT, etc.

General

Excellent time management and interpersonal skills
Familiarity with Agile methodologies and principles, experience in an Agile team
Understanding of DevOps practices (Continuous Integration/Delivery/Monitoring/etc.)
Willingness to work with the entire stack (from the database to containers to monitoring and alerting)
Ability to write and review code and make decisions regarding technical implementations
Growth mindset

Technical Skills

5+ years’ experience Applied Knowledge of Software Development
3+ years’ experience in DB2 or other relational databases or NoSQL databases
Knowledge of ETL practices and methodologies
Application SQL development and SQL Performance tuning, where applicable

Preferred Technical And Professional Expertise

Previous experience as a Backend or Full Stack Developer
Automated testing frameworks
Experience in working with containers and container management systems including Docker, Kubernetes and/or OpenShift
Cloud computing knowledge

About Business Unit

The Office of the Chief Information Officer (CIO) owns IBM’s IT strategy and provides the tools, workstations, devices, and infrastructure that IBMers use to do their jobs every day. Put simply, our mission is to create a productive environment for IBM's 350,000 worldwide employees. Join us as we lead with design to drive simplicity and ease of use, engineering the systems that run the business, and innovating to transform the business.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
80,"Data Engineer, Spark",Deloitte,"Boca Raton, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
81,Data Engineer,Stitch Fix,United States,"About The Team

The data engineering team is a small, nimble group of data engineers that drive the company toward clean and informative data. As a member of the data engineering team, you’ll contribute toward a clear, concise data model to help power data science, ETLs and tools to make us efficient, as well as self-service data and tools to facilitate scalable decision-making. As a team, we are driven by the thrill of helping our colleagues use data with less friction, which ultimately increases the velocity at which the business can progress!

About The Role
Individual contributor position on the data engineering team, within our Algorithms organization, working with a team that focuses on our internal business partners
You will build and own large additions to our data engineering framework, contributing to a code framework that centralizes ETL logic and definitions
You will help to define, build and maintain a clear, concise data model, especially focused on scalable analytics infrastructure
You will build scalable data engineering solutions & frameworks to solve business and data problems
You will be involved in the day-to-day operations of the team, including maintaining and improving our current tools & scripts and supporting data that powers our business
You will have autonomy to help shape the future of data engineering at Stitch Fix by bringing your ideas on improving and automating what we do and how we do it

You’re Excited About This Opportunity Because You Will...
You will work with a variety of cross functional partners including product managers, analysts and data scientists to deliver up-to-date metrics to our partners in Merchandise, Styling, CX and Operations.
You will focus on our data infrastructure, optimization, and scalability
Be part of a fast-growing team which has high visibility across the organization
Contribute ideas and direct the team’s investment to impactful directions
Contribute to a culture of technical collaboration and scalable development

We Get Excited About Candidates Who Have…
5+ years of independent and significant project experience
Experience in building out data models and data engineering capabilities
Experience coding and designing extensible and reusable Python and SQL
Experience in working autonomously and taking ownership of projects.
Ability to think globally, devising and building solutions to meet many needs rather than completing individual projects or tasks
Strong prioritization skills with business impact in mind
Familiarity with using Spark to access an S3 data warehouse
Strong cross functional communication skills that help simplify and move complex problems forward with business partners

YOU’LL LOVE WORKING AT STITCH FIX BECAUSE…
We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!
We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation
We are a technologically and data-driven business
We are committed to our clients and connected through our vision of “Transforming the way people find what they love”
We love solving problems, thinking creatively and trying new things
We believe in autonomy & taking initiative
We are challenged, developed and have meaningful impact
We take what we do seriously. We don’t take ourselves seriously
We have a smart, experienced leadership team that wants to do it right and is open to new ideas
We offer competitive compensation packages and comprehensive health benefits
You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day

About Stitch Fix

At Stitch Fix, we’re about personal styling for everybody, and we believe in both a service and a workplace where you can be your best, most authentic self. We’re the first fashion retailer to combine technology and data science with the human instinct of a Stylist to deliver a deeply personalized shopping experience. This novel juxtaposition attracts a highly diverse group of talented people who are both thinkers and doers. All of this results in a simple, powerful offering to our customers and a successful, growing business serving millions of men, women, and kids. We believe we are only scratching the surface on our opportunity, and we’re looking for incredible people like you to help us carry on that trend.

Please review Stitch Fix's Recruiting Privacy Policy here:

https://www.stitchfix.com/privacy/usrecruitingprivacy"
82,Snowflake Data Engineer - REMOTE,TEKsystems,"Minneapolis–Saint Paul, MN","TEKsystems is assisting in hiring a full time Data Engineer for a client in the windows and doors manufacturing industry.

The Snowflake implementation started last year. Foundation is set up, looking for someone that has an expertise in data ingestion and data transformation. This company is also trying to build a greenfield data product offering. Current environment is mostly in Qlik and Oracle.

Technology Environment
Qlik as reporting and data modeling - - trying to move more tool agnostic -- on the cloud
Snowflake hosted in Azure Central region
Fivetran
DBT
SFDC
Skills

Development, Data Engineering, Data Ingestion, Data Pipelines

Additional Skills & Qualifications

Snowflake Data Engineer expert who can help build a new data warehouse, set up data pipelines, and assist in building a greenfield data tool.
Ingestion Experience
Data transformation-SQL
Experience Level

Expert Level

About TEKsystems

We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.

The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law."
83,Data Engineer - Prime Machine Learning and Economics,Amazon,"Arlington, VA","Description

Are you interested in leveraging the latest AWS technologies, at-scale, to support cutting-edge Machine Learning? Are you someone who likes using big data to drive high-impact business decisions? Do you enjoy building data applications used worldwide by scientists, engineering teams, and business partners? If your answer is yes, join our team! The Prime Machine Learning and Economics team is the core science team within Amazon Prime. We build recommendation systems, simulation technology, and insight-driving data science to ensure Prime remains one of the world’s most loved membership programs. As part of our data engineering team, you will continually reinvent our data pipeline and engineering infrastructure. These systems form the backbone of our prediction and causal inference technology.

As a senior data engineer on this team, you will collaborate closely with Prime business leaders, scientists (economists, research scientists, applied scientists) and engineering leaders to build data solutions. You will leverage AWS technologies (EMR, EC2, S3, GLUE, KMS, Lambda, DynamoDB, etc.) to build novel systems and tackle challenges at scale. You will manipulate and process TB-sized data, supporting real-time access and orchestration across multiple systems. Your work will enhance our scientific models and data applications. As a consequence, you will have global impact, improving customer experiences for Prime members worldwide. As a successful candidate, you will successfully interact with both technical and business stakeholders.

Our team is unique for two reasons. First, it has a broad mandate to model customer behavior, given the reach of the Prime membership program (millions of members, world-wide). This mandate leads us to build models and data infrastructure that interact with those of other benefit teams (i.e. Prime Video, Groceries) or marketplace teams (e.g. EU, India, Japan). Second, we build cutting-edge customer-level simulations using a variety of statistical tools (ML/econometrics/causal inference). Supporting this simulation technology, at-scale and in real-time, creates interesting new data engineering challenges. These give a person in this role a head start in solving artificial intelligence challenges that will be ubiquitous 3-5 years from now.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, or a related field or 5+ years industry experience
Data modeling and ETL development experience
Data Warehousing Experience with Oracle, Redshift, Teradata, etc.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience in programming languages (, , Perl, etc.)
Query performance tuning skills using Unix profiling tools and SQL
Preferred Qualifications
MS in a technical or business field
Experience with AWS technologies including, EMR, Spark, S3 etc.
Ability to deal well with ambiguity
Strong sense of ownership, urgency, and drive
Demonstrated ability to drive operational excellence and best practices.
Excellence in technical communication with peers, partners, and non-technical cohorts
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Company - Amazon.com Services LLC

Job ID: A1488954"
84,"Data Engineer, Remote","ReedGroup, a Guardian® Company",United States,"Posted by
Susan Bearden
Recruiting & Talent Acquisition
Send InMail


Data Engineer

As our Data Engineer you will be responsible for delivery of data solutions supporting analytic and reporting efforts. Specifically, the position will focus on the creation of data pipelines to optimize the architecture, design, governance, and movement to support self-service BI as well as performing statistical analysis, correlations, etc. Responsibilities include leading, defining, designing, building, testing and maintenance of the AWS data architecture, including ingestion of data from S3 (Hadoop) across various source types – Parquet, csv, text, etc. Responsible for the architecture of the transformation and integration layers utilizing various ETL tools and Redshift to create optimal structures to support centralized usage across BU and analytics. Responsible for helping propagate knowledge across this space to other team members and help to promote standards and best practice definition.

You will:

Focus on data architecture, design, and delivery of data solutions across the AWS environment
Design, implement, and ensure data solutions are built, maintained, and updated based on established business requirements
Collaborate with leadership to provide meaningful and credible feedback on data architecture, design, and delivery
Work autonomously, in team settings and in partnership with management to review data design and solutions
Identify information needed, sources, and use tools to deliver optimal solutions per the use case
Partner with IT, Database Administrators, and business owners to ensure all data/data sources needed for reporting and analytics are defined and incorporated into the appropriate data solution
Develop, implement, communicate, and maintain automated processes adhering to the deployment and support standards
Develop data quality metrics that identify gaps and ensures compliance with standards across the enterprise
Lead analysis, estimation, planning and implementation of data solutions
Serve as a liaison with functional groups around data and BI. Lead planning and execution of multiple, simultaneous initiatives
Conduct business data analysis and design to support effective report development and business decisions
Leverage external best in class reporting solutions to support data needs
Understand the data ecosystem to support placing data in the correct infrastructure
Create the processes to show where and how data should be moved / aggregated once it is landed from source systems into the data lake environment (S3)
Create the design and models for combing data across sources for efficient query patterns from BI and analytics
Ensure governance standards are followed
Review ongoing performance of existing assets and modify if needed
Fast-track the use of Redshift and other AWS tools & services across the data lake environment

Desired Skills and Experience


You have:

Deep understanding of AWS data architecture, design, data modeling, and optimization of data solutions to support analytics and BI solutions requiring very fast query resolution
Experience designing solutions across disparate structures utilizing Redshift and other AWS tools and services
Ability to collaborate with multiple stakeholders, develop business requirements and design data solutions across the AWS environment
Problem anticipation, problem solving and issue resolution skills
Ability to influence organizational change
Strong interpersonal and communication skills with the ability to proactively build and leverage relationships internally and externally
An outstanding ability to communicate and share ideas across the organization
Ability to manage multiple tasks to deliver according to schedule and priority
Bachelor’s degree in Business, Technology or equivalent
Demonstrated successful business analysis, design and development of data solutions, or equivalent combination of education and experience in data solution architecture, design, and solution delivery
Multiple years of interaction with business and technology partners to collect and translate business needs into service deliverables
Experience with Data Visualization and Business Intelligence tools
Extensive development of data solutions across disparate sources - Hadoop, Hive, Microsoft SQL, Redshift, etc.




Location & Vaccination Requirements:

The primary location for this position is remote, with a work arrangement of home - #LI-remote
Covid – 19 vaccinations required for this position


Any offer of employment will be contingent upon the candidate’s affirmation of being Full Vaccinated prior to commencing employment in the new role. New hires will have five business days from their start date to provide proof of vaccination through our Vaccination Status Confirmation Form. “Fully Vaccinated” means that at least two weeks have elapsed since you received the recommended number of doses of any COVID-19 vaccine that is currently fully approved, or authorized for emergency use, by the Food and Drug Administration or that have been listed for emergency use by the World Health Organization.

If an applicant is unable to be vaccinated due to a medical condition or a sincerely held religious belief, practice or observance, Guardian may provide reasonable accommodations. This policy may not apply to any Guardian colleagues working in a local jurisdiction (state, county, or city) that implements a law prohibiting a private employer from requiring vaccination, unless there is an applicable vaccine mandate from the federal government that would supersede the vaccination laws of the local jurisdiction. Guardian, in its sole discretion, may modify or terminate this requirement at any time.



Our Promise:

Through skill-building, leadership development and philanthropic opportunities, we provide opportunities to build communities and grow your career, surrounded by diverse colleagues with high ethical standards




We Offer:

Meaningful and challenging work opportunities to accelerate technology and innovation in a secure and compliant way
Competitive compensation
Excellent medical, dental, supplemental health, life and vision coverage for you and your dependents with no wait period
Life and disability insurance
A great 401(k) with match
Tuition assistance, paid parental leave and backup family care
Dynamic, modern work environments that promote collaboration and creativity
Flexible time off,..."
85,Need - Data engineer + Python Developer - Remote - SG,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, ASCII Group LLC, is seeking the following. Apply via Dice today!

Hi, The following requirement is open with our client. Title Data Engineer Location Skillman NJ Rate OPEN Visa Status ANY In-Person Interview No Relevant Experience (in Yrs.) 9+ Detailed Job Description Create and maintain optimal data pipeline architecture, Assemble large, complex data sets that meet functional non-functional business requirements. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS lsquobig datarsquo technologies. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Must Have Skills Very Good in SQL and Python MS Azure Fundamentals Excellent in Communication, collaboration and Team Skill Experience in Data Engineering and Data Mining Thanks Regards Sai Gourav Email"
86,Data Engineer,Carta Healthcare,"Portland, OR","Posted by
Muhammad Qasim
Recruiting A-Listers | Remote Talent Acquisition Specialist | Technical Recruiter | International Hiring | Recruiting Across 6 Time Zones | Recruiting for Tech
Send InMail
At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare
We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.
Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.
You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.
Responsibilities:
Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.
What you’ll need:
Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends
Why we love Carta Healthcare, and why you will too!
Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
87,"Data Engineer, Spark",Deloitte,"Fort Lauderdale, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
88,Data Engineer,Deloitte,"Alexandria, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
2+ years of professional experience designing and developing real time ETL architecture
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Preferred:
5+ years of professional services and/or government consulting experience
Interest in event streaming architectures, such as Apache Kafka
Creativity and innovation - desire to learn and apply new technologies, products and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
89,Data Engineer,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Arthur Lawrence, is seeking the following. Apply via Dice today!

Arthur Lawrence is urgently looking for a Data Integration Engineer for a client in New York, NY. Kindly review the job requirements below. Your immediate application will enable us to place you successfully. Must Have 3+ years of experience as a Data Integration Engineer Skilled in SQL Databases, RDBMS, and sales compensation platforms Proven experience in CSV, SAP, Python, Java, and unique implementation Nice to Have Relevant certification Knowledge of Presto, Hive, EPI application, and WorkdayFieldGlass Contact Information Talent Advocate Stephen About Us Arthur Lawrence is a management and technology consulting firm providing enterprise-wide business transformation and business applications implementation services. Our in-depth technical knowledge and broad experience of working with world-class companies enables organizations to leverage our capabilities in developing winning strategies and cost-effective solutions. We are an UN Women Empowerment Principal Signatory and are certified from National Minority Supplier Development Council. Acknowledgements from Industry Peers Winner of Entrepreneur 360 Award (2019). IAOP Award Ranked in top 100 internationally. Arthur Lawrence ranked within the Inc 5000 twice in 2016 and 2017 as one of the fastest. Growing companies of America. Named one of the top ten fastest growing businesses in Houston in 2016. Ranked 25th in the HBJ s Fast 100 Private Companies Award in 2017. Our Seven Pillars We rely on the seven core values that we believe enable us to deliver quality for our consultants and clients Education, Integrity, Value Creation, Collaboration, Best Client, Best People and Stewardship Through strict adherence to these core values, we have achieved success beyond all documented forecasts and anticipation."
90,Data Engineer,Carta Healthcare,"Atlanta, GA","Posted by
Muhammad Qasim
Recruiting A-Listers | Remote Talent Acquisition Specialist | Technical Recruiter | International Hiring | Recruiting Across 6 Time Zones | Recruiting for Tech
Send InMail
At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare
We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.
Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.
You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.
Responsibilities:
Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.
What you’ll need:
Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends
Why we love Carta Healthcare, and why you will too!
Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
91,Spark Data Engineer,Perficient,"Jersey City, NJ","Overview

At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Spark Developer proficient in Scala and SQL We are looking for someone to be based in the US, but are flexible on specific location. Candidate will be expected to work US East Coast hours with occasional flexibility needed to work with client team based in India.

Job Overview

One of our large clients is expanding their current data footprint on the cloud to provide analytics, BI and data APIs. Majority of data will be batch processed with data validation, data quality and transformation into a multitude of data platforms such as Redshift, Postgres and Hive.

A Senior Technical Consultant is expected to be knowledgeable in two or more technologies within (a given Solutions/Practice area). The Senior Technical Consultant is expected to have strong development and programming skills in Spark with a focus on Scala/Java and other ETL development experience in the big data space. You are expected to be experienced and fluent in agile development and agile tools as well as code repositories and agile SLDC/DevOps frameworks.

You will work with architects and infrastructure teams to develop, test, deploy and troubleshoot your code as well as provide input into solutions and design of the system. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions focused on our clients' business needs.

Responsibilities
Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.
Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components
Document code artifacts and participate in developing user documentation and run books
Troubleshoot deployment to various environments and provide test support.
Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Qualifications
At least 3+ years of experience in developing large scale data processing/data storage/data distribution systems
At least 3+ years of experience on working with large Hadoop projects using Spark and Scala and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures.
Experience with ELT/ETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred.
Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics
Experience with SQL including Postgres, MySQL RDBMS platforms
Experience with Linux (RHEL or Centos preferred) environments
Experience with various IDE and code repositories as well as unit testing frameworks.
Experience with code build tools such as Maven.
Fundamental knowledge of distributed data processing systems and storage mechanisms.
Ability to produce high quality work products under pressure and within deadlines with specific references
Strong communication and collaborative skills
At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project
At least 5+ years of working with a complex Big Data environment
5+ years of experience with JIRA/GitHub/Git and other code management toolsets
Preferred Skills And Education

Bachelors’s degree in Computer Science or related field

Certification in Spark, AWS or other cloud platform

Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

Select work authorization questions to ask when applicants apply
Are you legally authorized to work in the United States?
Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"
92,Data Engineer,Robert Half,"New York, NY","Description

TITLE of JOB: Data Engineer

COMPANY: WES – World Education Services

LOCATION: Remote

Salary: $100,000

POC: Aaron Lozoya

WES is a social enterprise, non-profit organization, that evaluates international education qualifications and promotes their recognition.

As a Senior Data Engineer at World Education Service, you will be collaborating to build a robust and highly performant data platform using cutting-edge technologies. You will develop distributed services that process data in batch and real-time with a focus on scalability, data quality, and business requirements.

MUST Have Skills
Identify and implement improvements to our data ecosystem based on industry best practices
Build, refactor and maintain data pipelines that ingest data from multiple sources
Assembling large, complex sets of data that meet non-functional and functional business requirements
Build ETL Pipelines. Build and support the tools we use for monitoring data hygiene and the health of our pipelines.
Automate processes to reduce manual data entry
Ability to work with semi-structured and unstructured data
Interact with data via APIs. Knowledgeable on the creation of API endpoints
Requirements
Bachelor’s degree in Computer Science, Software Engineering or related field required or equivalent combination of industry related professional experience and education
Minimum 3 years in SQL and Python
Azure or Amazon storage solution
Experience building ETL Pipelines using code or ETL platforms
Experience with Jira and Confluence
Working knowledge on Relational Database Systems and concepts
Requirements

ETL - Extract Transform Load

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
93,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Experis, is seeking the following. Apply via Dice today!

Data Engineer At Experis Game Solutions we partner with developers and publishers of video games on all platforms to improve testing coverage using the latest technology. With more than 12 years of QA partnerships, we have shipped over 100 games and believe in testing smart and gaming hard!! We are looking for a Data Engineer that enjoys video games and has interest in the future of entertainment. Our Data teams are enthusiastic about the technology that drive innovation in the digital entertainment space and enjoy working on multiple products that entertain millions of people across the globe on any given day. Our teams utilize the resources provided to us by world-class publishers and technology leaders to build an environment that keeps skillful people happy through challenge and work-life balance. RESPONSIBILITIES Analyze game telemetry and evaluate project health. We are seeking a detail-oriented BI Analyst who thrives on a challenge. Desirable candidates will have strong technical and collaboration skills and experienced with KPI reports and ETL maintenance to provide game usage and game performance metrics to studio partners. Working with stakeholders to build out requirements for necessary data reporting and analysis Will work alongside QA to help them with insights that they seek to empower them to escalate issues as appropriate Develop ETL process to support various reports and trending analysis. Work with designers and testers to analyze game balance and test coverage issues. Work with data and gameplay engineers to ensure architecture will support requirements. Requirements Working with stakeholders to build out requirements for necessary data reporting and analysis Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for secondary datasets Proficiency using one or more programming or scripting language like Python, Scala, C, Java, JSON, or other programming languages Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline. Knowledge and expertise with SQL, JQL, and stored procedures Preferred qualifications Knowledge of AWS Infrastructure including S3, Redshift and RDS ??? A passion for gaming, experience with gaming industryprojects is preferred. Experience with product and service telemetry systems. Experience with predictive analytics We are an equal opportunity employer and value diversity, especially at our company. Our wide range of backgrounds brings diverse thinking, which, in-turn, crafts better video games through improved test coverage. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Data Engineer At Experis Game Solutions we partner with developers and publishers of video games on all platforms to improve testing coverage using the latest technology. With more than 12 years of QA partnerships, we have shipped over 100 games and believe in testing smart and gaming hard!! We are looking for a Data Engineer that enjoys video games and has interest in the future of entertainment. Our Data teams are enthusiastic about the technology that drive innovation in the digital entertainment space and enjoy working on multiple products that entertain millions of people across the globe on any given day. Our teams utilize the resources provided to us by world-class publishers and technology leaders to build an environment that keeps skillful people happy through challenge and work-life balance. RESPONSIBILITIES Analyze game telemetry and evaluate project health. We are seeking a detail-oriented BI Analyst who thrives on a challenge. Desirable candidates will have strong technical and collaboration skills and experienced with KPI reports and ETL maintenance to provide game usage and game performance metrics to studio partners. Working with stakeholders to build out requirements for necessary data reporting and analysis Will work alongside QA to help them with insights that they seek to empower them to escalate issues as appropriate Develop ETL process to support various reports and trending analysis. Work with designers and testers to analyze game balance and test coverage issues. Work with data and gameplay engineers to ensure architecture will support requirements. Requirements Working with stakeholders to build out requirements for necessary data reporting and analysis Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for secondary datasets Proficiency using one or more programming or scripting language like Python, Scala, C, Java, JSON, or other programming languages Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline. Knowledge and expertise with SQL, JQL, and stored procedures Preferred qualifications Knowledge of AWS Infrastructure including S3, Redshift and RDS ??? A passion for gaming, experience with gaming industryprojects is preferred. Experience with product and service telemetry systems. Experience with predictive analytics We are an equal opportunity employer and value diversity, especially at our company. Our wide range of backgrounds brings diverse thinking, which, in-turn, crafts better video games through improved test coverage. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
94,Data Engineer,Link Logistics Real Estate,"New York, NY","Link Logistics Real Estate (“Link”) is a leading national owner of last-mile logistics real estate designed to meet the needs of the modern supply chain. The Company, established in 2019 by Blackstone, operates the largest portfolio of logistics real estate assets located exclusively in the U.S. and serves more than 6,600 customers. The Company owns, has interests in, or has under development logistics facilities across key distribution markets in the U.S. that will represent a total of 534 million square feet (more than 460 million square feet at pro rata share) when completed. Link has the scale, geographic footprint, and logistics expertise, as well as a heightened focus on sustainability to power the supply chain of tomorrow.

Blackstone is a global leader in real estate investing. Blackstone's real estate business was founded in 1991 and has $174B of investor capital under management. The company is one of the world's largest property owners, owning and operating assets across every major geography and sector, including logistics, multifamily, and single-family housing, office, hospitality, and retail.

As a Data Engineer, your primary responsibilities will be to expand and optimize our data and data pipeline architecture. You are a data wrangler who loves to build. You will work with data analysts, data scientists, and data architects as part of the initiatives that support application development, system/platform integrations and day-to-day business. Your work connecting disparate datasets will enable phenomenal business growth, as well as supporting unique machine learning models and environmental and sustainability initiatives.

Responsibilities
Create and maintain reliable, performant data pipeline architecture: Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Assemble large, complex data sets that meet functional / non-functional business requirements
Use your expertise and creativity to proactively identify, design, and implement internal process improvements: optimizing data delivery, re-designing infrastructure for greater scalability, etc
Work with data and analytics experts to strive for greater functionality in our data systems
Create data tools for analysts and data scientists that assist them in building and optimizing our product into an innovative industry leader
Qualifications
3+ years of experience in a Data Engineer role
Expertise in Python. Spark experience a plus
Experience building and optimizing ‘big data’ data pipelines a plus
Experience with orchestration tools: Luigi, Airflow, DBT, etc
Experience with event-driven architectures
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
Experience manipulating, processing, and extracting value from large, disconnected datasets
Experience supporting and working with cross-functional teams in a dynamic environment
Technical

Subject Matter Expertise In The Following
Advanced working SQL knowledge and experience working with scalable distributed systems and databases (e.g., Snowflake, Databricks, Spark, Redshift, GBQ)
MS Azure
Software development practices like DevOps and CI/CD tool chains (i.e., Jenkins, Spinnaker, Azure DevOps, GitHub).
Familiarity with modern analytics and metadata management tools (e.g., DBT, Looker)
Continuous integration technologies, orchestration tools (e.g., Jenkins, Airflow)
Education
Bachelor's or Graduate degree in Computer Science, Informatics, Information Systems, or another quantitative field
EEO Statement

Our company is proud to be an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Our employment decisions are based on individual qualifications, job requirements and business needs without regard to race, color, marital status, sex, sexual orientation, gender identity and/or expression, age, religion, disability, citizenship status, national origin, pregnancy, veteran status and or any other legally protected characteristics. We are committed to providing reasonable accommodations, if you need an accommodation to complete the application process, please email employment@linklogistics.com

Covid Policy

Link complies with all applicable employment and other laws. At request of many of its customers and on recommendation of public health authorities, Link seeks to hire employees who are “fully vaccinated” against COVID-19, as defined by the CDC, and requires proof of vaccination status as a condition to employment subject to applicable federal, state, and local legal requirements. Applicants and employees may request an accommodation for an exemption from the vaccination requirement if available under applicable law, including for persons with disabilities, medical conditions, sincerely held religious beliefs or any other basis required under federal, state, or local law.

#LI=MB1"
95,Big Data Engineer,Dice,"Irving, TX","We are part of the USD 21 billion Mahindra Group that employs more than 200,000 people in over 100 countries. The Group operates in the key industries that drive economic growth, enjoying a leadership position in tractors, utility vehicles, after-market, and information technology and vacation ownership.

Tech Mahindra represents the connected world, offering innovative and customer-centric information technology experiences, enabling Enterprises, Associates and the Society to Rise . We are a USD 4.9 billion company with 121,840+ professionals across 90 countries, helping over 935 global customers including Fortune 500 companies. Our convergent, digital, design experiences, innovation platforms and reusable assets connect across a number of technologies to deliver tangible business value and experiences to our stakeholders. Tech Mahindra is the highest ranked Non-U.S. company in the Forbes Global Digital 100 list (2018) and in the Forbes Fab 50 companies in Asia (2018).

We are currently looking for Data Engineer for the below Job description to join our team with one of our clients.

Role Big Data Engineer

Location Irving, TX

Duration Long Term ContractPermanent

Job Description-

4 years"
96,Data Engineer (Python Spark and Data Lake),Capgemini,"New York, NY","Location: New York, NY (Hybrid)

Type: Fulltime

Job Description
8+ years of total experience.
Strong Python, Spark and Java scripting experience
Hand on experience working in the Cloud (Azure/AWS)
In depth understanding and experience working in architecture design (logical and physical design)
Strong SQL skills and database programming skills including creating views, stored procedures, triggers, implementing referential integrity, as well as designing and coding for performance.
Knowledge and hands-on experience of RDBMS systems (e.g.: Sybase, DB2, Teradata, or Oracle).
Good communication and leadership skills.
ETL experience with Informatica
Organization, discipline, detail-orientation, self-motivation, and focused on delivery.
In-depth knowledge and hands-on experience Unix/Linux programming (shell and/or Perl).
Disclaimer

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.

This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Click the following link for more information on your rights as an Applicant -

Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini."
97,Data Engineer,Amazon,"Seattle, WA","Job Summary

DESCRIPTION

Amazon’s mission is to be the most customer centric company in the world. The vision of Consumer Talent Operations is to design the ideal workforce to meet the customer promise anywhere. This organization leads and influences global workforce strategies that enable Amazon to scale operations more efficiently while also providing a unique voice for the hourly workforce. This is accomplished through a variety of science initiatives, experimentation, ML driven modeling, and data engineering. Amazon’s mission is to be the most customer-centric company in the world and we are on the front lines of that mission by providing robust research, data science and analytics to fill our jobs across the globe.

In this role, you will support a mission critical product for the business. You are responsible for prototyping and creating new metrics, creating the proper data modeling necessary to facilitate reporting (e.g., data integrity) and analytics (e.g., anomaly detection). You acquire new data sources, and build data visualizations for key stakeholders.

This role is truly a jack of all trades; it will be part process improvement, part analytics, part data modeling, and part strategic design. If you can seamlessly move across these various skillsets then you’ll thrive in this role.


Basic Qualifications
3 years of relevant work experience in a DE role
3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)

Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
Ability to turn complex problems into simple solutions
Proven ability to develop engaging customer-facing programs
Ability to self-direct, multitask, and prioritize a constantly evolving workload
Preferred Qualifications
Master's degree in computer science, engineering, mathematics, or a related technical discipline
A desire to work in a collaborative, intellectually curious environment.
7+ years of industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
The base pay range for this position in Colorado is $96,200 - $160,000 a year; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon’s internal or external careers site.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2033398"
98,Data Engineer Intern,Synchrony,"Stamford, CT","Job Description

Role Summary/Purpose:

The Cyber Fraud Detection Team proactively identifies and detects potential fraudulent accounts by monitoring for unusual patterns in customer credit application activity. A Cyber Fraud Detection Intern will gain hands-on experience working alongside analysts to assist with the development, testing and implementation of fraud detection strategies. This role will also include the research and development of solutions to improve efficiency in technical processes.

This is a summer internship opportunity - 40 hours per week for 10-12 consecutive weeks from June to August 2022. Internships will be fully remote and open to qualified students located near any Synchrony US Hub.

This role requires technical hands-on analysis and professional discretion to handle sensitive issues and investigations.

Essential Responsibilities
Assist with exploring, defining, and mapping data fields in multiple data warehouses.
Review all Synchrony credit application systems and data fields and map to data warehouse location and definitions.
Leverage machine learning and statistical modelling to build high fidelity detection, including tasks such as problem definition, data acquisition, data exploration, feature engineering, testing, and researching approaches and finally the building of production-quality code for implementation and deployment
Leverage Automation and analytics to develop fraud hunting capabilities to detect ring activity
Assist in developing the efforts to create, test, and implement use cases in Splunk
Interface with fellow team members, colleagues in fraud, physical security, business partners, management, vendors, and external parties on best practices
Ensure Information Security tools and processes meet regulatory requirements
Engage with external and internal parties to understand data from different sources
Present results of analysis to team and other parties (external and internal)
Perform other duties and/or special projects as assigned
Qualifications/Requirements
Must be an undergraduate student enrolled in an accredited college or university and have completed your sophomore or junior year
Demonstrable interest and capabilities in computer science, information systems, software engineering, electrical engineering or information assurance —technical majors as well as mathematics, economics, and liberal arts are applicable
Excellent academic record – Minimum overall 3.0 GPA or higher
Experience with APIs, risk mitigation, data science and data engineering is desirable
Must be able to work 40 hours per week for 10-12 consecutive weeks from June - August 2022.
Desired Characteristics
Splunk, Elasticsearch or SIEM/Log based tool experience
Ability to perform logical problem solving.
Possess basic understanding of enterprise grade technologies including operating systems, databases and web applications.
Ability to read and understand basic system data including data warehouse fields, system logs, application logs, and device logs.
Possess personal and professional integrity.
Good oral and written communication skills.
Basic network infrastructure knowledge (e.g. router, switch, firewall).
Security best practices configuration knowledge.
Programming experience (SAS, Python, Ruby, etc)
Eligibility Requirements
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the onboarding process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
New hires (Level 4-7) must have 9 months of continuous service with the company before they are eligible to post on other roles. Once this new hire time in position requirement is met, the associate will have a minimum 6 months’ time in position before they can post for future non-exempt roles. Employees, level 8 or greater, must have at least 24 months’ time in position before they can post. All internal employees must consistently meet performance expectations and have approval from your manager to post (or the approval of your manager and HR if you don’t meet the time in position or performance expectations).
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.

Reasonable Accommodation Notice
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am – 5pm Monday to Friday, Central Standard Time.
Grade/Level: 07

Job Family Group

Information Technology"
99,Spark Data Engineer,Perficient,"New York, NY","Overview

At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Spark Developer proficient in Scala and SQL We are looking for someone to be based in the US, but are flexible on specific location. Candidate will be expected to work US East Coast hours with occasional flexibility needed to work with client team based in India.

Job Overview

One of our large clients is expanding their current data footprint on the cloud to provide analytics, BI and data APIs. Majority of data will be batch processed with data validation, data quality and transformation into a multitude of data platforms such as Redshift, Postgres and Hive.

A Senior Technical Consultant is expected to be knowledgeable in two or more technologies within (a given Solutions/Practice area). The Senior Technical Consultant is expected to have strong development and programming skills in Spark with a focus on Scala/Java and other ETL development experience in the big data space. You are expected to be experienced and fluent in agile development and agile tools as well as code repositories and agile SLDC/DevOps frameworks.

You will work with architects and infrastructure teams to develop, test, deploy and troubleshoot your code as well as provide input into solutions and design of the system. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions focused on our clients' business needs.

Responsibilities
Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.
Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components
Document code artifacts and participate in developing user documentation and run books
Troubleshoot deployment to various environments and provide test support.
Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Qualifications
At least 3+ years of experience in developing large scale data processing/data storage/data distribution systems
At least 3+ years of experience on working with large Hadoop projects using Spark and Scala and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures.
Experience with ELT/ETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred.
Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics
Experience with SQL including Postgres, MySQL RDBMS platforms
Experience with Linux (RHEL or Centos preferred) environments
Experience with various IDE and code repositories as well as unit testing frameworks.
Experience with code build tools such as Maven.
Fundamental knowledge of distributed data processing systems and storage mechanisms.
Ability to produce high quality work products under pressure and within deadlines with specific references
Strong communication and collaborative skills
At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project
At least 5+ years of working with a complex Big Data environment
5+ years of experience with JIRA/GitHub/Git and other code management toolsets
Preferred Skills And Education

Bachelors’s degree in Computer Science or related field

Certification in Spark, AWS or other cloud platform

Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

Select work authorization questions to ask when applicants apply
Are you legally authorized to work in the United States?
Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"
100,Data Engineer - Prime Machine Learning and Economics,Amazon,"Arlington, VA","Description

Are you interested in leveraging the latest AWS technologies, at-scale, to support cutting-edge Machine Learning? Are you someone who likes using big data to drive high-impact business decisions? Do you enjoy building data applications that impact 100's of millions of customers? If your answer is yes, join our team! The Prime Machine Learning and Economics team is the core science team within Amazon Prime. We build recommendation systems, simulation technology, and insight-driving data science to ensure Prime remains one of the world’s most loved membership programs. As part of our data engineering team, you will continually reinvent our data pipeline and engineering infrastructure. These systems form the backbone of our prediction and causal inference technology.

As a data engineer on this team, you will collaborate closely with Prime business leaders, scientists (economists, research scientists, applied scientists) and engineering leaders to build data solutions. You will leverage AWS technologies (EMR, EC2, S3, GLUE, KMS, Lambda, DynamoDB, etc.) to build novel systems and tackle challenges at scale. You will manipulate and process TB-sized data, supporting real-time access and orchestration across multiple systems. Your work will enhance our scientific models and data applications. As a consequence, you will have global impact, improving customer experiences for Prime members worldwide. As a successful candidate, you will successfully interact with both technical and business stakeholders.

Our team is unique for two reasons. First, it has a broad mandate to model customer behavior, given the reach of the Prime membership program (millions of members, world-wide). This mandate leads us to build models and data infrastructure that interact with those of other benefit teams (i.e. Prime Video, Groceries) or marketplace teams (e.g. EU, India, Japan). Second, we build cutting-edge customer-level simulations using a variety of statistical tools (ML/econometrics/causal inference). Supporting this simulation technology, at-scale and in real-time, creates interesting new data engineering challenges. These give a person in this role a head start in solving artificial intelligence challenges that will be ubiquitous 3-5 years from now.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
MS in a technical or business field
4+ years of progressively responsible work experience
PySpark/, , Shell Scripting (bash, zsh, etc), git
Experience in building large scale distributed data pipelines
Ability to take a project from scoping requirements through actual launch of the project
Experience with AWS technologies including, EMR, Spark, S3 etc.
Ability to deal well with ambiguity
Strong sense of ownership, urgency, and drive
Demonstrated ability to drive operational excellence and best practices.
Excellence in technical communication with peers, partners, and non-technical cohorts
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Company - Amazon.com Services LLC

Job ID: A1645656"
101,"Data Engineer, Spark",Deloitte,"Austin, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
102,"Data Engineer, Advertiser Behavior Analytics",Amazon,"New York, NY","Job Summary

DESCRIPTION

Are you interested in shaping the future of Advertising and B2B Marketing? We are a growing machine learning science and engineering team with an exciting charter and need your passion, innovative thinking, and creativity to help take our products to new heights.

Amazon Advertising operates at the intersection of eCommerce and advertising, offering a rich array of digital advertising solutions with the goal of helping our customers find and discover anything they want to buy. We help advertisers reach Amazon customers on Amazon owned and operated sites and on other high quality sites across the web. We start with the customer and work backwards in everything we do, including advertising. If you’re interested in joining a rapidly growing team working to build a unique, world-class advertising product with a relentless focus on the customer, you’ve come to the right place.

A strategic part of our charter is to build a portfolio of self-service, cost-per-click advertising programs to enable both large and small advertisers to engage with customers in relevant ways. Core to these programs is our understanding of our advertisers and marketing programs from a robust analytical perspective. Teams are dedicated to diving deep into telemetry and performance data to drive insights and future marketing strategy to grow our self-service advertising programs among advertisers.

Key job responsibilities

As a contributor to the identification of advertiser insights and recommendations, you will:
take on projects and make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps).
build data pipelines to feed machine learning models for real-time and large-scale offline use cases.
work closely with Data and Applied scientists to scale model training and explore new data sources and model features.
work closely with Software Development Engineers to support offline data needs.
identify project ideas with stakeholders (e.g., analysts, scientists), and engineer peers.
help balance customer requirements with team requirements and help your team evolve by actively participating in the code review process, design discussions, team planning, and ticket/metric reviews.
focus on operational excellence, constructively identifying problems and proposing solutions
train peers about how team data solutions are constructed, how they operate, how secure they are, and how they fit into the bigger picture.
About The Team

The Advertiser Behavior Analytics (ABA) Team owns a set of Machine Learning models and services that identify and recommend key strategic opportunities for the advertisers on Amazon, and that prioritize Amazon’s scaled and 1:1 marketing activities. We inspire our advertising customers by highlighting the value they can derive from using key advertising product features, and provide them with education and support to get the most out of them. We are a dynamic team of economists, business intelligence engineers, data engineers, scientists, and software developers with a charter that influences every part of Amazon’s advertising community.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience working with AWS big data technologies (EMR, Redshift, S3)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Ability to quickly adapt to changing priorities and generate innovative solutions in an extremely fast-paced environment.
Preferred Qualifications
Excellent written and oral communication skills
Basic scripting in Python, R, Hive or Pig
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1803924"
103,Data Engineer I,ACLU,"New York, NY","About The Job

The ACLU seeks applicants for the full-time position of Data Engineer I in the Analytics Department of the ACLU’s National office in New York, NY/ Remote*.

ACLU Analytics partners with teams across the organization to enable the ACLU to make smart, evidence-based decisions and bring quantitative insights on our issues to the courtroom and the public. Our team's work ranges from social science research for litigation & advocacy, to analysis & reporting for fundraising and engagement, to building and maintaining our data infrastructure. We strive to ensure the ACLU leads by example in the ethical use of data and technology. This includes maintaining our privacy and security standards, pushing for transparent data practices from government and corporate actors, and helping to steward high standards for algorithmic fairness, accountability, and transparency.

Reporting to the Director of Data Infrastructure, the Data Engineer will be focused on reliably transforming data in the ACLU’s internal systems into analysis-ready forms, extracting data from public datasets (e.g. court dockets) and working to improve our overall data and analysis pipeline. Data engineers will work in teams alongside data scientists, data analysts, and business users, to enable the use of innovative data-driven methods and advanced analytics to transform the way the ACLU approaches fundraising, digital communications, and supporter engagement, as well as program work in litigation and advocacy.
Note: this position may be approved for remote work from a different U.S. location
Responsibilities
Use transformation tools like Python, DBT, and Looker to maintain a well-tested analytical layer (e.g., modeling donor data from multiple sources into an event stream)
Work directly with the Engagement Analytics team to build business logic into our fundraising data warehouse
Use and mentor others on the team use data warehousing and SQL best practices to keep our analytics swift and reliable
Advise other ACLU engineering teams such as Web and CRM on architecture decisions that impact data quality
Support our Legal Analytics team to provide data support for fast developing legal crises
Collaborate with leadership of the ACLU Data Infrastructure team to architect complex data pipelines, plan work, and set team policy
Communicate complicated technical solutions to members of the ACLU in a friendly and easy to understand way
Engage in special projects and other duties as assigned
Center principles of equity, inclusion, and belonging in all work, embedding the values in program development, policy application, and organizational practices and processes
Commitment to the mission of the ACLU
Demonstrate a commitment to diversity within the office using a personal approach that values all individuals and respects differences in regards to race, ethnicity, age, gender identity and expression, sexual orientation, religion, disability and socio-economic circumstance
Commitment to work collaboratively and respectfully toward resolving obstacles and/or conflicts
Preferred Qualifications
Knowledge of latest trends and technologies in data infrastructure and data warehousing
Knowledge of common software engineering best practices (e.g. interacting with file system, using version control)
Familiarity with AWS services such as S3, IAM, EC2
Knowledge of Looker and Lookml
Knowledge of CRM technologies such as Salesforce
Compensation

The annual salary for this position is $91,237 (Level H). This salary is reflective of a position based in New York, NY. This salary will be subject to a locality adjustment (according to a specific city and state), if an authorization is granted to work outside of the location listed in this posting. Note that most of the salaries listed on our job postings reflect New York, NY salaries, where our National offices are headquartered.

About The Aclu

The ACLU dares to create a more perfect union – beyond one person, party, or side. Our mission is to realize this promise of the United States Constitution for all and expand the reach of its guarantees.

For over 100 years, the ACLU has worked to defend and preserve the individual rights and liberties guaranteed by the Constitution and laws of the United States. Whether it’s ending mass incarceration, achieving full equality for the LGBTQ+ community, establishing new privacy protections for our digital age, or preserving the right to vote or the right to have an abortion, the ACLU takes up the toughest civil liberties cases and issues to defend all people from government abuse and overreach.

Equity, diversity, and inclusion are core values of the ACLU and central to our work to advance liberty, equality, and justice for all. We are a community committed to learning and growth, humility and grace, transparency and accountability. We believe in a collective responsibility to create a culture of belonging for all people within our organization – one that respects and embraces difference; treats everyone equitably; and empowers our colleagues to do the best work possible. We are as committed to anti-oppression and anti-racism internally as we are externally. Because whether we’re in the courts or in the office, we believe ‘We the People’ means all of us.

The ACLU is an equal opportunity employer. We value a diverse workforce and an inclusive culture. The ACLU encourages applications from all qualified individuals without regard to race, color, religion, gender, sexual orientation, gender identity or expression, age, national origin, marital status, citizenship, disability, veteran status and record of arrest or conviction, or any other characteristic protected by applicable law. Black people, Indigenous people, people of color; lesbian, gay, bisexual, transgender, queer, and intersex people; women; people with disabilities, protected veterans, and formerly incarcerated individuals are all strongly encouraged to apply.

The ACLU makes every effort to assure that its recruitment and employment provide all qualified persons, including persons with disabilities, with full opportunities for employment in all positions.

The ACLU is committed to providing reasonable accommodation to individuals with disabilities. If you are a qualified individual with a disability and need assistance applying online, please email benefits.hrdept@aclu.org. If you are selected for an interview, you will receive additional information regarding how to request an accommodation for the interview process.

The Department of Education has determined that employment in this position at the ACLU does not qualify for the Public Service Loan Forgiveness Program."
104,Senior Data Engineer,Loom,United States,"Loom is the video communication platform for async work that helps companies communicate better at scale. Loom makes it easy to record quick videos of your screen and camera and instantly share them with a link. More than 14M users across more than 200k companies around the world trust Loom to share feedback, updates, intros, training, and more – every day. Founded in late 2015, Loom has raised $203M from world-class investors including Andreessen Horowitz, Sequoia, Kleiner Perkins, Iconic, and Coatue.

The Role

We’re looking to add a Senior Data Engineer to our growing Data team to help us build the data foundation that will enable data science and analytics to derive insights and build models that will unlock the next stage of Loom’s growth.

You will be collaborating with the data team, engineering team & stakeholders across the company to build the foundation of our data infrastructure. You will take ownership of the transformation layer from raw data to the models that drive all critical analysis and dashboards across the organization. We are looking for a SQL wizard trained in the arts of performant & well-documented query writing, a Python expert, someone who puts data quality first, and someone that can translate business problems into technical models that drive actionable insight.
We are currently hiring remotely in the United States and Canada
Responsibilities
Help build data foundation, data pipelines and ETL that enable the creation of insightful automated dashboards and data visualization to track key business metrics.
Partner with data, engineering and business teams to translate business problems into technical projects and infrastructure that drives business results.
Drive table design and architecture, transformation logic and efficient query development to support the growing needs of the data organization.
Automate analyses and data pipelines while building scalable data infrastructure, to enable self-serve dashboards and automation of product experimentation results.
Create and manage Data Integrations between the Snowflake Data Warehouse and 3rd party tools such as Salesforce, Zendesk, Intercom, etc.
Develop testing and monitoring across the transformation layer to ensure data quality from raw sources and all models downstream.
Build out documentation that supports code maintainability and ultimately a Data Dictionary that makes data accessible to the whole company.
What We're Looking For
5+ years experience in a data engineering or data science role
Strong communication skills to work with stakeholders to translate business needs and ideas into tractable technical work items.
Proficiency in SQL and database table design - able to write structured and efficient queries on large data sets.
Experience with ETL tools such as Airflow or DBT.
Proficiency in Python and/or R
Experience working in the command line and in git workflows.
How We Work

Freedom and Flexibility: At Loom, we believe work is an act, not a place. When we disengage work from a location, we can accommodate a broad range of life choices. You can choose to work from home or while you travel. If you are in the Bay Area, you can drop by our San Francisco office on 2nd and Market St. We are able to hire across the United States and Canada and are continuing to expand our international presence. And you are free to move within a country without any adjustment to compensation.

Engaging Workday: The key to an engaging workday is finding the right balance between different ways to work sync, async, text and video. Loom makes it easier for us to say what we mean without having to schedule a zoom meeting or struggle to find the right words. And using async communication allows us to free up time for deep work every day. We believe there's a place for sync time - often a conversation on Zoom is exactly what is needed. For team meetings, this ‘sync time’ is typically between 9AM-Noon PT given where the overlap of timezones where Loommates are located.

Work-Rest Balance: The way we balance the fast-paced demands of a high-growth startup and sustainability is making rest a priority. We offer a flexible PTO policy so you're able to take time off when you need it. We also go fully offline once a year between Christmas and New Year (and twice during the Pandemic). Rest is also part of the workday, not just during PTO. Taking a break for personal commitments, whether it's watching the kids or getting some exercise is a natural part of the workday.

Social Connection: Relationships and connectedness matter. We are intentional about building trust and relationships through unique, shared experiences. Our virtual workspace keeps us connected day-to-day whether it's through Looms celebrating wins or our buzzing Slack communities. Loommates are able to meet in person with their teams at least twice a year purely for fun. We have also built local offices for those who want to work or meet in person, starting with San Francisco and New York City.

Perks at Loom
Competitive compensation and equity package
99% company paid medical, dental, and vision coverage for employees and dependents (for US employees)
Flexible Spending Account (FSA) and Dependent Care Flexible Spending Account (DCFSA)
Healthcare reimbursement (for International employees)
Life, AD&D, Short and Long Term Disability Insurance
401(k) with 4% company matching
Professional development reimbursement
Wellness stipends
Unlimited PTO
Paid parental leave
Remote work opportunities
Home office & technology reimbursement
Loom = Equal Opportunity Employer

We are actively seeking to create a diverse work environment because teams are stronger with different perspectives and experiences.

We value a diverse workplace and encourage women, people of color, LGBTQIA individuals, people with disabilities, members of ethnic minorities, foreign-born residents, older members of society, and others from minority groups and diverse backgrounds to apply. We do not discriminate on the basis of race, gender, religion, color, national origin, sexual orientation, age, marital status, veteran status, or disability status. All employees and contractors of Loom are responsible for maintaining a work culture free from discrimination and harassment by treating others with kindness and respect."
105,Cloud Data Engineer,JPMorgan Chase & Co.,"Jersey City, NJ","As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Solid background in Java Development
Expertise with Kafka
Experience with Spark and Cloud (AWS, Azure or Google etc)
Advanced knowledge of application, data, and infrastructure architecture disciplines
Understanding of architecture and design across all systems
Working proficiency in developmental toolsets
Ability to work in large, collaborative teams to achieve organizational goals
Proficiency in one or more modern programming languages
Understanding of software skills such as business analysis, development, maintenance, and software improvement
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
106,Data Engineer,Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
4 to 6 years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
107,Junior Data Engineer,The Coca-Cola Company,"Atlanta, GA","Job Description

The Coca-Cola Company’s IT organization is in the midst of a digital transformation that allows our employees to use world-class technology to connect our products to our customers all over the world. This journey is a very exciting time for Coca-Cola and our employees are big contributors to our Success and Growth. Our large, scale and complex environment offers an incredible opportunity to address challenges, enable innovative solutions to make a difference for our customers, leveraging technology such as AWS, Salesforce, GitHub, and many more

To continue this digitalization, we are looking for a Software Engineer to join our dynamic team in Latin America to help build scalable, cutting-edge technologies for today and tomorrow. This position is part of a global network with a predominance of interaction with North America that uses deep technical skills to create and improve software applications that help the Coca-Cola Company grow and be more productive If you are up for challenges, eager to learn, and highly motivated, we’d love to speak with you!

Function Related Activities/Key Responsibilities
Lead from the front lines to define, design, and implement high-quality solutions.
Design, develop, test, document, deploy, monitor, and support code
Be accountable for Production Issues by supporting the developers in short-term and long-term resolutions in respect to Coca-Cola Service Level Agreement.
Participate in a scrum team continuously delivering incremental high-quality code to production
Connect with other teams to integrate products
Provide visibility over plans and status to multiple stakeholders (IT)
Required Experience
Bachelor’s degree in Computer Science or related field
1-year experience (or educational equivalent) as a Full Stack Software Engineer with experience in creating elegant, efficient, and testable code.
Solid understanding of Git-based version control
Solid understanding of object-oriented programming
Exposure to two of the following languages: Java, Python, NodeJS
Experience developing applications using HTML/CSS, JavaScript (preferably ReactJS framework)
Knowledge of REST APIs and web services
Knowledge of approaches to accessing databases (SQL – MySQL, SQL Server, Oracle and/or NoSQL – MongoDB, DynamoDB, CosmosDB)
Experience using Amazon Web Services (AWS)
Effective Communication both in Spanish and English (Oral and Writing)
Can connect beyond Technical speak
Preferred Experience
Understand DevOps best practices
Experience using CI/CD tools such as Bitbucket, GitHub, GoCD, Bamboo, Terraform
Experience with infrastructure – servers, networking
Experience with security best practices
Experience working in an Agile Scrum / DevOps development methodology
Amazon Web Services (AWS) Certifications
Experience working in a multinational, distributed team, with in-house and external delivery resources.
Proficiency in Jira, or similar Agile life cycle management tools
Experience in Production support space with dealing with the pressure of SLA, Short Term resolution situation
Our Purpose And Growth Culture

We are taking deliberate action to nurture an inclusive culture that is grounded in our company purpose, to refresh the world and make a difference. We act with a growth mindset, take an expansive approach to what’s possible and believe in continuous learning to improve our business and ourselves. We focus on four key behaviors - curious, empowered, inclusive and agile - and value how we work as much as what we achieve. We believe that our culture is one of the reasons our company continues to thrive after 130+ years. Visit Our Purpose and Vision to learn more about these behaviors and how you can bring them to life in your next role at Coca-Cola.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class. When we collect your personal information as part of a job application or offer of employment, we do so in accordance with industry standards and best practices and in compliance with applicable privacy laws.

R-70297"
108,Junior Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

We are looking for experienced Data Engineers to build and deliver innovative, game-changing mission-driven data pipelines. On this project, you will be responsible for leading the architecture and setup of hosted data lakes, as well as the ingestion pipeline and processing for large datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Travel up to 10%
1+ years of experience with extract, transform, and load (ETL) methods and tools
1+ years of experience with data modeling, data warehousing, and building ETL pipelines
1+ years of experience with SQL queries and JSON objects
1+ years of experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB
Preferred:
Familiarity with microservice architectures
Interest in event streaming architectures, such as Apache Kafka
Prior professional services or federal consulting experience
Knowledge of data mining, machine learning, data visualization and statistical modeling
Ability to thrive in a fast-paced work environment with multiple stakeholders
Creativity and innovation - desire to learn and apply new technologies, products, and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
109,Data Engineer,Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
4 to 6 years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
110,Data Engineer,Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
7+ years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
111,"Data Engineer, Spark",Deloitte,"Boston, MA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
112,Data Engineer - hybrid or remote,INNOCEAN USA,"Huntington Beach, CA","INNOCEAN USA, a full-service advertising agency, is looking for a Data Engineer to join our growing team!


Department Overview
The Data Science Department works with the Hyundai Marketing clients to identify business objectives and develop insights that help optimize tactics and strategy for paid media, social media, web content, and CRM marketing. The team centralizes marketing data by managing a data warehouse; developing and implementing technical requirements for web analytics tools; managing tagging; conducting A/B and multivariate testing; producing reports and dashboards; developing models; and managing attribution. The Data Science team also works closely with other agencies and third-party vendors to extend its capabilities and produce more holistic views of the client’s marketing efforts.

Position Overview
We are currently searching for a Database Engineer who will be responsible for understanding functional and business requirements, define solutions, identify ideal technology solutions, and build/deploy technical solutions that satisfy business needs.
This person must have advanced skills in major skill categories such as Marketing Analytics, Application Development, and ETL.

Our primary languages are Python and Postgres SQL, and we make heavy use of AWS platform and services. We use Databricks as the ETL tool and use Python as the language of choice.

With success in client contact and intra-team communications, this person will fulfill a key role within the Data Science & Analytics team. In time, this person will have the opportunity to mentor junior teammates.

Key Responsibilities
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects


Requirements
The ideal candidate will have a strong combination of skills that include but are not limited to:
4+ years’ experience with database development including sophisticated SQL coding techniques (Postgres is preferred)
University degree, ideally in the field of computer science, engineering or another rigorous technical discipline.
Core Technology Skills:
Additional relevant certifications (for example, on AWS) are a plus.
Knowledge of relevant data and analytics tools such as:
Oracle Siebel CRM Systems
Analytics: Adobe Analytics or Google Analytics and IHS Polk MyAnalytics
Databases: SQL, Redshift, MongoDB, Dynamo
BI tools: Tableau Desktop/Scheduler, DOMO, PowerBI or similar software
ETL: Databricks, Spark, Alteryx
Statistical tools: R Studio, SAS, Python

Benefits
3 PPO Medical Plans to choose from low to no cost
Free Dental and Vision
Matching 401K
Wellness reimbursement program
Company paid short- and long-term disability, and basic life
Generous PTO


INNOCEAN USA’s top priority is the health and safety of our employees, their families, and the communities where they live and work. As part of our commitment to health and safety, we require all employees to be fully vaccinated against COVID-19. INNOCEAN USA offers a reasonable accommodation process for individuals who are unable to meet our vaccination requirement due to a disability, medical condition or sincerely held religious belief. INNOCEAN’s ability to make reasonable accommodation will vary based on the role in question."
113,Data Engineer,Amazon,"Trenton, NJ","Description

Are you ready to change Prime? Join our team of data professionals!

Are you somebody that likes to use big data to drive business decisions? Do you enjoy building data applications used worldwide by business leaders? Do you want to be part of the data team which measures the pulse of Prime and drives change? If your answer is yes, join our team. We are the analytical arm of Prime globally. We work with country leaders, product managers, marketing and finance to ensure Prime remains one of the most loved membership programs.

What is Prime? It’s a membership program including over 150 million members in more than 18 countries. Prime offers some of the best shopping and entertainment experiences with Amazon. Prime members enjoy Prime FREE One-Day Shipping and Prime FREE Same-Day Delivery in more than 8,000 cities and towns, two-hour delivery with Prime Now in more than 30 major cities and unlimited Free Two-Day Shipping on more than 100 million items. In addition, in the U.S., members enjoy unlimited access to award-winning movies and TV entertainment with Prime Video; unlimited access to Prime Music, Prime Reading, Amazon Photos, Twitch Prime; early access to select Lightning Deals, Whole Foods Market discounts and more.

As part of the global Prime Analytics team, we build scalable data solutions to enable insight generation in addition to providing analytical support for flagship initiatives within Amazon. Example data solutions include 1) campaign analyzer, a tool which allows business leaders to A/B test product launches and marketing campaigns and 2) variance analyzer, a tool which enables country leaders to examine the movement of any business KPI along multiple dimensions and determine rate/mix effects. Recent analytical projects include impact measurement of 1 day shipping introduction, analysis of nation-wide brand campaigns and investigation of usage patterns to recommend products and campaigns to increase customer satisfaction.

As a Data Engineer in this team, you will collaborate closely with business leads and product managers working on improving Prime member satisfaction. You will work on creating analytical solutions, which help pinpoint, predict and surface customer pain points. Your analytical solutions will be used across the globe to ensure customer satisfaction with Prime continuously improves. As a successful candidate, you will be able to ingest and analyze complex data sets to help drive actionable insight, enable automation of insight where possible and ensure scalability, reliability, accuracy and performance of Prime analytical solutions.


Basic Qualifications
Bachelor’s degree in a technical field
Proficiency in at least one modern programming language such as , , or .
Proficiency in manipulating big data and building analytical solutions
SQL/ETL expertise
3+ years of relevant work experience
Preferred Qualifications
MS in a technical or business field
5+ years of progressively responsible work experience
Experience building interactive, scalable data solutions
Experience in building large scale distributed data processing pipelines
Ability to take a project from scoping requirements through actual launch of the project
Experience with AWS technologies including Cradle, EMR, Spark, S3 etc.
Ability to deal well with ambiguity
Strong sense of ownership, urgency, and drive
Demonstrated ability to drive operational excellence and best practices.
Excellence in technical communication with peers, partners, and non-technical cohorts
“ Ability to work on a diverse team or with a diverse range of coworkers”
“Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us

We believe passionately that employing a diverse workforce is central to our success and we make recruiting decisions based on your experience and skills. We welcome applications from all members of society irrespective of age, gender, disability, sexual orientation, race, religion or belief.”


Company - Amazon.com Services LLC

Job ID: A1413009"
114,AWS Data Engineer,Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
115,Data Engineer - Snowflake,Deloitte,"Miami, FL","Are you an experienced, passionate pioneer in technology? A cloud solutions builder who wants to work in a collaborative environment. As an experienced DATA ENGINEER - SNOWFLAKE, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

Essential Functions:
Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Indeed's Data Warehouse as well as data model changes that align with warehouse standards and backfill or other warehouse data management processes
Develop and execute testing strategies to ensure high quality warehouse data
Provide documentation, training, and consulting for data warehouse users
Perform requirement and data analysis in order to support warehouse project definition
Provide input and feedback to support continuous improvement in team processes
The Team

The US Cloud Engineering Offering focuses on enabling our client's end-to-end journey from On-Premise to Cloud, with opportunities in the areas of Cloud Strategy and Op Model Transformation, Cloud Development & Integration, Cloud Migration, and Cloud Infrastructure & Managed Services. Cloud Engineering supports our clients as they improve agility, resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited Sponsorship: Limited immigration sponsorship may be available"
116,"Data Engineer, Consumer Security Strategies",Amazon,"New York, NY","Job Summary

DESCRIPTION

Amazon.com strives to be Earth's most customer-centric company where people can discover virtually anything they want to buy online. The world's brightest minds come to Amazon.com to research and develop technology that improves the lives of shoppers, developers, publishers, and sellers around the world. Amazon's vision is to offer the Earth's largest selection and lowest prices to our customers.

Our Consumer Security teams span over ten countries worldwide, and our focus areas include security intelligence, application security, incident response, security operations, risk and compliance, acquisitions and subsidiaries, and external partner security.

We are looking for Data Engineers with broad technical skills to support our analytics team to deliver on strategic security projects, produce strategic insights, and define/produce end-to-end metrics that enable security and business decisions.

In this role, you will join a a team that delivers insights and analytics to elevate our data and security metrics to tell the the story of how our Consumer Security teams are doing. You will help build a data management and reporting infrastructure that enables our leaders to leverage data to look around corners to solve complex business and security challenges across a range of data types and domains. You will help enable the Consumer Security team's data insights strategy and deliverables, understanding customer needs, and work backwards from there.

As a Data Engineer You Will
Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions with a flexible and adaptable data architecture.
Contribute to the architecture and implementation of next generation data and BI solutions from the ground up.
Manage AWS resources including Redshift, S3, EC2, Athena, Glue, etc.
Optimize automated processes for performance and fault tolerance.
Optimize support for ad-hoc analysis across various data sources
Collaborate with BI Engineers, Software Engineers, and cross-functionally to deliver high quality data architecture and pipelines.
Make large and/or complex data more accessible, understandable and usable by applications and teams.
This position may be located in Austin TX, Arlington VA, New York or Seattle, with other locations possible. Relocation available.

What makes this role different than all the others out there? Simple: scope. You get the opportunity to work with every part of our Consumer Security team plus external business stakeholders. You will be working on a large number of complex migration projects and be given a lot of independence. This is a new focus area for Consumer Security, so you have the opportunity to put your stamp on it.


Basic Qualifications
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
5+ years of experience in Data Engineering, BI Engineer, or related field in Architecting and developing end to end scalable data applications and data pipelines.
4+ years of hands on experience in building big data solution using EMR/Elastic Search/Redshift or equivalent MPP database.
2+ years of coding experience with modern programming or scripting language (Python, Scala, Java).
2+ years of experience in advanced SQL and query performance tuning skills.
Database design and administration experience with one RDBMS, such as MS SQL Server, PostgreSQL, MySQL, etc.
Participate in data strategy and road map exercises, data warehouse design and implementation.
Develop strong collaborative relationships with key partners in data engineering, business intelligence, software development, finance, modelling and product teams.
Demonstrated strength and experience in data modelling, ETL development and data warehousing concepts.
Hands-on experience in working with any reporting/visualization tools in the Industry.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testing.
Knowledge of distributed systems as it pertains to data storage & computing.
Preferred Qualifications
Meets/exceeds Amazon’s leadership principles requirements for this role.
Meets/exceeds Amazon’s functional/technical depth and complexity for this role.
5 to 7 years of experience as a Data Engineer, BI Engineer, or related field in a company with large, complex data sources.
Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Kinesis and Lambda) or equivalent industry tools.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience on working with different SQL/NOSQL databases.
Knowledge on data lake platform and difference technologies used in data lake to retrieve and process the data.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2032426"
117,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Blue Rose Technologies LLC, is seeking the following. Apply via Dice today!

Data Engineer Full Time Remote Experience with SQL and Python Experience building and deploying ETL pipelines and data modeling, and databases Experience with orchestration tools (Airflow, Luigi, etc) Thorough understanding of data technologies such as Spark, Kafka, and Kafka Connect Familiarity with AWS services such as S3, Redshift, EKS Kubernetes, and Personalize"
118,Data Engineer,Snap Finance,"Salt Lake City, UT","Snap Finance is a rapidly growing FinTech company located in Salt Lake City, Utah, focused on digital disruption in the $1+ trillion financial services industry. Our technology platform and machine-learning algorithms are changing the face and pace of consumer retail finance. Snap has a strong, supportive culture and is dedicated to its customers, merchant partners, and team members. We are looking for someone who is hard-working and enjoys an environment that is mission- and results-driven.


*This position is hybrid, however not 100% remote. If outside of Utah, relocation is expected. *


The Job


We are looking for a Data Engineer who will participate in the expansion and organization of our operational and analytical data pipelines. Should support our developers, data scientists, business intelligence analysts, and machine learning engineers in ensuring consistent, accurate data delivery. We expect this Data engineer to deliver high quality code after unit testing and automated tests meets expected results. Able to debug existing and new pipelines and fix production issues.


Key Responsibilities:



Design, build and implement new data models

Implement new data pipelines and support production.

Work with stakeholders to assist with data-related technical issues and support their data needs

Establish systems for monitoring data quality and generating alerts.

Involve in technical, data catalog documentation


You


Required Skills:



SQL and relational databases such as PostgreSQL

Programming using Python, Java, and/or Scala

Source control systems: git, GitHub

Data warehouse systems such as Hive, Redshift, Snowflake, Databricks

Column-oriented data formats such as Parquet, ORC

Automated workflows and CI/CD tools: Airflow, Jenkins, etc.

Amazon Web Services: S3, EC2, ECR, ELB, RDS, etc.

Understanding of Data assets partitioning, indexing, sort order

Pipeline design incremental vs. entire dataset

Knowledge of latest distributed computing like Apache Spark


Education and Background Required:



B.S. (Computer Science or other STEM field) combined with robust experience will be considered, M.S. preferred

3-5 years of experience


Personal Characteristics:



Strong analytical and problem-solving skills

Excellent communication skills with the ability to clearly explain technical topics to a non-technical audience

Team-oriented but able to complete tasks independently at a high standard

Structured, organized, and detail-oriented

Proactive, enthusiastic, and flexible

Fluency in English, both in oral and written form

Ability to take projects from conceptualization to implementation

Must be able to work onsite in our Salt Lake City office and be legal to work in the United States


Why Youll Love It Here



Unlimited PTO

Competitive medical, dental & vision coverage

401K with company match

Company-paid life insurance

Company-paid short-term and long-term disability

Legal coverage and other supplemental options

Pet insurance, free snacks, and fun events

A value-based culture where growth opportunities are endless


More


Snap values diversity, and all qualified applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.


Learn more by visiting our website at www.snapfinance.com


California Residents please review our California Consumer Privacy Act Notice at https://snapfinance.com/ccpa-notice





PM21"
119,Data Engineer / Analyst,GSK,"Warren, NJ","Site Name: USA - New Jersey - Warren

Posted Date: Apr 6 2022

Are you energized by managing a team dedicated to the development of data management systems? If so, this opportunity could be the perfect fit for your career.

As Data Engineering Manager, you will manage all aspects of the data systems such as data sourcing, migration, quality, design, and implementation. You will also assist with planning and execution of departmental goals.

These Responsibilities Include Some Of The Following

This role will provide YOU the opportunity to lead key activities to progress YOUR career.
Lead and develop sustainable data driven solutions with current new gen data technologies to meet the needs of our organization and business customers
Build and run forums to train resources across the enterprise
Champion rollout of industry frameworks across the company (e.g. continuous integration)
Contribute to strategic roadmap for a technical domain
Handle highly ambiguous situations
Proactively sponsor process and technology improvements
Provide technical guidance to team members
Act as subject matter expert for a technical domain
Develop frameworks that are used by multiple teams and applications
Participate in external speaking engagements
Manage team of resources (both full-time associates and/or third party resources)
Why you?

Basic Qualifications

We are looking for professionals with these required skills to achieve our goals:
Bachelor's degree in Computer Science, Mathematics, other relevant discipline, or equivalent practical experience.
15+ years’ total experience with at least 5 years Senior Engineering experience.
Preferred Qualifications

If you have the following characteristics, it would be a plus:
Master’s Degree in Computer Science, Information Systems, Engineering, or related field
Strong problem-solving skills
Familiarity with Azure DW/Synapse
Familiarity with Machine Learning, Data Science & AI Technologies.
Experience building and operating GxP-regulated technology in Healthcare
Experience managing containers in Azure Kubernetes Service
5+ years utilizing Agile Product Management- Scrum and SAFe methodologies
Data Bricks / Spark / Azure Data Lake Analytics
IaC / Automated delivery technology such as Azure DevOps, Terraform, Ansible, etc.
Why GSK?

Our values and expectations are at the heart of everything we do and form an important part of our culture. These include Patient focus, Transparency, Respect, Integrity along with Courage, Accountability, Development, and Teamwork. As GSK focuses on our values and expectations and a culture of innovation, performance, and trust, the successful candidate will demonstrate the following capabilities.
Agile and distributed decision-making – using evidence and applying judgement to balance pace, rigour and risk
Managing individual and team performance.
Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.
Implementing change initiatives and leading change.
Sustaining energy and well-being, building resilience in teams.
Continuously looking for opportunities to learn, build skills and share learning both internally and externally.
Developing people and building a talent pipeline.
Translating strategy into action - a compelling narrative, motivating others, setting objectives and delegation.
Building strong relationships and collaboration, managing trusted stakeholder relationships internally and externally.
Budgeting and forecasting, commercial and financial acumen.
If you require an accommodation or other assistance to apply for a job at GSK/ViiV Healthcare, please contact your local HR department.

GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.

If you require an accommodation or other assistance to apply for a job at GSK, please contact the GSK Service Centre at 1-877-694-7547 (US Toll Free) or +1 801 567 5155 (outside US).

GSK is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.

At GSK, the health and safety of our employees are of paramount importance. As a science-led healthcare company on a mission to get ahead of disease together, we believe that supporting vaccination against COVID-19 is the single best thing we can do in the US to ensure the health and safety of our employees, complementary workers, workplaces, customers, consumers, communities, and the patients we serve.

GSK has made the decision to require all US employees to be fully vaccinated against COVID-19, where allowed by state or local law and where vaccine supply is readily available. The only exceptions to this requirement are employees who are approved for an accommodation for religious, medical or disability-related reasons.

Important notice to Employment businesses/ Agencies

GSK does not accept referrals from employment businesses and/or employment agencies in respect of the vacancies posted on this site. All employment businesses/agencies are required to contact GSK's commercial and general procurement/human resources department to obtain prior written authorization before referring any candidates to GSK. The obtaining of prior written authorization is a condition precedent to any agreement (verbal or written) between the employment business/ agency and GSK. In the absence of such written authorization being obtained any actions undertaken by the employment business/agency shall be deemed to have been performed without the consent or contractual agreement of GSK. GSK shall therefore not be liable for any fees arising from such actions or any fees arising from any referrals by employment businesses/agencies in respect of the vacancies posted on this site.

Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, GSK may be required to capture and report expenses GSK incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure GSK’s compliance to all federal and state US Transparency requirements. For more information, please visit GSK’s Transparency Reporting For the Record site."
120,Data Engineer,Carta Healthcare,"Los Angeles, CA","Posted by
Muhammad Qasim
Recruiting A-Listers | Remote Talent Acquisition Specialist | Technical Recruiter | International Hiring | Recruiting Across 6 Time Zones | Recruiting for Tech
Send InMail
At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare
We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.
Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.
You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.
Responsibilities:
Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.
What you’ll need:
Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends
Why we love Carta Healthcare, and why you will too!
Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
121,Data Engineer /Spark Developer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Expedite Technology Solutions, is seeking the following. Apply via Dice today!

Remote Duration Long Term Job Description Need Data Engineer who can write APIrsquos in Spark using Java Must have ndash Spark Thanks Regards Prashant Kumar 678.509.7221"
122,Securities Lending Data Engineer,Morgan Stanley,"New York, NY","Position Description: Data Engineer

Morgan Stanley’s Securities Lending business is looking for a senior Data Engineer (DE) to help design,

manage, and build out our data analytics infrastructure. The role is within the Desk Strat (Data Science) group that is

embedded within the Sales & Trading desk. The team’s main responsibility is to use data and analytical solutions to

optimize trading, price products, and drive the trading and sales decisions.

In this role you’ll be shaping how the future business will operate by helping guide and build our data platform. You

will help manage all the business’s data infrastructure and build data pipelines, dashboards, and tools that enable

multidisciplinary users (e.g. Strats, Traders, Sales, Technologies, and others) to effectively use data in growing the

business. You will be solving our most challenging data integration problems, utilizing optimal ETL patterns,

frameworks, query techniques, sourcing from structured and unstructured data sources. Additionally you will assist

in owning existing processes running in production, optimizing complex code through advanced algorithmic

concepts.

You will also have the opportunity to learn about and gain experience in other areas of the Strats team including

portfolio optimization, pricing, client analytics, and much more. This is a global role and you will need to

understand the needs of different consumers in many regions.

Key Responsibilities
Own and enhance our current data platform
Work cross-functionally with upstream teams to guide and consult them on how to structure their databases
to expose good data APIs, and with downstream Strat and Tech teams to expose well-designed and well-

documented data models and metrics
Build clean software, systems abstractions, and understand that modular reusable components are key to
manageably supporting thousands of pipelines
Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and
operational cost-benefit tradeoffs within systems
Demonstrate strength in data management fundamentals and data storage principles
Mentor team members by giving/receiving actionable feedback
Required Qualifications
At least 3 years of experience in the following areas:
Data warehouse and data analytics capabilities on big-data architecture such as Hadoop
Custom ETL design, implementation and maintenance
Schema design and dimensional data modeling
Strong Python or Scala or Java backend development skills
Prior experience with an OLAP and/or columnar databases
Extensive hands on experience with Airflow or Luigi for building complex data pipelines
Excellent communication, organization, and presentation skills
Preferred Qualifications
Experience with Snowflake
Knowledge implementing Apache Spark and Spark Architecture
Applied Machine Learning experience
Financial experience desired but not necessary
This role requires that all successful applicants be fully vaccinated against COVID-19 as a condition of employment and provide proof of such vaccinations within 3 days of commencement of employment.

Posting Date

Feb 16, 2022

Primary Location

Americas-United States of America-New York-New York

Job

Investment Banking/Sales/Trading/Research

Employment Type

Full Time

Job Level

Associate"
123,"Data Engineer, Spark",Deloitte,"Costa Mesa, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
124,Data Engineer with Python,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Pyramid Consulting, Inc., is seeking the following. Apply via Dice today!

Immediate need for a talented Data Engineer with Python with experience in the Healthcare Industry. This is a 12 Months contract opportunity with long-term potential and is located in Remote Please review the job description below. Key Responsibilities Need 9 Years of experience in Project Management Good Team Player Key Requirements and Technology Experience 9-10 years of professional experience in software development with ETL ,Python, pySpark and Pandas Familiarity with the basic principles of distributed computing and data modeling Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures Working experience with Airflow and Snowflake Data Analysis, schema design and dimensional data modeling, Data lake, Data importexport approaches, tooling, and data quality issues 2+ years of experience on cloud platforms (AWS preferred) Highly motivated with outstanding communication and interpersonal skills Our client is a leading Healthcare Industry and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration. Regards, Ravindra (Pyramid Consulting INC.) Cell"
125,Data Engineer - Base Compute Data Analytics,JPMorgan Chase & Co.,"Plano, TX","We are looking for a savvy Data Engineer to join our growing team of platform engineers. We are building analytical tools and data products enabling observability of our cloud infrastructure. You will be responsible for building data solutions from ground up-including new data pipelines to support business intelligence, enabling new valuable machine learning driven insights and observability--and optimizing data flow and collection from cross functional teams.

The ideal candidate is a data pipeline builder, with a modern skillset, and data wrangler who enjoys building scalable data systems from the ground up. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or re-designing data architecture to support the delivery of value to our internal customers with next generation of products and data initiatives

Qualifications
Working knowledge and experience working with a variety of data sources including relational databases, NoSQL stores, data API's--familiarity with graphQL and graph database engines is a plus
Experience building and optimizing 'big data' data pipelines, architectures and datasets
1+ year working with containerization technology (Docker, Kubernetes, etc) and cloud infrastructure providers
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Experience building processes supporting data transformation, data structures, metadata, dependency, quality controls, and workload management
A successful history of sourcing, manipulating, processing, and extracting value from large disconnected datasets in a scalable manner
Working knowledge of message queuing, stream processing, and 'big data' data stores
Strong customer focus mindset and a team player
Experience supporting and working with cross-functional teams in a dynamic environment
We are looking for a candidate with 3+ years of experience in a Software/Data Engineer role, preferably one who has also attained Bachelor level degree in Computer Science, Statistics, Informatics, Data Science, Information Systems or another equivalent field
They Should Also Have Experience Using The Following Software/tools
Experience with big data tools: Hadoop, Spark, Kafka, etc
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Glue, etc
Experience with at least one object-oriented/object function scripting languages: Java or Python
Some experience with Data Science and Machine Learning workflows is a big plus
Responsibilities
Assemble large, complex data sets that meet functional / non-functional business requirements
Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using internal data reservoirs, AWS 'big data' technologies, automating them via Airflow---and containerizing these solutions for scalability
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Assist in creating data tools for analytics and data scientist team members that assist them in building and optimizing our internal products to improve our ability to deliver new ones
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
126,"Data Engineer, Spark",Deloitte,"New Orleans, LA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
127,FinTech Data Engineer,Amazon,"Jersey City, NJ","Job Summary

DESCRIPTION

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access? If so, Amazon Finance Technology (FinTech) is for you!

FinTech is seeking a Data Engineer to join the team consistent of software engineers and data engineers, and shaping the future of the finance data platform. We are committed to building the data stores, and enabling machine learning-based Forecasting applications and Risk Management capabilities for Amazon's rapidly growing and dynamic businesses. Our data platform is used to power data-driven decision-making in Finance and Risk Management through a timely, accurate, and actionable manner.

As a Data Engineer, you should be an expert with technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of large data-sets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The candidate is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring data-sets together to answer business questions and drive change.

Ideal candidates thrive in this fast-paced environment and relish working with large volumes, enjoy the challenge of highly complex business contexts, and, is a passionate about data and analytics. In this role you will be part of a team of engineers and supporting Amazon's expanding global footprint.

Key job responsibilities
Design, implement, and support a platform providing secured access to large datasets.
Interface with customers or business stakeholders, gathering requirements and delivering complete data solutions.
Model data and metadata to support ad-hoc and pre-built reporting.
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Tune application and query performance using profiling tools and SQL.
Analyze and solve problems at their root, stepping back to understand the broader context.
Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
Triage possible courses of action in a high-ambiguity environment, and adopt quantitative analysis and business judgment when making design decisions.

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in Computer Science or related technical field.
Hands-on and excellent experience in SQL
5+ years of work experience with ETL, Data Modeling, and Data Architecture.
3+ years experience using at least one of the Big Data technologies (Parquet, Spark, Hadoop, Presto, EMR, etc.)
Proficiency in at least one modern programming language such as Scala, or Python
Preferred Qualifications
Master’s degree in Computer Science or a related field.
Experience in distributed data storage and computing
Excellent understanding of software development life cycle and/or agile development environment.
Strong problem-solving skills and ability to prioritize conflicting requirements.
Experience using business intelligence reporting tools (e.g. Tableau, QuickSight).
Excellent written and verbal communication skills and ability to succinctly summarize key findings.
Experience working with Amazon Web Services or other Big Data Technologies.
Strong organizational and multitasking skills with ability to balance multiple priorities.
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space.
An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1826910"
128,Senior Data Engineer - Remote Opportunity,Pearson,United States,"Senior Data Engineer

This is a Hybrid Role - Must be able to go into office in Denver, CO or Raleigh, NC

Do you want to be a part of big data in education? Pearson – the world’s leading learning company – has an exciting opportunity for a mid-level Data Engineer on the forefront of big data collection and curation.

Imagine all the interactions that a learner, educator or administrator has every day. Multiply that by the millions of learners, educators, and administrators around the world. All those interactions and events generate data. Now you can begin to understand the scale of data at Pearson.

You can have an impact on learners around the world by contributing to the curated data eco-system that is the foundation for a wide range of vital platform and business processes here at Pearson.

The Digital Analytics Cloud (DAC) Data team is the heart of Pearson’s data environment. As a member of this team, you’ll be responsible for ingesting, transforming, and curating data in our data lake. We use multiple technologies including AWS Snowflake and Google BigQuery. Our data lake has over 300 terabytes of data generated by application platforms, user interactions, and system monitors. Our consumers include applications, data science, machine learning, monitors, dashboards and reports.

Responsibilities:
Design, code, unit test, and deploy data processes for ingestion, transformation, or curation of big data while keeping data security and privacy in mind.
Actively participate in requirements and design meetings.
Enjoy a healthy technical debate but know how to collaboratively solve problems that move everyone forward.
Communicate well in-person, over email, and via instant messaging both within the team and with other development teams.
Work efficiently and collaboratively in an Agile (Scrum or Kanban) team environment.
Directly and meaningfully impact the experience of students, institutions, professors and administrators.
Be able to organize your workload based on team priorities.
Be willing to learn new skills whether they are directly tied to software development, software testing, server maintenance, or release and delivery.
Qualifications
The ideal candidate will be detail-oriented, self-directed, self-motivated, with a strong capacity for working successfully and flexibly with members across the organization.
2+ years of data engineering or ELT experience.
Experience and practice with one or more of the following technologies:
Snowflake
AWS Redshift, DMS or, Kinesis
Google BigQuery
Databricks
Commercial or open source ETL / ETL tools
Experience and practice with SQL.
Experience or interest working in cloud-based environments preferred.
Experience or interest in Python, Java or Scala.
Experience with automated deployments a plus.
Solid foundation in computer science, with experience in OO design and development and multiple data structures.
Experience with data modeling a plus.
Easy to work with, stays confident and optimistic in face of challenges.
Strong communication skills and the ability to communicate technical concepts to non-technical people.
As required by the Colorado Equal Pay Transparency Act, Pearson provides a reasonable range of minimum compensation for roles that may be hired in Colorado. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location. For the state of Colorado only , the range of starting pay for this role is $91,000 - $140,000 and information on benefits offered is here.

Benefits available to eligible employees can be seen at: https://pearsonbenefitsus.com/

Pearson is an Affirmative Action and Equal Opportunity Employer and a member of E-Verify. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be. All employment is decided based on qualifications, merit, and business need. All qualified applicants will receive consideration for employment without regard to race, ethnicity, color, religion, sex, sexual orientation, gender identity, gender expression, age, national origin, protected veteran status, disability status, or any other group protected by law.

Learning is the most powerful force for change in the world. More than 20,000 Pearson employees deliver our products and services in nearly 200 countries, all working towards a common purpose – to help everyone achieve their potential through learning. We do that by providing high quality, digital content and learning experiences, as well as assessments and qualifications that help people build their skills and grow with the world around them. We are the world’s leading learning company. Learn more at pearsonplc.com.

Pearson believes that wherever learning flourishes, so do people. We are committed to being an anti-racist company in everything we do. We value the power of an inclusive culture and a strong sense of belonging. We promote a culture where differences are embraced, opportunities are accessible, consideration and respect are the norm, and all individuals are supported in reaching their full potential. Through our talent, we believe that diversity, equity, and inclusion make us a more innovative and vibrant place to work. People are at the center, and we are committed to a sustainable environment and workplace where talent can learn, grow, and thrive.

To learn more about Pearson’s commitment to a diverse and inclusive workforce, please click here:  http://www.pearson.com/careers/diversity-and-inclusion.html

Pearson is an Affirmative Action and Equal Opportunity Employer and a member of E-Verify. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be. All employment is decided based on qualifications, merit, and business need. All qualified applicants will receive consideration for employment without regard to race, ethnicity, color, religion, sex, sexual orientation, gender identity, gender expression, age, national origin, protected veteran status, disability status, or any other group protected by law.

Job: TECHNOLOGY

Organization: Corporate Strategy & Technology

Schedule: FULLTIME

Req ID: 2767"
129,"Data Engineer, Analytics",Meta,"New York, NY","Every month, billions of people leverage Meta products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products both internally and externally by delivering the best data foundation that drives impact through informed decision making. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for our 3 billion plus users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match. As we continue to expand and create, we have a lot of exciting work ahead of us!

Data Engineer, Analytics Responsibilities:

Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way
Define and manage SLA for all data sets in allocated areas of ownership
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources
Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Influence product and cross-functional teams to identify data opportunities to drive impact
Mentor team members by giving/receiving actionable feedback

Minimum Qualifications:

Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
4+ years of work experience in data engineering (a minimum of 2+ years with a Ph.D)
Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)

Preferred Qualifications:

Master's or Ph.D degree in a STEM field
Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy
Experience working with terabyte to petabyte scale data

Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com."
130,Data Engineer,UBS,"New York, NY","Job Reference #
242133BR

Job Type
Full Time

Your role
UBS Evidence Lab is looking for a data engineer to join its Engineering Team. The team's focus is building scalable frameworks and services used to analyze large datasets in the context of financial analysis and alternative data. They leverage well-established engineering processes and techniques pioneered by giants in the tech industry. Projects span general pipelining to building internal web tools to helping develop low-latency query and analysis engines.

Responsibilities:

Develop highly scalable data processing pipelines

Develop data query and analysis services to be leveraged by clients, internally and externally

Maintain a high standard of code quality within the broader engineering team

Adapt the latest data processing and infrastructure techniques to our growing stack

Your team
You'll be working in the Engineering team in New York. No background in finance is required, so this is a great opportunity to get exposure to the finance industry. Think of the team as a well-funded engineering startup within an investment bank.

Diversity helps us grow, together. That’s why we are committed to fostering and advancing diversity, equity, and inclusion. It strengthens our business and brings value to our clients.

Your expertise
Minimum Qualifications:

Established track record in designing, building, deploying, and maintaining scalable systems

Second-nature knowledge of algorithms and data structures

Knowledge developing and debugging Python

Preferred Qualifications:

Formal education in Computer Science or related quantitative field

Experience with distributed systems

Experience with Apache Spark

Experience with Apache Airflow

Experience with Microsoft Azure

About Us
UBS is the world’s largest and only truly global wealth manager. We operate through four business divisions: Global Wealth Management, Personal & Corporate Banking, Asset Management and the Investment Bank. Our global reach and the breadth of our expertise set us apart from our competitors.

With more than 70,000 employees, we have a presence in all major financial centers in more than 50 countries. Do you want to be one of us?

Join us
From gaining new experiences in different roles to acquiring fresh knowledge and skills, at UBS we know that great work is never done alone. We know that it's our people, with their unique backgrounds, skills, experience levels and interests, who drive our ongoing success. Together we’re more than ourselves.

Ready to be part of #teamUBS and make an impact?

Disclaimer / Policy Statements
UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce.

Your Career Comeback
We are open to applications from career returners. Find out more about our program on ubs.com/careercomeback."
131,Data Engineer - Snowflake,Deloitte,"Austin, TX","Are you an experienced, passionate pioneer in technology? A cloud solutions builder who wants to work in a collaborative environment. As an experienced DATA ENGINEER - SNOWFLAKE, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

Essential Functions:
Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Indeed's Data Warehouse as well as data model changes that align with warehouse standards and backfill or other warehouse data management processes
Develop and execute testing strategies to ensure high quality warehouse data
Provide documentation, training, and consulting for data warehouse users
Perform requirement and data analysis in order to support warehouse project definition
Provide input and feedback to support continuous improvement in team processes
The Team

The US Cloud Engineering Offering focuses on enabling our client's end-to-end journey from On-Premise to Cloud, with opportunities in the areas of Cloud Strategy and Op Model Transformation, Cloud Development & Integration, Cloud Migration, and Cloud Infrastructure & Managed Services. Cloud Engineering supports our clients as they improve agility, resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited Sponsorship: Limited immigration sponsorship may be available"
132,Data Engineer (Mid-Level) (The Atlantic),The Atlantic,"New York, NY","The Atlantic has, for more than 160 years, advanced ideas that matter and sparked global conversation on the most important issues of our time. We aim to bring clarity and original thinking to questions of consequence, on topics ranging from politics, the economy, and global affairs to technology, science, and culture. As the third-longest-running magazine in America, we find ourselves at a remarkable moment: one of both continuation and transformation, of upholding our legacy while continuously reinventing ourselves for the future.

The Atlantic is seeking a Mid-Level Data Engineer to help design and build the infrastructure that powers the company's mission to use data to drive engagement with our journalism and advertising and to inform company-wide strategy. Data Engineering at the Atlantic is committed to building a secure and scalable modern data stack that makes it possible for data stakeholders at the Atlantic to analyze, build models, and to deliver insights on user and advertising data.

As a member of our Data Engineering team you will:
Architect, implement and optimize data pipelines that sync data between our data warehouse and third-party platforms. Transform data in our warehouse to actionable insights.
Iterate and expand upon the Atlantic’s modern data stack. The team currently uses Airflow, AWS, BigQuery, Github Actions, Fivetran, dbt, and Looker.
Closely collaborate with the Data Science team to transfer software engineering knowledge for optimizing queries and to run machine learning models in production.
Influence the team’s direction through daily stand-ups, and biweekly sprint planning and retrospectives at the agile team level and up.
Deliver code reviews with helpful and meaningful feedback for your teammates.
Partner with our legal team to ensure our data pipelines and warehouse comply with regulatory requirements.
Promote a data-driven culture by working with teams to access and understand their data in addition to incorporating their data to support the Atlantic’s business goals.
As a Data Engineer
You have a broad set of skills: You are knowledgeable about software engineering best practices including systems design, testing, and documentation in addition to data engineering skills, such as data pipelines, workflow orchestrators, data modeling, and streaming data. You are interested in gaining a deeper understanding of data governance and data quality.
You are a strong communicator: You can collaborate closely with other engineers-as well as data scientists and marketers on a daily basis. You can write clear and concise documentation.
You are a critical thinker: You don't need to be told how to get from point A to point B; you feel comfortable tackling well-defined tasks with little to no supervision. You are up for the challenge of figuring out how the pieces of complex data and business logic fit together.
You have a team-first mentality: You care deeply about the people you work with and are excited to contribute to others' success.
The Atlantic Monthly Group LLC and its affiliates (""The Atlantic"") are Equal Opportunity Employers. We do not discriminate against our applicants because of race, color, religion, sex (including gender identity, sexual orientation, and pregnancy), national origin, age, disability, veteran status, genetic information, or any other status protected by applicable law."
133,"Data Engineer, Amp",Amazon,"Culver City, CA","Job Summary

DESCRIPTION

Want to build the future of live audio?

Join Amazon’s newest mobile app, Amp, where anyone can host live radio shows with the music they love. Currently in beta in the US, Amp’s evolving how people discover and share music. Create a show, call-in, or bop along to some tunes – no subscription, additional hardware, or editing needed.

As part of an agile team, you will be working in a startup atmosphere while being able to leverage the resources of a Fortune-500 company. Amp is on the bleeding-edge of consumer-facing products, where every team member is a critical voice in the decision-making process. We build systems that will be distributed around the world, spanning mobile apps and voice-forward experiences on Amazon Echo devices, powered by Alexa. Amp products support our mission of delivering audio entertainment in new and exciting ways that creators and listeners will love.

The Amp team is looking for team members across a variety of functions, including software engineering/development, product, marketing, design, and more. Come make history, as we expand and launch Amp for millions of listeners.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Experience with Python
Experience with Spark
Preferred Qualifications
Masters's degree in Math, Operations Research, Statistics or related discipline
3+ years of experience as a Data Analyst, Business Analyst, Business Intelligence Engineer
Proficiency with data querying or modeling technique with SQL or Python
Experience with AWS services (EMR, Lambda, Redshift, Kinesis)
Experience with automated self-service reporting tools
Ability to work cross-functionally and with technical and non-technical senior level staff
Ability to operate successfully and independently in a fast-paced environment
Comfortable with ambiguity and curious to learn new skills
About Amazon Music

Amazon Music reimagines music listening by enabling customers to unlock millions of songs and thousands of curated playlists and stations with their voice. Amazon Music provides unlimited access to new releases and classic hits across iOS and Android mobile devices, PC, Mac, Echo, and Alexa-enabled devices including Fire TV and more. With Amazon Music, Prime members have access to ad-free listening of 2 million songs at no additional cost to their membership. Listeners can also enjoy the premium subscription service, Amazon Music Unlimited, which provides access to more than 75 million songs and the latest new releases. Amazon Music Unlimited customers also now have access to the highest-quality listening experience available, with more than 75 million songs available in High Definition (HD), more than 7 million songs in Ultra HD, and a growing catalog of spatial audio. Customers also have free access to an ad-supported selection of top playlists and stations on Amazon Music. All Amazon Music tiers now offer a wide selection of podcasts at no additional cost, and live streaming in partnership with Twitch. Engaging with music and culture has never been more natural, simple, and fun. For more information, visit amazonmusic.com or download the Amazon Music app.

Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

#MusicJobs

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Media Venture LLC

Job ID: A1991843"
134,Associate Data Engineer,MissionWired,United States,"At MissionWired, we help clients create revolutionary digital strategies that advance their mission, change our country, and have a positive impact on the world.

We’re digital-obsessed, tech-savvy do-gooders who care deeply about social change. We’ve brought digital strategies to life for nonprofit organizations working around the world, including Save the Children, Sandy Hook Promise, and Friends of the Earth, as well as progressive political organizations, campaigns and candidates. This cycle, we're excited to support the DGA in flipping and protecting governorships across the country while expanding the Democratic majority in the Senate via our work with the DSCC, Sen. Raphael Warnock, Rep. Val Demings, Sen. Catherine Cortez Masto and Sen. Maggie Hassan.

We’re an equal-opportunity employer and take seriously our commitment to equality and equity. Our efforts to be inclusive and create opportunity don’t end when someone joins us – they begin.
We’ve set our sights on changing the world through our work and with our clients, and representation is at the foundation of what we do. We know that diversity of thought and background makes us stronger. That’s why we’re committed to building and maintaining a diverse community.

Every new team member broadens our perspective and allows us to think bigger. We’ll be at our best when people from underrepresented communities and people with a range of perspectives and lived experiences want to come, stay, and push the boundaries of what’s possible.

Overview: We are looking for an Associate Data Engineer to transform millions of data points into unparalleled opportunities – supporting everything from electing Democrats to combating climate change throughout the world. With us, you’ll put your skills to use for disruptive innovation that powers social good.

You’ll join a product team that is deeply devoted to inclusion, particularly in the following three areas:

Process: We create a context in which everyone is empowered to contribute.
Machine Learning: We are committed to identifying and removing bias from our algorithms.
Philanthropy: We empower diverse groups of individuals to participate in giving.

With us, you’ll put your skills to use for innovation that powers social good. We want your ideas and your leadership. Join us. Let’s go!

You will be responsible for:
Joining each and every one of your colleagues in creating an inclusive workspace;
Building data-intensive applications to extract, transform, and load massive data sets from a variety of internal, external, and public data sources;
Assembling large and complex data sets that meet project needs;
Developing processes for data mining, data modeling, and data production;
Using an array of technological languages and tools to connect systems;
Working toward constantly improving data reliability and quality;
Collaborating with cross-functional teams to support their data infrastructure needs.

Must-have qualifications:
Experience with one or more key data analytics tools: (Pandas, PySpark)
Experience extracting data from public APIs;
Ability to learn how to build and optimize data pipelines, architectures, and data sets;
Experience with SQL;
Attention to detail;
Intellectual curiosity to innovate on ways to solve data management issues;
Passion, energy, and excitement for progressive and philanthropic causes and all things digital.

Nice-to-have qualifications:
1-3 years of professional experience;
Experience building data-intensive applications that collect data from diverse sources in the service of creating high-performance algorithms and predictive models;
Experience with deep-learning models;
Experience managing data warehouses and/or data lakes;
Experience working with cross-functional teams in a dynamic environment.

Salary range for this role is $55,000 to $75,000 per year, depending on experience.

If you feel you can do the job and are excited about this opportunity but are not sure if you meet all the qualifications, consider applying anyway. We’d love to hear from you!"
135,Data Engineer [Remote],Braintrust,"New York, NY","ABOUT US:

Braintrust is the only network that gives in-demand talent all the freedom of freelance with all the benefits, community and stability of a full-time role. As the first decentralized talent network, our revolutionary Web3 model ensures the community that relies on Braintrust to find work are the same people who own and build it through the blockchain token, BTRST. So unlike other marketplaces that take 20% to 50% of talent earnings, Braintrust allows talent to keep 100% of earnings and to vote on key changes to improve the network. Braintrust is working to change the way freelance works – for good.
JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $30-$50 /hr
ESTIMATED DURATION: 40h/week - long-term, ongoing project

THE OPPORTUNITY
Web scraping
S3/AWS
Python and SQL (for dashboards)
Snowflake

Apply Now!

ABOUT THE HIRING PROCESS:

Qualified candidates will be invited to do a screening interview with the Braintrust staff. We will answer your questions about the project, and our platform. If we determine it is the right fit for both parties, we'll invite you to join the platform and create a profile to apply directly for this project.

C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
136,Data Engineer,NorthOne,"New York, NY","We’re the banking app that believes in small businesses.

Our company is looking for high performing builders to join our growing team as we enter our next phase of expansion 🚀. We are a San Francisco, New York, Portland, Toronto and remote-friendly based, venture-backed startup at the heart of the FinTech movement that is shaping the way financial services are delivered. We are backed by some of the best VCs in the world (Battery Ventures, Redpoint Ventures and more) and are growing fast.

Our mission

We eliminate the pain that small business owners face through financial management and banking. Poor financial literacy has an outsized impact on the costs and failure rates amongst small businesses, and we are on a mission to solve these problems. We’ve created a mobile, tech-powered account built specifically for startups, freelancers, and small/medium-sized businesses (SMBs) that acts as a world-class Finance Department that most small businesses could never afford. Small businesses are the bedrock of every community and the economy, and you’ll be their champion. Ready to start?

Our team

We ❤ technology. As a Data Engineer, you’ll build and maintain resilient data pipelines between NorthOne’s Service, our partners, our data warehouses and ETL infrastructure. Reporting to our Sr Engineering Manager, you’ll also benefit from our incredible advisors and experienced management team, who have helped create and scale companies like Square, McKinsey, Google, Frank and Oak, Strava, Instacart, Prodigy, Oportun, eBay and more.

What You’ll Be Doing
Build out data pipelines that are performant and reliable
Improve and maintain our current data infrastructure in AWS
Design and document table schemas
Work with the team to aid in the transfer of data and events between NorthOne and Data Partners
React quickly to data, growth, and attribution team needs by engineering durable yet flexible best-fit solutions
Requirements

Today you might be a Data Engineer, Data Architect, Analytical Engineer, Business Intelligence Engineer, or the equivalent in your company… or something that we haven’t heard of yet - we keep an open mind. The most important characteristic of our Data Engineer is your attitude. We want you to join because you don't see roadblocks, you see opportunities to be at your best. You know that with a growth mindset and great teamwork, you can figure anything out.

Skills, Qualities & Experience
You have at least 4+ years of experience within Data Engineering
You’ve built and designed user-friendly data platforms upon which data scientists and data analysts can easily and efficiently access the data they need
You’re able to scope, design and build tools for internal users to increase their efficiency ten-fold
You’re constantly improving and maintaining data platform uptime
You enjoy working closely with analysts and data scientists to deeply understand business problems and be a guiding voice in architecting the solutions
You get excited in building and maintaining robust, observable data pipelines
You have deep experience in maintaining a strong data driven culture within the company by interacting with diverse internal functions
Required
You have expertise in one or more programming languages (preferably Python)
You enjoy and have worked with AWS data infrastructure
You have strong SQL and experience with relational databases
You’re great with data visualization tools (LookerML, Tableau, etc)
Bonus points
You have experience with workflow management frameworks (eg. Step Functions, Airflow)
You’re familiar with event-driven and stream-based processing
You have experience with Apache Spark or Dask
You’ve worked with Parquet and Avro file formats
Benefits

Our mission is big and audacious, but we're assembling a team to take the challenge head on.

As a Data Engineer, , you'll be joining a team that prioritizes:
People : Our company is more than just a business. We’re a tight-knit team supporting each other on our mission to rebuild business banking. We’re really serious about mission, fit, and the people we work with. You’ll be part of a rapidly scaling team that reflects these values and keeps this place special.
Diversity : You'll find yourself in an environment that values diversity and inclusivity. We believe that a broad array of lived experiences and backgrounds are essential for creating the best possible product and company culture.
Learning and Development: We have a learning culture and provide opportunities for our employees to invest in their professional development, whether it’s perfecting their craft or learning about other areas of the business.
Leadership : You’re right in the thick of it, making critical decisions that will clear our path forward.
You’ll Receive
Top-tier health/dental benefits: We care about the people we work with and put their health first. NorthOne is proud to offer inclusive health and dental coverage.
Flexible working hours: We don't clock in and out at set times. You know when you do your best work. We celebrate accomplishments, not how many hours are spent at the office.
Unlimited paid time off: We hire talented people and know that they need time off to be at their best. Take as much time off as you need to recharge and make sure you’re working sustainably.
The latest computer equipment: We make sure you have the best equipment so you can produce great work.
Professional development budget: We support lifelong learners by covering the cost of classes, workshops, conferences and books.
Remote and in person get-togethers: Bond with your teammates over shared interests at regular get-togethers. Find like-minded people who are passionate about everything from sport and music to gaming and cooking.
Remote-friendly environment: Flexibility to work from home, office, or both! We support our teammates wherever they prefer to work within North America. We are a distributed team and do not require our employees to be located in one of our office hubs.
One hell of an adventure!
Few people have been trained for this sort of role and research shows that women and people from underrepresented groups are less likely to apply if they haven’t satisfied every requirement. If you are this close to what we’ve described, but aren’t sure, apply. Let’s figure out together if this is where you could shine.

NorthOne is proud to be an equal opportunity employer and celebrates diversity. We welcome all applicants regardless of race, colour, gender, age, religion, sexual orientation, disability status or national origin."
137,Data Engineer,Deloitte,"Austin, TX","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

The Data Engineer will join a team responsible for developing advanced analytics products; applying data visualization and statistical programming tools to enterprise data to advance and enable the key mission outcomes. In this role, they will support all phases of analytic work product development, from the identification of key business questions, through data collection and ETL, from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. This role requires special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree in STEM field
2+ years of experience with programming languages such as Python, R, SPSS, SAS, SQL
2+ years of experience with data visualization tools, such as Tableau, Qlik, PowerBI, or equivalent
2+ years of experience with ETL/ETL Pipeline, Data Warehouse Development and data modelling
Preferred:
Prior professional services or federal consulting experience
Experience with ETL, NoSQL Apache Hadoop, and cloud computing technology, especially Microsoft Azure
How You'll Grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
138,Data Engineer,Talentpair,"Sunnyvale, CA","Our CONFIDENTIAL client is looking for a Data Engineer to build the 100 TB-scale datasets that power our AI systems.
Location: 100% remote

As a Data Engineer here you will:
Extract data from standard PACS servers.
Standardize, organize, normalize and augment data acquired from multiple sources.
Extract data from databases and join it with image data acquired from the PACS servers.
Extract non-standard data from multiple source using non-standard methods (e.g. with Selenium robots).
Write and maintain applications for automating data acquisition and processing.
Help run studies on the existing data to generate reports and statistics.
De-identify data sets for specific purposes.
Generate standard and non-standard datasets impactful projects.

Must Have Experience
Python and object-oriented programming
SQL and Database knowledge
Amazon S3
Shell

Nice to Have Experience
DICOM
Javascript
Docker

Once you apply, we'll send you all the company information and additional details.

Talentpair is the 2021 RemoteTech Job Marketplace of the Year: https://talentpair.com/talentpair/job-marketplace-of-2021/"
139,Data Engineer,IBM,"Baton Rouge, LA","490531BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

The position of the Data Engineer plays a key role in the development and deployment of innovative big data platforms for advanced analytics and data processing. The Data Engineer defines and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Develops applications on Big Data and Cognitive technologies including API development. Expected to have traditional Application Development background along with knowledge of Analytics libraries, open-source Natural Language Processing, statistical and big data computing libraries. Strong technical abilities to understand, design, write and debug complex code.

The role of the Data Analyst is to work directly with the client using Pyspark,Scala, Hadoop, Hive and Postgre SQL. The Data Analyst must possess an understanding of the relational databases. The Data Analyst must also possess the skills to effectively collaborate with the client Subject Matter Experts (SMEs) to provide necessary solutions.

This position requires relocation to Louisiana within 30 days of the office reopening. This position requires up to 50% travel. This is not a permanent work from home position.

sprgg21

Required Technical and Professional Expertise

2 years + experience with JavaScript, Java, or other object-oriented programming languages.
Hands-on experience and understanding of object-oriented programming, data structures, algorithms, profiling & optimization (stacks, queues, linked lists, hash tables, trees, arrays, common algorithms, iteration and recursion, etc.)
2 years + coding challenge experiences including LeetCode and Hackerrank, etc.

Minimum 3 – 5 years relevant experience as: Should have a strong knowledge on Pyspark, Scala, Hadoop,Hive and Postgre SQL

Preferred Technical And Professional Expertise

Unless specified as a Required Skill, the following are additionally preferred but not required:

Experience with big data solutions such as Hadoop, MapReduce, Hive, Pig, Kafka, Storm etc. is a major plus.
Experience in Node.JS, Database, REST, Event Source, Web Sockets, HTML5, CSS3, RWD, JQuery is highly desirable.

About Business Unit

IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.

12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.

We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
140,"Data Engineer, Spark",Deloitte,"Newton, MA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
141,Data Engineer - Innovate on a New Platform in AWS,Amazon Web Services (AWS),"Arlington, VA","Description

Do you want to innovate on a new platform with Amazon Web Services (AWS)?

Come join our teams in AWS, you’ll help us design and deliver data engineering solutions that enables AWS to expand our operations into new data centers worldwide. Our green-field solutions will support programs in compliance and sustainability, and build a platform that will extend to first-class AWS services. You will collaborate with engineers designing data solutions at massive scale.

To succeed in this role, you have 4+ years of experience and you believe in high standards for writing SQL, building data pipelines with infrastructure as code, code quality, code reviews, testing, and operational excellence. You support your peers to deliver results in a space that operates much like a start-up, with the autonomy to build the right solution from Day One. As a DE with our team, you will collaborate with other engineers on technical direction, strategy with a career opportunity for growth that AWS can offer.

This role is located in Seattle, Portland, San Francisco, San Luis Obispo, Boston, New York City or Arlington. We have flexible work options, enabling you to work at home and in the office when the team collaborates. Our team also puts a high value on work-life balance, family-first approach. We care about your career growth and strive to assign projects based on what will help each team member grow.


Basic Qualifications
Bachelor’s Degree in Computer Science or related field, or equivalent experience
4+ years professional experience in data engineering, business intelligence, data science or related field
Experience with technologies including Big Data, EMR, EML or similar solutions
Demonstrated strength in data, development, and data warehousing - Experience in Python, Java or other similar languages
Experience with data management fundamentals and data storage principles
Experience with distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience with serverless computing, enterprise-wide systems, and AWS products
Understanding of Agile software engineering practices
Meets/exceeds Amazon’s leadership principles requirements for his role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Company - Amazon Web Services, Inc.

Job ID: A1761987"
142,DATA ENGINEER strong PYTHON Exp - REMOTE,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Advent Global Solutions, Inc., is seeking the following. Apply via Dice today!

Title Senior Data Engineer with Strong Python Location Remote (SFO CA) Duration 6 months with possible extensions Rate All Inc Visa Citizen Mode of interview Code test Skype call W2 all inclusive Responsibilities 10 years of professional experience 7+ yearsrsquo experience working in data engineering, business intelligence, or a similar role Proficiency in programming languages such as Python 3+ years of experience in ETL orchestration and workflow management tools like Airflow, flink, Oozie and Azkaban using AWSGoogle Cloud Platform Expert in Database fundamentals, SQL and distributed computing 3+ years of experience with the Distributed datasimilar ecosystem (Spark, Hive, Druid, Presto) and streaming technologies such as kafkaFlink. Experience working with Snowflake, Redshift, PostgreSQL andor other DBMS platforms Excellent communication skills and experience working with technical and non-technical teams Knowledge of reporting tools such as Tableau, superset and Looker Comfortable working in fast paced environment, self-starter and self-organizing Ability to think strategically, analyze and interpret market and consumer information Please send me your Updated word format resume with the below information for this role Full Name Contact Current Location with postal code Visa Status and Valid period ( Attach your visa n DL copies if you are not USC) LinkedIn, if so any Availability once confirmed for the project Are you fine to work Onsite Whenever client asks you to be Onsite in CA (yn) Last 4 digits SSN Skype ID Relocation Ratehr all Inc with Tax term ( Mention rate with Tax term) If on C2C please provide me your employer details with their email id and contact numbers DOB (MM DD) A Small write-up why think a idealfit for the above role 3 Professional Managerial Level Reference Certifications if any kindly mention and attach the copy Best Time and Contact number we can reach you in business working hours Highest Education details with University Name, Year of Start and Passout"
143,AWS Data Engineer 100% Remote| Market competitive Salary,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Jefferson Frank, is seeking the following. Apply via Dice today!

I have a role with Confidential, and they work in Cloud Optimization the must have skills are AWS Lambda or S3, Python and Software Engineering at least 5yrs. The salary is market value + yearly bonus! They are working with cutting edge tech stacks and there is room for growth What they are looking for is a Backend Data Engineer, that has very good experience and can talk deeply about redshift OR snowflake OR spark OR CICD OR data pipelines OR data lake along with Python, AWS serverless tech, they also need lead experience and some mentorship to Jr developers or teammates. They need to be able to talk about how the infrastructure works together and how data flows through it. The Sr. role is between180-208k Base + 8 bonus The client wants to fill this role ASAP, reach out if you are interested in moving forward in your career!"
144,Data Engineer-Supercharger,Tesla,"Fremont, CA","The Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. We are looking for someone with strong hands on experience in all layers of data Integration and analytics ! We especially need experience in using Python as an ETL tool. The Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies.

Technology We Use
Python
Informatica
SQL Server and MySQL
Vertica
Kafka
Your Role
Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
Derive an overall strategy of data management, within an established information architecture that supports the development and secure operation of existing and new information and digital services
Plan effective data storage, security, sharing and publishing within the organization
Gathers and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks
Ensures data quality and implements tools and frameworks for automating the identification of data quality issues
Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
Mentor and lead data engineers providing technical guidance and oversight
Provides ongoing support, monitoring, and maintenance of deployed products
Experience

Qualifications
Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus
Strong background with data modeling, data access, and data storage techniques
Experience with design, development, and implementation of highly scalable, high-volume software systems and components, source of truth systems for different business areas, developing and maintaining web services in an agile environment
Working experience with Kafka Streaming layer
Experience in Spark Framework on both batch and real-time data processing is a plus
Experience in Big Data Integration & Analytics is a plus
Experience in Supply Chain and Logistics data is a plus
Bachelor’s degree in Computer Science or related field or equivalent combination of industry related professional experience and education"
145,Public Cloud Data Lake Engineer,JPMorgan Chase & Co.,"New York, NY","As an experienced member of our Securities Services Technology Engineering and Architecture team, you will be working hands-on to develop big data solutions. These solutions will provide actionable insights for our internal teams and our clients. You will use your Public Cloud experience to build out large data-lakes with industry leading technologies. You will live and breathe Data Analytics and be familiar with tooling/formats such as: Trino, Iceberg, Spark, Parquet.

You do not need financial services experience to apply to this role.

As a Cloud Data Lake Engineer, You Will
Lead the engineering of the data platform in the Public Cloud using the latest industry tooling
Manage large data sets (1PB+) in different formats
Build data-analytics tooling for consumers of the data platform
We Are Looking For Someone Who Has
Experience building big data solutions using technologies such as Trino/Iceberg/Spark/Parquet/S3
Experience working with Petabyte scale data lakes.
Deep knowledge of AWS product/services and Kubernetes/container technologies and how they are best used for specific workloads.
Understanding of managing multi-vendor third party data
Real world experience in building out applications on AWS across multi-AZ, multi region and multi-cloud vendor environments.
An excellent understanding of modern engineering practices to take advantage of key benefits of Public Cloud (e.g., auto-scaling, automated environment creation)
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
146,Jr. Python Data Engineer,Dice,"Dallas, TX","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Anblicks, is seeking the following. Apply via Dice today!

Position Jr. Python Data EngineerLocation Dallas, TXDuration Full Time Looking for a Python developer with Spark experience.Databricks is strong plus."
147,Data Engineer,Angi,"New York, NY","Angi® is transforming the home services industry, creating an environment for homeowners, service professions and employees to feel right at “home.” For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for most indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks.

Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home!

The Team and Role

Angi is looking for a Data Engineer to play a key role on the Data Engineering team. The successful candidate will develop and maintain strong relationships with teammates while ensuring delivery of high quality Engineering solutions. The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

As a Data Engineer, You Will Be Responsible For
Establishing and instilling innovative practices, patterns, and toolkits to deliver enterprise-grade data assets.
Interact closely with stakeholders to determine analytics needs and translate those into efficient and scalable data processes
Partnering with passionate counterparts to deliver awesomeness and continuously evaluate the best way to deliver short-term and long-term solutions
The folks in this role are usually successful when they have experience in:
Extensive hands on experience in developing reusable data integration and streaming platforms using Python or another comparable language
Broad knowledge of data infrastructure ecosystem
Experience with modern cloud database platforms, such as Snowflake or Redshift
Strong Analytical and SQL skills with demonstrated strength in data modeling, ELT development, and data warehousing
Experience with GitLab, CI/CD workflows, AWS services, containerization (Docker), Grafana, and orchestration tools
Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teams
Compensation & Benefits
The salary band for this position ranges from 70k-170k, commensurate with experience and performance. Compensation may vary based on factors such as cost of living.
This position will be eligible for a competitive year end performance bonus & equity package.
Full medical, dental, vision package to fit your needs
Flexible vacation policy: work hard and take time when you need it
Pet discount plans & retirement plan with company match (401K)
The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world
#BI-Remote"
148,"Data Engineer, Spark",Deloitte,"Houston, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
149,Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do
Contribute to database planning and will serve as the point of contact for database related issues.
This person will design databases including modeling, diagramming, and creating scripts to create relational Database tables, and scripts to help migrate and mitigate data.
Also be expected to contribute to ETL pipelines and data scraping
The team

Deloitte's Government and Public Serv ices (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Active Secret security clearance required
2+ years of experience with SQL, ETL, and Python
2+ years of experience with Azure
Bachelor's degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Preferred:
Familiarity with JSON and XML data formats
Manage, monitor and optimize relational and non-relational data solutions
Make requested changes, updates and modifications to database structure and data
Performance tuning including queries, tables, index design, code redesign and system layout
Strategic road mapping of database environment (warehousing, infrastructure)"
150,"Data Engineer, Spark",Deloitte,"Albany, NY","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
151,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Experis, is seeking the following. Apply via Dice today!

Data Engineer At Experis Game Solutions we partner with developers and publishers of video games on all platforms to improve testing coverage using the latest technology. With more than 12 years of QA partnerships, we have shipped over 100 games and believe in testing smart and gaming hard!! We are looking for a Data Engineer that enjoys video games and has interest in the future of entertainment. Our Data teams are enthusiastic about the technology that drive innovation in the digital entertainment space and enjoy working on multiple products that entertain millions of people across the globe on any given day. Our teams utilize the resources provided to us by world-class publishers and technology leaders to build an environment that keeps skillful people happy through challenge and work-life balance. RESPONSIBILITIES Analyze game telemetry and evaluate project health. We are seeking a detail-oriented BI Analyst who thrives on a challenge. Desirable candidates will have strong technical and collaboration skills and experienced with KPI reports and ETL maintenance to provide game usage and game performance metrics to studio partners. Working with stakeholders to build out requirements for necessary data reporting and analysis Will work alongside QA to help them with insights that they seek to empower them to escalate issues as appropriate Develop ETL process to support various reports and trending analysis. Work with designers and testers to analyze game balance and test coverage issues. Work with data and gameplay engineers to ensure architecture will support requirements. Requirements Working with stakeholders to build out requirements for necessary data reporting and analysis Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for secondary datasets Proficiency using one or more programming or scripting language like Python, Scala, C, Java, JSON, or other programming languages Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline. Knowledge and expertise with SQL, JQL, and stored procedures Preferred qualifications Knowledge of AWS Infrastructure including S3, Redshift and RDS ??? A passion for gaming, experience with gaming industryprojects is preferred. Experience with product and service telemetry systems. Experience with predictive analytics We are an equal opportunity employer and value diversity, especially at our company. Our wide range of backgrounds brings diverse thinking, which, in-turn, crafts better video games through improved test coverage. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Data Engineer At Experis Game Solutions we partner with developers and publishers of video games on all platforms to improve testing coverage using the latest technology. With more than 12 years of QA partnerships, we have shipped over 100 games and believe in testing smart and gaming hard!! We are looking for a Data Engineer that enjoys video games and has interest in the future of entertainment. Our Data teams are enthusiastic about the technology that drive innovation in the digital entertainment space and enjoy working on multiple products that entertain millions of people across the globe on any given day. Our teams utilize the resources provided to us by world-class publishers and technology leaders to build an environment that keeps skillful people happy through challenge and work-life balance. RESPONSIBILITIES Analyze game telemetry and evaluate project health. We are seeking a detail-oriented BI Analyst who thrives on a challenge. Desirable candidates will have strong technical and collaboration skills and experienced with KPI reports and ETL maintenance to provide game usage and game performance metrics to studio partners. Working with stakeholders to build out requirements for necessary data reporting and analysis Will work alongside QA to help them with insights that they seek to empower them to escalate issues as appropriate Develop ETL process to support various reports and trending analysis. Work with designers and testers to analyze game balance and test coverage issues. Work with data and gameplay engineers to ensure architecture will support requirements. Requirements Working with stakeholders to build out requirements for necessary data reporting and analysis Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for secondary datasets Proficiency using one or more programming or scripting language like Python, Scala, C, Java, JSON, or other programming languages Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline. Knowledge and expertise with SQL, JQL, and stored procedures Preferred qualifications Knowledge of AWS Infrastructure including S3, Redshift and RDS ??? A passion for gaming, experience with gaming industryprojects is preferred. Experience with product and service telemetry systems. Experience with predictive analytics We are an equal opportunity employer and value diversity, especially at our company. Our wide range of backgrounds brings diverse thinking, which, in-turn, crafts better video games through improved test coverage. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
152,"Data Engineer, Predictive Metrics, Gale",Amazon,"Cambridge, MA","Job Summary

DESCRIPTION

The Data Engineer – Applied Modeling & Data Science on the spoken language understanding (SLU) analytics product team is responsible for supporting the data pipeline & engineering needs for analytics and ML products. The Data Engineer will build and optimize logical data model and data pipelines for difficult datasets in the Alexa SLU analytics product(s), accountable for ongoing data quality, efficiency, testing, and maintenance. The Data Engineer should thrive and have demonstrated success in an environment which offers ambiguously defined problems, big challenges, and quick changes. They will influence mid-size data solutions/access to dataset(s) in team architecture, advising product managers, program managers, and other engineers.

We are looking for passionate data engineers to optimize the consumption of very large data sources we require to generate unique insights. As a data engineering leader within Alexa, we look to you for design, implementation, and successful delivery of large-scale, critical, or difficult data solutions involving a significant amount of work. You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top technical professionals developing complex systems at scale and with a focus on sustained operational excellence. Where needed, you integrate your team’s data solutions with those owned by other teams. You influence your team’s technical and business strategy by making insightful contributions to team priorities and overall data approach. You take the lead in identifying and solving ambiguous problems, architecture deficiencies, or areas where your team bottlenecks the innovations of other teams. You make data solutions simpler. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive profitability. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.

The Data Engineer
Has knowledge of recent advances in distributed systems (e.g. MapReduce, MPP architectures, and NoSQL databases). You are proficient in a broad range of data design approaches and know when it is appropriate to use them (and when it is not).
Knowledge of engineering and operational excellence best practices. Can make enhancements that improve data processes (e.g., data auditing solutions, management of manually maintained tables, automating, ad-hoc or manual operation steps).
Works with engineers to develop efficient data querying and modeling infrastructure.
Understands how to make appropriate data trade-offs. Can balance customer requirements with technology requirements. Knows when to re-use code. Is judicious about introducing dependencies.
Writing code that a Data Engineer or Software Development Engineer unfamiliar with the system can understand.
Can create coherent Logical Data Models that drive physical design.
Delivers pragmatic solutions. You do things with the proper level of complexity the first time (or at least minimize incidental complexity).
Understands how to be efficient with resource usage (e.g., system hardware, data storage, query optimization, AWS infrastructure etc.)
Collaboration with colleagues from multidisciplinary science, engineering and business backgrounds.
Communicate proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
3+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics
3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets
Demonstrable experience in scripting languages (Python, Perl, Ruby) and Excel
Experience in data modeling, ETL development, and Data warehousing
Experience with massively parallel processing (MPP) databases (data warehouse and data lake)
Experience with Tableau, Matillion, and AWS services (Redshift, S3, AWS Glue, EMR, DynamoDB)
Experience with cloud data platforms and big data solutions
Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
Master’s degree in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
Experience working with enterprise reporting systems, data analytics.
Experience working as an Analytics Engineer or Data Scientist working with cross functional teams.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2033555"
153,Data Engineer - Open on W2 ONLY,Dice,"San Jose, CA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Dataflix Inc., is seeking the following. Apply via Dice today!

Overview We are looking for a Data Engineer to build out and scale our Analytics platform. As a member of the team, you will be responsible for building and scaling a robust platform that will act as the driver for insights and business recommendations. To be successful in this role, you should be an excellent problem solver, critical thinker, knowledgeable about data engineering best practices, comfortable in a fast-paced organization, and excited about imparting change through data-driven insights. Key Responsibilities Build and automate advanced ETL pipelines in SQL and PySpark to provide business critical data to Genuine teams. Partner with other data engineers and software engineers to build reliable datasets, reports, and dashboards to monitor and analyze critical KPIs and funnels. Develop innovative approaches to build in-house tools that address challenges around data quality, pipeline optimization, and insights delivery. Collaborate cross-functionally to bring development best practices to the team, improving continuous integration and version control methodology. Work closely with business and technical stakeholders to understand pain points and help provide and enable data driven solutions. Act as a data steward to bring governance to our organization, setting the foundation for accountability for the team when sharing insights to our business partners, while maintaining a stable analytics platform QualificationsExperience. Requirements 5+ years of hands-on experience in a Data Engineering or Analytics related role (subscription based company is a plus). Proficient in writing complex analytics queries via SQL, Bash and Python. Comfortable interacting and engaging with Hadoop. Experience with PySpark. Driven to develop using industry best practices and tools such as Airflow and Github. Strong experience supporting cross-functional multidisciplinary teams to deliver high-impact business initiatives and analyses. Extensive experience leading team-based projects to completion. Excellent communication skills, both verbal and written, at all levels. Ability to rapidly assess a problem, define options, evaluate and execute to address critical business issues using data. BSMasters with 5+ years in quantitative fields Business Analytics, Computer Science, MathStatistics, Economics, or equivalent practical experience."
154,Associate Data Engineer,The New York Stem Cell Foundation Research Institute,"New York, NY","Posted by
Tammy Shieh
Relationship Manager at JobTarget
Send InMail
The New York Stem Cell Foundation (NYSCF) Research Institute is a growing and successful nonprofit whose mission is to accelerate cures through stem cell research.



NYSCF is seeking an Associate Data Engineer who will be responsible for building custom pipelines to ingest and process biological data generated by teams within the NYSCF Research Institute. You should be a skilled data engineer who has knowledge and experience in working with large datasets and data mining in Python and wide experience with databases including SQL. You will report directly to the Principal Scientist, Data Science.
What you'll do

Develop, deploy, and document software that supports the analysis, annotation, and quality control pipelines for data

Work with both data science and software engineering teams, as well as end user biologists, on requirements for processing, analyzing, and generate appropriate logs and reports of data

Process and analyze large datasets of microscopy images

Perform image processing for data standardization, quality control, characterization and feature extraction

Develop and implement novel data visualization strategies to summarize results and QC features

Ingest data from screens and process it to look for first impressions, outliers and nuances

Optimize the pipelines to be operated on different clusters and virtual machines







PI173703845


Desired Skills and Experience
What we're looking for

B.S. or M.S. computer science, engineering, data science, mathematica

Experience with statistical analysis and basic machine learning modeling

Strong experience programming in Python

Strong database experience including languages such as SQL

Familiarity with Python libraries for data framing and visualization (eg Pandas, Seaborn, pyplot)

Familiarity with cloud computing infrastructures on AWS, Google Cloud, Azure, etc.

Familiarity with GPU programming and resource optimization and parallel computing

Experience with Git repository systems

Level will be commensurate with experience


What we'd love for you to have but is not required

Knowledge of image processing techniques

Experience with microscopy images and fluorescence images


We offer all full-time employees a comprehensive benefits package that goes into effect on the first of the month following your start date.

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and complete the required employment eligibility verification upon hire. The position is located onsite on a full-time basis at our location in Manhattan.

NYSCF is an equal opportunity employer, and we value diversity in our company. It ensures equal employment opportunity without discrimination or harassment based on race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity or expression, age, disability, national origin, marital or domestic/civil partnership status, genetic information, citizenship status, veteran status, or any other characteristic protected by law.

Our Research Institute facility is a fully COVID-19 vaccinated workplace.



PI173703845"
155,"Data Engineer, Spark",Deloitte,"Nashville, TN","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
156,"Data Engineer, Spark",Deloitte,"Bristol, TN","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
157,Data Engineer - Snowflake,Deloitte,"Chicago, IL","Are you an experienced, passionate pioneer in technology? A cloud solutions builder who wants to work in a collaborative environment. As an experienced DATA ENGINEER - SNOWFLAKE, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

Essential Functions:
Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Indeed's Data Warehouse as well as data model changes that align with warehouse standards and backfill or other warehouse data management processes
Develop and execute testing strategies to ensure high quality warehouse data
Provide documentation, training, and consulting for data warehouse users
Perform requirement and data analysis in order to support warehouse project definition
Provide input and feedback to support continuous improvement in team processes
The Team

The US Cloud Engineering Offering focuses on enabling our client's end-to-end journey from On-Premise to Cloud, with opportunities in the areas of Cloud Strategy and Op Model Transformation, Cloud Development & Integration, Cloud Migration, and Cloud Infrastructure & Managed Services. Cloud Engineering supports our clients as they improve agility, resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited Sponsorship: Limited immigration sponsorship may be available"
158,"Data Engineer, Spark",Deloitte,"Charlotte, NC","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
159,Data Engineer,Amazon,"New York, NY","Description

Our team is passionate about Brands who sell on Amazon - we help them grow their businesses, build their story, and serve their customers. How do we do this? Data! Help us serve this valuable data to our Brands in digestible ways so they can run their businesses more effectively.

The ideal candidate will have excellent problem investigation abilities, and the ability to synthesize data into crisp and clear recommendations for scientists and product leaders. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business needs.

In the role, you will work closely with scientists, product managers and software engineers to build out infrastructure, data pipelines, and reporting mechanisms for our team and our Brands.

Our Data Engineer Duties & Responsibilities Will Include

Design and deliver big data architectures for experimental and production consumption between scientists and software engineering

Develop the end-to-end automation of data pipelines, making datasets readily-consumable by visualization tools and notification systems.

Create automated alarming and dashboards to monitor data integrity.

Create and manage capacity and performance plans.

Act as the subject matter expert for the data structure and usage.


Basic Qualifications
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
4+ years of industry experience in Software Development, Data Engineering, Business Intelligence, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
Hands-on experience and advanced knowledge of SQL.
Experience in Data Modeling, ETL Development, and Data Warehousing.Experience using business intelligence reporting tools (Power BI, Tableau, Cognos, etc.).
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.).
Knowledge of Data Management fundamentals and Data Storage fundamentals.
Experience coding and automating processes using Python or R.
Strong customer focus, ownership, urgency, and drive.
Excellent communication skills and the ability to work well in a team.
Effective investigation, troubleshooting, and problem-solving skills.
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative fields.
Experience working with AWS big data technologies (Redshift, S3, EMR, Glue).
Proven success in communication with users, other technical teams, and senior management to collect requirements, describe data modeling choices and data engineering strategy.
Experience providing technical leadership and supporting other engineers for best practices on data engineering.
Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus.

Company - Amazon.com Services LLC

Job ID: A1745544"
160,Data Engineer,Amazon,"New York, NY","Job Summary

DESCRIPTION

Amazon QuickSight Q team is seeking a Data Engineer with broad technical skills to help build the infrastructure and tools required to support our ML services for business intelligence.

In this role, the ideal candidate will be responsible for developing and managing big data systems using advanced data engineering knowledge in the data warehousing space and redefining best practices with a cloud-based approach to scalability and automation. Additionally, you will be responsible for scaling our existing infrastructure, incorporating new data sources, and building robust data pipelines for production level systems. In partnership with business intelligence engineers and analysts, you will work backwards from our business questions to build reliable and scalable data solutions to meet the business needs. Finally, this individual will work closely with project management teams to ensure proper guidance, use cases, and documentation is available to our Sales and Operations teams.

Responsibilities Include
Understand existing databases and warehouse structures in order best determine how to consolidate and aggregate data in an efficient and scalable way.
Design and code all aspects of data solutions using Amazon Redshift (SQL Server) to build out an Operations data warehouse.
Create and propose technical design documentation, which includes current and future ETL functionality, database objects affected, specifications, and flows and diagrams to detail the proposed implementation.
Design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics.
Implementing data structures using best practices in data modeling to provide on-line reporting and analysis using business intelligence tools and a logical abstraction layer against large, multi-dimensional datasets and multiple sources.
Evaluating and making decisions around dataset implementations designed and proposed by peer data engineers.
Manage AWS resources including EC2, Redshift, S3, etc. Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency.
Participating in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance to produce comprehensive, usable dataset documentation and metadata.
Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

By working together on behalf of our customers, we are building the future one innovative product, service, and idea at a time. Are you ready to embrace the challenge? Come build the future with us.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences.

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.


Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, or a related field and 2-3+ years industry experience
Must have one year of experience in the following skill(s): (1) Developing and operating large-scale ETL/ELT processes; database technologies; data modeling (2) Experience with at least one relational database technology such as Redshift, RDS, Oracle, Postgres
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
2+ year of coding experience with modern programming or scripting language (Python, Scala, Java, C# etc.).
Advanced SQL and query performance tuning skills
Experience with at least one massively parallel processing data technology such as Redshift, Spark or Hadoop based big data solutions
Preferred Qualifications
Master's/PhD degree in or Computer Science, Engineering, Mathematics or related discipline
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience operating a very large data environment that may include data lake, lakehouse, datawarehouse, etc.
Query performance tuning skills using Unix/Linux profiling tools and SQL
Experience with AWS Tools and Technologies (Redshift, S3, EC2, Glue, Lambda, Sage Maker)
Experience creating visual data representations, such as Tableau, PowerBI, QuickSight or other BI platforms
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Dev Center U.S., Inc.

Job ID: A2002231"
161,"Data Engineer, Spark",Deloitte,"Manhattan Beach, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
162,Data Engineer,Robert Half,"New York, NY","Description

Robert Half is looking for a Data Engineer for our client here in Downtown NY. This position will be a 6+ month contract and would require consultants to be onsite. Below are the requirements of the positions. If interested, please apply!

Requirements

Oracle SQL, Oracle PL/SQL, Python, Tableau, R Programming, BSA - Bank Secrecy Act, AML - Anti Money Laundering, Datastage, Datastage ETL, Datastage, Amazon Web Services (AWS), Data Pipelines, Amazon Kinesis, Azure SQL Database, Oracle, Microsoft Power BI, Power BI, Qlik, Alteryx, Scala, Snowflake, Redshift, Amazon Redshift, Shell Scripting

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
163,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Rays Techsolutions Inc., is seeking the following. Apply via Dice today!

Data Engineer Remote Role Description 10+ years of overall experience. Snowflake, AWS, Data Modeling Snowflake, Informatica, Python, AWS, Data Modeling Excellent presentation and communication skills Ability to understand business requirements and translate them into technical requirements A knack for benchmarking and optimization"
164,AWS Data Engineer,Deloitte,"Boston, MA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
165,Data Engineer,Federal Reserve Bank of New York,"New York, NY","Company

Federal Reserve Bank of New York

Working at the Federal Reserve Bank of New York positions you at the center of the financial world with a unique perspective on national and international markets and economies. You will work in an environment with a diverse group of experienced professionals to foster and support the safety, soundness, and vitality of our economic and financial systems. It is a challenge that demands the skills of a financial service professional and the intelligence of an academic—all combined with a passion for public service.

The Bank believes in work flexibility to balance the demands of work and life while also connecting and collaborating with our colleagues in person. Employees can expect to be in the office a couple of days per week as needed for meetings and team collaboration and should live within a commutable distance.

What we do:

The Data and Analytics chapter in the Technology Group builds data products that provide the organization with analytical capabilities in support of its mission. Reporting to the chapter lead for Data and Analytics, you will be part of a diverse, dynamic, and agile squad that enables rapid, repeatable, and resilient self-service analytics capabilities for the enterprise. This includes Data Preparation, ETL, Data Integration, RESTful analytical apps, Visualization/BI, and Surveys.

Your role as an Data Engineer (Alteryx, Cloudera, Tableau, AWS):
Develop and maintain workflows using Alteryx with wide-ranging source and target configurations in a customer facing role
Wireframe, design and build Tableau dashboards with advanced features in a customer facing role
Migrate on-premise data management solutions to AWS cloud only as well as hybrid configuration.
Advice, design, tune and optimize Alteryx flows build by customers and deploy to gallery
Display strong understanding of Cloudera’s SQL processing solutions
Research, troubleshoot and recommend solutions to complex data integration problems.
Mature analytics self-service adoption through active contributions towards community of practices, center of excellence and other forums.
Deep understanding of interoperability between current and future platforms. (Alteryx, Tableau, AWS, etc.)
Ability to articulate where the strengths and weaknesses are for each of our platforms to the end users

What we are looking for:
Technologist with background in data engineering with hands on experience in Alteryx, Tableau, Hadoop, Hive, Impala, AWS Data services
Experience with Python in data engineering or application development
Expertise in data wrangling, data integration, and visualization
Knowledge of data architecture and data management best practices
Experience implementing and maturing an analytics self-service model
Collaborative working style to support larger team goals and outcomes
Experience with relational databases and SQL-based technologies such as Oracle, Microsoft SQL Server or MySQL
Experience with data catalog tools like Collibra

Benefits:

Our organization offers benefits that are the best fit for you at every stage of your career:
Fully paid Pension plan and 401k with Generous Match
Comprehensive Insurance Plans (Medical, Dental and Vision including Flexible Spending Accounts and HSA)
Subsidized Public Transportation Program
Tuition Assistance Program
Onsite Fitness & Wellness Center
And more

Please note that the position requires access to confidential supervisory information and/or FOMC information, which is limited to ""Protected Individuals"" as defined in the U.S. federal immigration law. Protected Individuals include, but are not limited to, U.S. citizens, U.S. nationals, and U.S. permanent residents who either are not yet eligible to apply for naturalization or who have applied for naturalization within the requisite timeframe. Candidates who are permanent residents may be eligible for the information access required for this position if they sign a declaration of intent to become a U.S. citizen and pursue a path to citizenship and meet other eligibility requirements.

In addition, all candidates must undergo an enhanced background check, comply with all applicable information handling rules, and will be tested for all controlled substances prohibited by federal law, to include marijuana.

The Federal Reserve Bank of New York is committed to a diverse workforce and to providing equal employment opportunity to all persons without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, age, genetic information, disability, or military service.

The successful candidate must be fully vaccinated against COVID-19, and receive a booster shot within 30 days of being eligible to do so, unless the Bank grants an exemption based on a medical condition or sincerely held religious belief.

This is not necessarily an exhaustive list of all responsibilities, duties, performance standards or requirements, efforts, skills or working conditions associated with the job. While this is intended to be an accurate reflection of the current job, management reserves the right to revise the job or to require that other or different tasks be performed when circumstances change.

Full Time / Part Time

Full time

Regular / Temporary

Regular

Job Exempt (Yes / No)

Yes

Job Category

Information Technology

Work Shift

First (United States of America)

The Federal Reserve Banks believe that diversity and inclusion among our employees is critical to our success as an organization, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool. The Federal Reserve Banks are committed to equal employment opportunity for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.

Privacy Notice"
166,Data Engineer - Innovate on a New Platform in AWS,Amazon Web Services (AWS),"Portland, OR","Description

Do you want to innovate on a new platform with Amazon Web Services (AWS)?

Come join our teams in AWS, you’ll help us design and deliver data engineering solutions that enables AWS to expand our operations into new data centers worldwide. Our green-field solutions will support programs in compliance and sustainability, and build a platform that will extend to first-class AWS services. You will collaborate with engineers designing data solutions at massive scale.

To succeed in this role, you have 4+ years of experience and you believe in high standards for writing SQL, building data pipelines with infrastructure as code, code quality, code reviews, testing, and operational excellence. You support your peers to deliver results in a space that operates much like a start-up, with the autonomy to build the right solution from Day One. As a DE with our team, you will collaborate with other engineers on technical direction, strategy with a career opportunity for growth that AWS can offer.

This role is located in Seattle, Portland, San Francisco, San Luis Obispo, Boston, New York City or Arlington. We have flexible work options, enabling you to work at home and in the office when the team collaborates. Our team also puts a high value on work-life balance, family-first approach. We care about your career growth and strive to assign projects based on what will help each team member grow.


Basic Qualifications
Bachelor’s Degree in Computer Science or related field, or equivalent experience
4+ years professional experience in data engineering, business intelligence, data science or related field
Experience with technologies including Big Data, EMR, EML or similar solutions
Demonstrated strength in data, development, and data warehousing - Experience in Python, Java or other similar languages
Experience with data management fundamentals and data storage principles
Experience with distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience with serverless computing, enterprise-wide systems, and AWS products
Understanding of Agile software engineering practices
Meets/exceeds Amazon’s leadership principles requirements for his role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Company - Amazon Web Services, Inc.

Job ID: A1761811"
167,Senior Data Engineer,Walmart,"Sunnyvale, CA","Position Summary... What You'll Do...

Data Strategy Requires knowledge of: Understanding of business value and relevance of data and data enabled insights / decisions; Appropriate application and understanding of data ecosystem including Data Management, Data Quality Standards and Data Governance, Accessibility, Storage and Scalability etc; Understanding of the methods and applications that unlock the monetary value of data assets. To understand, articulate, and apply principles of the defined strategy to routine business problems that involve a single function.

Data Source Identification Requires knowledge of: Functional business domain and scenarios; Categories of data and where it is held; Business data requirements; Database technologies and distributed datastores (e.g. SQL, NoSQL); Data Quality; Existing business systems and processes, including the key drivers and measures of success. To support the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.

Data Transformation and Integration Requires knowledge of: Internal and external data sources including how they are collected, where and how they are stored, and interrelationships, both within and external to the organization; Techniques like ETL batch processing, streaming ingestion, scrapers, API and crawlers; Data warehousing service for structured and semi-structured data, or to MPP databases such as Snowflake, Microsoft Azure, Presto or Google BigQuery; Pre-processing techniques such as transformation, integration, normalization, feature extraction, to identify and apply appropriate methods; Techniques such as decision trees, advanced regression techniques such as LASSO methods, random forests etc; Cloud and big data environments like EDO2 systems. To extract data from identified databases. Creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. Develops knowledge of current data science and analytics trends.

Tech. Problem Formulation Requires knowledge of: Analytics/big data analytics / automation techniques and methods; Business understanding; Precedence and use cases; Business requirements and insights. To translate/ co-own business problems within one's discipline to data related or mathematical solutions. Identifies appropriate methods/tools to be leveraged to provide a solution for the problem. Shares use cases and gives examples to demonstrate how the method would solve the business problem.

Understanding Business Context Requires knowledge of: Industry and environmental factors; Common business vernacular; Business practices across two or more domains such as product, finance, marketing, sales, technology, business systems, and human resources and in-depth knowledge of related practices; Directly relevant business metrics and business areas. To provide recommendations to business stakeholders to solve complex business issues. Develops business cases s for projects with a projected return on investment or cost savings. Translates business requirements into projects, activities, and tasks and aligns to overall business strategy and develops domain specific artifact. Serves as an interpreter and conduit to connect business needs with tangible solutions and results. Identify and recommend relevant business insights pertaining to their area of work.

Data Modeling Requires knowledge of: Cloud data strategy, data warehouse, data lake, and enterprise big data platforms; Data modeling techniques and tools (For example, Dimensional design and scalability), Entity Relationship diagrams, Erwin, etc. ; Query languages SQL / NoSQL; Data flows through the different systems; Tools supporting automated data loads; Artificial Intelligent - enabled metadata management tools and techniques. To analyze complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develops the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Defines relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluates existing data models and physical databases for variances and discrepancies. Develops efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions. Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.

Code Development and Testing Requires knowledge of: Coding languages like SQL, Java, C++, Python and others; Testing methods such as static, dynamic, software composition analysis, manual penetration testing and others; Business, domain understanding. To write code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Creates test cases to review and validate the proposed solution design. Creates proofs of concept. Tests the code using the appropriate testing approach. Deploys software to production servers. Contributes code documentation, maintains playbooks, and provides timely progress updates.

Data Governance Requires knowledge of: Data value chains; Data processes and practices; Regulatory and ethical requirements around data; Data modeling, storage, integration, and warehousing; Data value chains (identification, ingestion, processing, storage, analysis, and utilization); Data quality framework and metrics; Regulatory and ethical requirements around data privacy, security, storage, retention, and documentation; Business implications on data usage; Data Strategy; Enterprise regulatory and ethical policies and strategies. To establish, modify, and document data governance projects and recommendations. Implements data governance practices in partnership with business stakeholders and peers. Interprets company and regulatory policies on data. Educates others on data governance processes, practices, policies, and guidelines. Provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines.

Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.

Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.

Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.

Live our Values

Culture Champion
Models the Walmart values to foster our culture; holds oneself and others accountable; and supports Walmart’s commitment to communities, social justice, corporate social responsibility, and sustainability; maintains and promotes the highest standards of integrity, ethics and compliance.
Servant Leadership
Acts as an altruistic servant leader and is consistently humble, self-aware, honest, and transparent.
Embrace Change

Curiosity & Courage
Demonstrates curiosity and a growth mindset; fosters an environment that supports learning, innovation, and intelligent risk-taking; and exhibits resilience in the face of setbacks.
Digital Transformation & Change
Seeks and implements continuous improvements and encourages the team to leverage new digital tools and ways of working.
Deliver for the Customer

Customer Focus
Delivers expected business results while putting the customer first and consistently applying an omni-merchant mindset and the EDLP and EDLC business models to all plans.
Strategic Thinking
Adopts a holistic perspective that considers data, analytics, customer insights, and different parts of the business when making plans and shaping the team’s strategy.
Focus on our Associates

Diversity, Equity & Inclusion
Embraces diversity in all its forms and actively supports diversity of ideas and perspectives, as well as diversity goal programs.
Collaboration & Influence
Builds strong and trusting relationships with team members and business partners; works collaboratively and cross-functionally to achieve objectives; and communicates with energy and positivity to motivate, influence, and inspire commitment and action.
Talent Management
Contributes to an environment allowing everyone to bring their best selves to work, demonstrates engagement and commitment to the team, and recognizes others’ contributions and accomplishments.
Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

As permitted by applicable law, provide evidence of full vaccination as defined by CDC guidelines OR secure approval of medical or religious accommodation for the vaccination mandate., Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years’ experience in

software engineering or related field. Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related

field.

2 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering Primary Location...840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America"
168,Spark Data Engineer,Perficient,"Allentown, PA","Overview

At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Spark Developer proficient in Scala and SQL We are looking for someone to be based in the US, but are flexible on specific location. Candidate will be expected to work US East Coast hours with occasional flexibility needed to work with client team based in India.

Job Overview

One of our large clients is expanding their current data footprint on the cloud to provide analytics, BI and data APIs. Majority of data will be batch processed with data validation, data quality and transformation into a multitude of data platforms such as Redshift, Postgres and Hive.

A Senior Technical Consultant is expected to be knowledgeable in two or more technologies within (a given Solutions/Practice area). The Senior Technical Consultant is expected to have strong development and programming skills in Spark with a focus on Scala/Java and other ETL development experience in the big data space. You are expected to be experienced and fluent in agile development and agile tools as well as code repositories and agile SLDC/DevOps frameworks.

You will work with architects and infrastructure teams to develop, test, deploy and troubleshoot your code as well as provide input into solutions and design of the system. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions focused on our clients' business needs.

Responsibilities
Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.
Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components
Document code artifacts and participate in developing user documentation and run books
Troubleshoot deployment to various environments and provide test support.
Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Qualifications
At least 3+ years of experience in developing large scale data processing/data storage/data distribution systems
At least 3+ years of experience on working with large Hadoop projects using Spark and Scala and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures.
Experience with ELT/ETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred.
Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics
Experience with SQL including Postgres, MySQL RDBMS platforms
Experience with Linux (RHEL or Centos preferred) environments
Experience with various IDE and code repositories as well as unit testing frameworks.
Experience with code build tools such as Maven.
Fundamental knowledge of distributed data processing systems and storage mechanisms.
Ability to produce high quality work products under pressure and within deadlines with specific references
Strong communication and collaborative skills
At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project
At least 5+ years of working with a complex Big Data environment
5+ years of experience with JIRA/GitHub/Git and other code management toolsets
Preferred Skills And Education

Bachelors’s degree in Computer Science or related field

Certification in Spark, AWS or other cloud platform

Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

Select work authorization questions to ask when applicants apply
Are you legally authorized to work in the United States?
Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"
169,Data Engineer - Remote,"TrueCar, Inc.","Santa Monica, CA","Job Description:

TrueCar envisions a world where car shopping is an uplifting experience. Our shopping experience helps buyers consider choices from every angle, builds confidence in their decisions, and enables every step of the process with tools and information that make car buying easy. Ultimately, TrueCar is helping people in the second largest purchase they will make in their lives. We're removing the complexity out of buying a car, using technology and personalization, to create a one-of-a-kind experience that transforms car buying and ultimately people’s lives.

Come join the team and help us accomplish our mission. TrueCar maintains a Dynamic Workplace, allowing employees to have their primary workstations at home, with office space in Santa Monica, CA and Austin, TX to be made available to individuals and teams to use as needed. Employees enjoy excellent benefits (health/vision/dental coverage, 401k with contribution matching, equity, etc.) as well as perks like monthly credits for at-home food delivery, internet/mobile phone service coverage, fitness expenses, and Caregiver support.

About the Job:

The Data Engineering team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.



What you'll do:
Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
Build complex workflows and orchestrate data dependencies.
Monitor and support data pipelines to honor internal and external SLA’s.
Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.
Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
Working with a team of engineers where mentorship is valued.
Ability to learn and adapt to continually evolving technologies in the big data ecosystem.
What you'll need:
5+ years of experience programming in Java.
2+ years of experience in Big Data technologies.
Experience in any of big data technologies: MapReduce, Spark, HBase,
Proficient in SQL and experience with RDBMS/NoSQL databases.
Experience working with Cloudera/Hortonworks/EMR distribution in AWS.
Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work.
Team-player with strong collaboration and communication skills, who is able to respond positively to feedback.
Bachelor degree (or Master) in Computer Science or related engineering field
While this position is open to remote work through TrueCar's Dynamic Workplace initiative, applicants may not reside in Colorado. Colorado candidates will be required to relocate. ***
Location(s):

Santa Monica, CA"
170,Data Engineer,Deloitte,"Harrisburg, PA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

The Data Engineer will join a team responsible for developing advanced analytics products; applying data visualization and statistical programming tools to enterprise data to advance and enable the key mission outcomes. In this role, they will support all phases of analytic work product development, from the identification of key business questions, through data collection and ETL, from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. This role requires special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree in STEM field
2+ years of experience with programming languages such as Python, R, SPSS, SAS, SQL
2+ years of experience with data visualization tools, such as Tableau, Qlik, PowerBI, or equivalent
2+ years of experience with ETL/ETL Pipeline, Data Warehouse Development and data modelling
Preferred:
Prior professional services or federal consulting experience
Experience with ETL, NoSQL Apache Hadoop, and cloud computing technology, especially Microsoft Azure
How You'll Grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
171,"Data Engineer, Prime Video Search",Amazon,"New York, NY","Job Summary

DESCRIPTION

Do you want to take a leading role in shaping the future of digital video search? Do you want to define the next generation of search engine that allows Amazon customers to discover our great contents (including our amazing originals) seamlessly? If so, we, Prime Video Search, are responsible for developing structured video search systems that understand the intent of the search query accurately, utilize rich metadata sets, and consistently display relevant search results to millions of Prime Video customers, in over 210 countries around the world, across all Prime Video surfaces (e.g. iOS, Android, Fire TV, Alexa, Roku, TV Apps, Retail Web).

Key job responsibilities

As a Data Engineer II, you will develop new data engineering patterns that leverage new cloud architectures, and will extend or migrate existing data pipelines to the architectures as needed. You will be responsible for designing and implementing the complex ETL pipelines in data warehouse platform and other BI solutions to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making at Amazon.com

A day in the life

We are looking for a talented Data Engineer II to help build/enhance the global data platform that enhances the search experience to ease discovery our content for our world audience (200+ countries.) In this role, you will own many large datasets, implementing new data pipelines that feed into or from critical data systems at Amazon. You will design, implement, and support new systems that ensure the quality of the data sets and work with our data scientists to make the data available for metrics, visualization, ad-hoc insights, and statistical modeling.

About The Team

The mission of Prime Video Search is to help customers work less and watch more. We're a high energy, fast growth business excited to have the opportunity to shape new ways for search to reach their audiences. We need your passion, innovative ideas, and creativity to help continue to expand our space and enhance the search experience.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
Bachelor's or Master's degree in computer science, engineering, mathematics, or a related technical discipline
2+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
Knowledge of data management fundamentals and data storage principles
A desire to work in a collaborative, intellectually curious environment.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1598109"
172,Python Developer & Data Engineer (Remote Full-time Permanent or Contract Opportunity),BDO Canada,"California, United States","Putting people first, every day

BDO is a firm built on a foundation of positive relationships with our people and our clients. Each day, our professionals provide exceptional service, helping clients with advice and insight they can trust. In turn, we offer an award-winning environment that fosters a people-first culture with a high priority on your personal and professional growth.

Your opportunity

BDO Lixar is looking for a Python Developer & Data Engineer to join the BDO Lixar team on either a full-time permanent or contract basis, with the ability to work remotely from anywhere in Canada.

The individual will be working with an AI Operations team and focus on developing APIs for our clients’ data models using Python, including data wrangling and deploying applications. An application development background and understanding of CI/CD pipelines is preferred.

How do we define success for your role?

You demonstrate BDO's core values through all aspect of your work: Integrity, Respect and Collaboration
You understand your client’s industry, challenges, and opportunities; clients describe you as positive, professional, and delivering high quality work
You identify, recommend, and are focused on effective service delivery to your clients
You share in an inclusive and engaging work environment that develops, retains & attracts talent
You actively participate in the adoption of digital tools and strategies to drive an innovative workplace
You grow your expertise through learning and professional development.

Your Experience And Education

Strong Python skills
Python data wrangling experience
Background in application development
Knowledge of SQL Server, PostgreSQL, Neo4j databases (having a basic knowledge of Neo4j is fine)
Basic knowledge of OpenShift (to be able to deploy applications in there)
Basic knowledge of Elastic search

Why BDO?

Our people-first approach to talent has earned us a spot among Canada’s Top 100 Employers for 2022 . This recognition is a milestone we’re thrilled to add to our collection of awards for both experienced and student talent experiences.

Our firm is committed to providing an environment where you can be successful in the following ways:

We enable you to engage with how we change and evolve, being a key contributor to the success and growth of BDO in Canada.
We help you become a better professional within our services, industries, and markets with extensive opportunities for learning and development.
We support your achievement of personal goals outside of the office and making an impact on your community.

Giving back adds up: Where company meets community. BDO is actively involved in our communities by supporting local charity initiatives. We support staff with local and national events where you will be given the opportunity to contribute to your community.

Total rewards that matter: We pay for performance with competitive total cash compensation that recognizes and rewards your contribution. We provide flexible benefits from day one, and a market-leading personal time off policy. We’re committed to supporting your overall wellness beyond working hours, and provide reimbursement for wellness initiatives that fit your lifestyle.

Everyone counts: We believe every employee should have the opportunity to participate and succeed without barriers. Through leadership by our Chief Inclusion, Equity and Diversity Officer, we are committed to a workplace culture of respect, inclusion, equity, and diversity. We recognize and celebrate the valuable differences among each of us, including race, religious beliefs, physical or mental disabilities, age, place of origin, marital status, family status, gender or gender identity and sexual orientation. If you require accommodation to complete the application process, please contact us .

Hybrid new normal: As a hybrid workplace, all BDO personnel are expected to spend some of their time working in the office, at the client site, and remotely unless accommodations or alternative work arrangements are in place. To protect the health and safety of our people, clients, and communities, we require all partners and employees to be COVID-19 fully vaccinated in order to enter a BDO office. Individuals that cannot be fully vaccinated with a Health Canada approved vaccine due to medical reasons or another protected ground under Human Rights legislation may request an accommodation.

Ready to make your mark at BDO? Click “Apply now” to send your up-to-date resume to one of our Talent Acquisition Specialists.

To explore other opportunities at BDO, check out our careers page ."
173,Data Engineer - Snowflake,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Urpan Technologies, Inc., is seeking the following. Apply via Dice today!

6+ years of experience in supporting Data and Analytics projects Understanding Data Architecture iPaaS platforms to connect the data bases Working experience with Snowflake iPaaS as a Snaplogic and any experience will be great to have Monitor and troubleshoot the issues and report to the project teams with an RCA."
174,Data Engineer,Dice,"Los Angeles, CA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Luxoft USA Inc, is seeking the following. Apply via Dice today!

Project DescriptionLuxoft is looking for a Senior Python Developer who would be working with our Customer - one of the world's largest investment management companies. Based in Southern California, our client manages close to 2 trillion in assets and is working with Luxoft in a full-scale upgrade of their technology. Responsibilitiesbull Work in a SCRUM team, close with client and business analysts, be able to quickly understand the new requirements and helps to identify the changes to the system. bull Develop and present recommendations from analysis of current solution. Assist in code peer reviews, build and test functional enhancements. Identify areas for code refactoring. Review and contribute to development process and procedures to assist in the continual improvement of the development team activities. bull Strong organization, communication, and interpersonal skills. bull Responsible for fullstack development to meet specifications and designs and using best practices. Development of a world-class web services code-base. bull Effort estimation and task breakdown - estimating cost of implementing new changes and identifying risk. Mandatory Skills Descriptionbull High proficiency in SQL bull Strong Python programming experience bull Data analysis skills bull Familiarity with financial industry domain Nice-to-Have SkillsAWS"
175,"Data Engineer in Huntsville, AL",Deloitte,"Huntsville, AL","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do
As part of a cross-functional agile team, design and manage a data schema to support the launch of innovative products for Public Sector clients
Understand user requirements and translate them into data storage requirements
Design cutting edge data integrations in AWS
Design and manage relational databases
Extract, transform, and load (ETL) methods and tools
Create and updating physical data models
The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role - Secret
5+ years of experience designing and managing relational databases
Expertise in extract, transform, and load (ETL) methods and tools, writing SQL queries and JSON objects
Ability to document and explain data storage decisions
Strong knowledge of best practices - ability to understand user requirements and translate them into data storage requirements
Experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB
Experience creating and updating physical data models
Ability to understand complex legacy systems and their data schemas
Preferred:
Interest in enabling data-sharing across multiple systems
Experience writing and interpreting data dictionaries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
176,"Data Engineer, Snowflake",Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
177,Data Engineer,Robert Half,"New York, NY","Description

Robert Half is looking for a Data Engineer for our client here in Downtown NY. This position will be a 6+ month contract and would require consultants to be onsite. Below are the requirements of the positions. If interested, please apply!

Requirements

Oracle SQL, Oracle PL/SQL, Python, R Programming, BSA - Bank Secrecy Act, AML - Anti Money Laundering, Tableau, Datastage, Datastage ETL, Data Pipelines, Amazon Web Services (AWS), Amazon Kinesis, Azure SQL Database, Microsoft Power BI, Power BI, Microsoft Power BI, Power BI, Qlik, Oracle, Scala, Shell Scripting, Redshift, Amazon Redshift, Snowflake, Alteryx

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
178,Data Engineer,UBS,"New York, NY","Job Reference #
246525BR

Job Type
Full Time

Your role
UBS Evidence Lab is looking for a data engineer to join its Data Engineering team or its R&D Team. The teams’ focus is building scalable frameworks and services used to analyze large datasets in the context of financial analysis and alternative data. They leverage well-established engineering processes and techniques pioneered by giants in the tech industry. Projects span general pipelining to building internal web tools to helping develop low-latency query and analysis engines.

No background in finance is required, so this is a great opportunity to get exposure to the finance industry. Think of the team as a well-funded engineering startup within an investment bank.

Responsibilities:

develop highly scalable data processing pipelines

develop data query and analysis services to be leveraged by clients, internally and externally

maintain a high standard of code quality within the broader engineering team

adapt the latest data processing and infrastructure techniques to our growing stack

Your team
Diversity helps us grow, together. That’s why we are committed to fostering and advancing diversity, equity, and inclusion. It strengthens our business and brings value to our clients.

Your expertise
Minimum Qualifications:

established track record in designing, building, deploying, and maintaining scalable systems

second-nature knowledge of algorithms and data structures

second-nature knowledge of SQL

knowledge developing and debugging Python

Preferred Qualifications:

formal education in Computer Science or related quantitative field

experience with distributed systems

experience with Apache Spark

experience with Apache Airflow

About Us
UBS is the world’s largest and only truly global wealth manager. We operate through four business divisions: Global Wealth Management, Personal & Corporate Banking, Asset Management and the Investment Bank. Our global reach and the breadth of our expertise set us apart from our competitors.

With more than 70,000 employees, we have a presence in all major financial centers in more than 50 countries. Do you want to be one of us?

Join us
At UBS, we embrace flexible ways of working when the role permits. We offer different working arrangements like part-time, job-sharing and hybrid (office and home) working. Our purpose-led culture and global infrastructure help us connect, collaborate, and work together in agile ways to meet all our business needs.

From gaining new experiences in different roles to acquiring fresh knowledge and skills, we know that great work is never done alone. We know that it's our people, with their unique backgrounds, skills, experience levels and interests, who drive our ongoing success. Together we’re more than ourselves. Ready to be part of #teamUBS and make an impact?

Disclaimer / Policy Statements
UBS is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills and experiences within our workforce."
179,"Data Engineer, Snowflake",Deloitte,"Glen Mills, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
180,Data Engineer,Cisco,"San Francisco, CA","(To be considered for this role, please apply directly here https//grnh.se/1761a1091us )

Cisco Meraki is revolutionizing the way IT administrators manage their infrastructure by providing simple and secure cloud-managed solutions. With a large install base of customers and rich multifaceted data sets, the potential for data analytics to improve business performance for both our customers and our own business is enormous.

About The Role

The Data Science Infrastructure team is a growing group that works closely with executives and leaders across the company to support the development and alignment on our business strategy. We are looking for a Data Engineer to develop cross-functional relationships and work closely with data engineering and data science teams to drive near and long term initiatives to improve our product and customer experience. This would be an outstanding fit for a solution-oriented technical pro who is hands-on, has the ability to work autonomously, and can drive technical efforts building robust and resilient auto-scaling platform solutions. In near real time, Cisco Meraki collects massive amounts of data from its devices all over the world. We collect nearly 6 million data points each second (~50B events per day). In this role, you will analyze raw data, develop and maintain datasets, and improve data quality and efficiency. You will also help scale our systems to handle ever increasing amounts of data points and requests.

What Will You Do
Collaborate with several teams(product and non product) within Cisco Meraki to understand their data requirements and requests.
Build systems that process raw data from multiple sources, and create intuitive and interesting insights for our customers.
Analyze raw data, develop and maintain datasets, improve data quality, lineage, and efficiency.
Conduct complex data analysis and prepare report summarizing results
Maintain and improve quality of data warehouse to serve data needs at Cisco Meraki.
What Skills You Posses
BS or MS in Computer Science / related technical field or equivalent combination of graduate degree and work experience
Experience in designing, implementing, and debugging ETL pipelines in a distributed system, preferably working with large data sets.
Good understanding of SQL, including experience working with one or more relational databases and data warehouse (e.g., MySQL or Snowflake).
4+ years of experience writing professional production level code and tests.
2+ years of experience in Python, Java, and scripting languages.
Strong written and verbal communication skills and excellent attention to detail and accuracy
Bonus Points For
Experience with AWS or Google cloud environments.
Experience with reporting tools such as MicroStrategy or Tableau.
Experience or a desire to lead technical decisions and design discussions.
Experience or willing to work in an agile environment (Scrum, Kanban, etc.).

We encourage you to drop us a line even if you don’t have all the points above. That's a lot of different areas of responsibility! We will help you pick them up because we believe that great engineers come from diverse backgrounds.

(To be considered for this role, please apply directly here https//grnh.se/1761a1091us )"
181,Data Engineer (Big Data),U.S. Bank,"Dallas, TX","At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.

Job Description

Be a part of transformational change where integrity matters, success inspires and great teams collaborate and innovate. As the fifth-largest bank in the United States, we're one of the country's most respected, innovative, ethical and successful financial institutions. We're looking for people who want more than just a job - they want to make a difference! U.S. Bank is seeking a Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.

This position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.

Essential Responsibilities
Responsible for designing, developing, testing, operating and maintaining products.
Takes full stack ownership by consistently writing production-ready and testable code.
Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs.
Makes sound design/coding decisions keeping customer experience in the forefront.
Takes feedback from code review and apply changes to meet standards. Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures.
Accountable for ensuring all aspects of product development follow compliance and security best practices.
Exhibits relentless focus in software reliability engineering standards embedded into development standards.
Embraces emerging technology opportunities and contributes to the best practices in support of the bank's technology transformation.
Contributes to a culture of innovation, collaboration and continuous improvement.
Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery. Anticipates and communicates blockers and delays before they require escalation.
Basic Qualifications
Bachelor's degree, or equivalent work experience
Three to five years of relevant experience
Preferred Skills/Experience
Thorough understanding of a feature, the users impacted, the flows impacted and feature's purpose
Technical designs are clear, well thought out, and considers dependencies, failure states, maintainability, testability and ease of support
Considers scalability, reliability and performance of systems/contexts affected when defining technical designs
Understands the team's domain, how work in this domain relates to the team's objectives and deliverables and how it contributes to overall business strategy and how technical strategy maps to this
Thoroughly understands the business model in relation to their current product focus area
Ability to analyze the bigger picture, identifying and prioritizing with the aim to consider more than one domain within an analysis
Looks for opportunities to simplify product and technical design
Adept with agile software development lifecycle and DevOps principles
Able to communicate processes and results with all parties involved in the product team, including engineers, product owner, scrum master, third party vendors and customers
Strong problem-solving and analytical skills
Excellent communication and interpersonal skills
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.

Benefits

Take care of yourself and your family with U.S. Bank employee benefits. We know that healthy employees are happy employees, and we believe that work/life balance should be easy to achieve. That's why we share the cost of benefits and offer a variety of programs, resources and support you need to bring your full self to work and stay present and committed to the people who matter most - your family.

Learn all about U.S. Bank employee benefits, including tuition reimbursement, retirement plans and more, by visiting usbank.com/careers.

EEO is the Law

Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal EEO is the Law poster.

E-Verify

U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.

Due to legal requirements, U.S. Bank requires that the successful candidate hired for some positions be fully-vaccinated for COVID-19, absent being granted an accommodation due to a medical condition, pregnancy, or sincerely held religious belief or other legally required exemption. For these positions, as part of the conditional offer of employment, the successful candidate will be asked to provide proof of vaccination or approval for an accommodation or exemption upon hire."
182,"Data Engineer, Spark",Deloitte,"Horsham, PA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
183,Data Analytics Engineer,"KCF Technologies, Inc.","State College, PA","KCF Technologies is an exciting and rapidly growing technology company dedicated to putting innovative solutions to work in the Industrial World. SmartDiagnostics, our IIOT Technology, fuels our mission to solve the worlds machine health problems to drive safety and sustainability for our communities. To accomplish this epic technical revolution, we integrate our core values into our everyday actions and consistently challenge the status quo! Learn more at www.kcftech.com.


At KCF, we value autonomy, working smart, and doing things a little bit differently. If youre an A-player who wants to be part of one of the most important companies revitalizing machine health across the world, KCF is the place for you. If our values resonate with you, please keep reading!


Core Values:


Smarts: We are humbly aggressive lifelong learners.


Grit: We are scrappy, proactive problem solvers who dont stop until the job is done.


Drive: We demonstrate an insatiable hunger to serve others.


Responsibility: We do the right thing and contribute to the greater good.


Autonomy: We own our work and define how we do it, while aligned with the greater mission.







Where You Come In:


KCF Technologies is looking for a talented and motivated Data Analytics Engineer to help develop computer algorithms and models trained to ingest sensor and machine process data and diagnose fault conditions, prognose time-to-failure, provide recommended actions, and perform root-cause analysis.


We are looking for an individual who not only has an aptitude for data analytics, statistical analysis, and pattern recognition, but also has a passion for learning and understanding how things work. You consider yourself to be independently motivated and an analytically deductive problem solver, with a factual and to-the-point communication style. Youre a case matter expert on things that draw your interest, in addition to being extremely disciplined with perfectionist tendencies.


Our ideal candidate exemplifies our cultural values of Smarts, Grit, and Drive, and considers him or herself to be:



Self-motivated

Passionate for solving real-world problems

Hard-working

Adaptable

Detail-oriented

Accepting of the unknown


If this sounds like you, we encourage you to keep reading!





Essential Functions:



Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysis

Produce models for edge and/or cloud (AWS) execution

Integrate analytics solutions within the KCF SMARTdiagnostics platform

Construct data mining tools for acquiring training and validation data sets





Qualifications:



Bachelors degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of study

Strong mathematical background (linear algebra, calculus, probability and statistics)

Machine Learning experience (regression and classification, supervised, and unsupervised learning)

At least 2 years of experience in machine health

Strong algorithm design skills

Programming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languages

Communicates verbally and in writing in a clear and professional manner

Able to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracy

Highly service-oriented disposition with aptitude in problem-solving

Must exemplify the following KCF cultural values: Smarts, Grit, Drive

Strong organizational, time management, and prioritization abilities

Should be able to deal with difficult, sensitive, and confidential issues







Perks & Benefits:


At KCF, we are committed to providing best-in-class benefits, engaging development opportunities, and powerful perks that are focused on bringing out the best in you.


Full-time benefits are as follows:



100% company-paid Medical, Dental, and Vision premiums

Health Savings Account with a generous annual employer contribution

Hybrid work model for most positions, work from home, work from anywhere

Competitive compensation & bonus opportunities

Four weeks PTO; Paid Holidays

401(k) with KCF match

Wellness Perk- including annual reimbursement program

Monthly cell and office expense stipend

Learning Culture committed to growth and continuous development





At KCF, we are an equal opportunity employer. The only things we require for employment, compensation, advancement and benefits are performance and a good team attitude. No one will be denied opportunities or benefits, and no employment decisions will be made, on the basis of race, religion/creed, national origin, ancestry, sex, sexual orientation, gender, gender identity, age, disability that does not prohibit performance of essential job functions, protected veteran status, medical condition, marital status, pregnancy, genetic information, possession of a general education development certificate (GED) as compared to a high school diploma, or any other characteristic protected by applicable federal or state laws. KCF complies with applicable state and local laws governing nondiscrimination in employment in every location in which KCF has facilities."
184,Azure Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, R Systems, Inc., is seeking the following. Apply via Dice today!

Azure Data Engineer 6 months Contract to Hire (CTH) Remote Role We are looking for an Azure Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple sources of data while also working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data analytics lifecycle. Key Responsibilities To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Other duties may be assigned to meet business needs. Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan. Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques. Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse build within those constraints. Ability to define and implement best practices across database design and ETL. Ability to direct the work of others, including but not limited to directing ETL development demonstrating an understanding of key concepts of ETLELT including best practices for optimization and scheduling. Supervisory Responsibilities None Qualifications Education and Experience Proven understanding of data warehousing, Data Architecture, and BI. Experience with data pipelines and architectureengineering. Knowledge of modern apps and data platforms. Cloud-based project implementation (Azure) SnowflakeDB experience is a plus but not must to have Knowledge, Skills, and Abilities BIData Warehousing (3+ years) Cloud platforms (1+ years) ETL (3+ years) SQL SSIS (3+ years) Azure (3+Years)"
185,Data Engineer - FULL TIME - REMOTE,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Acunor Infotech, is seeking the following. Apply via Dice today!

Acunor is looking for Data Engineers for one of its clients. this is complete remote and full time opportunity. Job Title Data Engineer Location Remote Duration Fulltime Role Overview Demonstrated understanding and experience in applying traditional statistical, machine learning, deep learning, NLP to address business problems Experience in leading customer expectation, project planning , execution and project closure Experience in Python, SQL Good ability to handle large volumes of data independently Able to explain statistical and NLP concept in user friendly manner. Using effective visualization for conveying the stats model results The person will also be required to participate actively in brainstorming sessions for improvement opportunities and take it to completion A suitable candidate should have 5-8 years of experience in a similar role and should possess a go -getter attitude. He She should be able to deal with ambiguity. Experience in life insurance preferred but not mandatory. Key Responsibilities Skillsets Technical Skillsets Superior analytical and problem solving skills High Proficiency in Python Coding along with good knowledge of SQL Knowledge of using Python Libraries such as scikit-learn, scipy, pandas, numpy , nltk, matplotlib Deep rooted knowledge and understanding Traditional Machine Learning Algorithms Advanced modelling techniques (e.g. Random Forest, SVM, time series etc.) and Text Analytics technique (NLTK, Genism, LDA etc.) Must have hands on experience of building and deploying Predictive Models Must have experience of building Predictive Models using AWS Sagemaker and other AWS services Have worked on at least one of the GIT repository management solution (Github, Bitbucket etc.) Should be able to work on a problem independently and prepare client ready deliverable with minimal or no supervision Good communication skill for client interaction Data Management Skillsets Ability to understand data models and identify ETL optimization opportunities. Exposure to ETL tools is preferred Should have strong grasp of advanced SQL functionalities (joins, nested query, and procedures). Strong ability to translate functional specifications requirements to technical requirements Candidate Profile BachelorrsquosMaster's degree in economics, mathematics, actuarial sciences, computer scienceengineering, operations research or related analytics areas candidates with BABS degrees in the same fields from the top tier academic institutions are also welcome to apply 5-8 yearsrsquo experience in process improvement and automation, preferably in Life Insurance but not mandatory Strong and in-depth understanding of statistics, data analytics Data analysis experience Superior analytical and problem solving skills Outstanding written and verbal communication skills Able to work in fast pace continuously evolving environment and ready to take up uphill challenges Is able to understand cross cultural differences and can work with clients across the globe Thanks Regards Sai kumar mailto"
186,"Data Engineer, Snowflake",Deloitte,"Newtown, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
187,"Data Engineer, MI Data - Platform",Point72,"New York, NY","A Career with Point72’s Market Intelligence Team

Point72’s Market Intelligence team is responsible for developing proprietary research products and providing data and research management services for investment teams to support their pursuit of superior, risk-adjusted returns. We leverage innovative alternative data sources, advanced data analytics and technologies, and deep fundamental research to create high-quality compliant and differentiated research. Backed by the full resources of Point72, our sector aligned teams collaborate to solve important research problems in partnership with the Firm’s investment and compliance professionals.

What you’ll do

In This Role, You Will

Platform Engineers build solutions for processing big and unstructured data sets at Point72. Our team works closely with portfolio managers and data scientists to understand the potential business value of data sets and ultimately build data processing pipelines around those data sources.
Develop big data processing pipelines for new data sources containing structured and unstructured data
Build platform infrastructure using Hadoop technologies
Build and support visualization and exploration capabilities around our big data sets
Maintain knowledge of new technology developments and conduct proof of concepts to evaluate new technologies
What’s Required

We want you to join us if you have extensive experience or demonstrated interest in big data technologies. Other requirements include:
2+ years of experience in Data Engineering or related field
Commitment to the highest ethical standards
Strong experience in Python Development
Experience with Spark or Scala
Ability to devise novel and innovative solutions to challenges
Knowledge of/experience with graph databases is a plus
We take care of our people

When You Work Here, We Provide

We invest in our people, their careers, their health, and their well-being. We want you to concentrate on success and leave the rest to us.
Fully-paid health care benefits
Generous parental and family leave policies
Mental and physical wellness programs
Tuition assistance
A 401(k) savings program with an employer match and more
Point72 policy requires all U.S. employees to be fully vaccinated against COVID-19 in order to enter any Point72 office or participate in any Point72 business-related event in person. Should an offer of employment be made, your acceptance of that offer means that you will comply with this policy. You will be advised of your anticipated schedule to be in the office should we move forward with the offer. Please note that all visitors must be fully vaccinated against Covid-19 to enter any of Point72’s U.S. offices.

Point72 is an Equal Opportunity Employer. Point72 is committed to the principles of equal employment opportunity for all employees and applicants for employment. Point72 complies with applicable, local, state and federal laws on the subject of equal employment opportunity."
188,Data Engineer – Webscraping,Balyasny Asset Management L.P.,"New York, NY","The Data Intelligence Group (DIG) is a key part of BAM’s continued growth. Year over year, the knowledge needed to leverage data plays an increasingly important role in the firm’s core business. The analysis, services, software, and operational expertise that DIG provides are part of BAM’s competitive advantage.

Role Overview
We are looking for a creative and meticulous developer to join our Webscraping team. The data we provide drives investment decisions across the firm and we work hard to make sure it’s timely and accurate.

The optimal candidate will be strongly self-motivated with the ability to work and solve problems independently. In your role, you will:
Collaborate with analysts to understand and anticipate requirements
Design, implement, and maintain webscrapes for a wide variety of alternative datasets
Author tests to validate data availability and integrity
Maintain alerting systems to ensure smooth day-to-day operations
Investigate and defuse time-sensitive data incidents

Minimum Qualifications
Bachelors/Masters degree in Computer Science or a related field
1-3 years web development experience (Python/SQL/HTML/CSS/HTTP)
Linux experience (Windows experience a plus)
Excellent verbal and written communication skills

Preferred Qualifications
Aptitude for designing infrastructure, data products, and tools for Data Scientists
Familiarity with scraping and common scraping tools (Selenium, scrapy, Fiddler, Postman, xpath)
Experience containerizing workloads with Docker (Kubernetes a plus)
Experience with build automation (Jenkins, TeamCity)
Experience with AWS"
189,Data Engineer || 100% Remote,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Empower Professionals, is seeking the following. Apply via Dice today!
Role Title Google Cloud Platform Data Engineer Location 100 Remote (Client based in Memphis, TN) Duration 12 month contract ndash expected to extend for 24+ months. Shift CST work schedule Requirements middot Bachelorrsquos degree in Computer Science or related field required middot Minimum of 5 - 8 years of hands-on experience designing, developing, and delivering software solutions in Java (preferred) or Python middot Solid knowledge of Solr, Apache Beam, Java Springboot, middot ATGEndeca (preferred but not required) middot Working experience with Streaming and batch processing tools like Apache Beam (preferred), Spark etc. middot Understanding of Big Table and Big Query. middot Experience in Spring boot framework and microservices development. middot Working experience in Google Cloud on cloud functions, specifically developing and deploying google dataflow middot Experience with all aspects of Java, Spring Framework, Spring Boot, etc. as well as microservices development middot Advanced problem-solving, domain technical and analytical skills middot Advanced system estimation, planning, and execution skills 2) Role Senior Data Engineer Location 100 Remote Duration 12+ Months Required Skills Azure Databricks Spark-Scala Python or Java or Pyspark Basic Qualifications 2 senior data engineers to work on one of our development teams. While data science skills are always welcome in addition to data engineering skills, our primary need right now is data engineering. The following is the job description we recently put together for this resource need 10+ years of post education experience with 5+ years as a data engineer. Significant experience with big data ETL pipeline development with Spark, Hive, and related technologies Significant experience with a general purpose programming language such as Python or Java 2+ years of experience with Azure Databricks Experienced with Spark framework and related tools (PySpark, SparkR, Spark SQL, Spark UI) Experienced with Hadoop ecosystem using HDFS, ADLS Gen2, or AWS S3 Experienced with data visualization development using Python, Tableau, or PowerBI Experienced with Azure, AWS or Google Cloud Platform Quick learner Solid understanding of performance tuning concepts for relational and distributed database systems Familiarity with distributed programming, big data concepts, and cloud computing"
190,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, West Coast Consulting LLC, is seeking the following. Apply via Dice today!

Top 3 must-have HARD skills middot Ability to work as part of a team, as well as work independently or with minimal direction. middot Excellent written, presentation, and verbal communication skills middot Collaborate with data architects, modelers and IT team members on project goals. Good to have skills 1. Query Optimization 2. Strong technical experience 3. Python, SQL, Data modeling skills Years of experience required middot 5-8 years related experience middot Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI."
191,"Data Engineer, Blink Sales and Data Analytics",Amazon,"Washington County, OR","Job Summary

DESCRIPTION

Are you interested in being part of a high visibility, strategic team that directly impacts the Blink business worldwide? Blink was the result of one of the most successful crowdfunding campaigns ever and is now part of Amazon Devices, the consumer electronics division that brings you the Kindle, Fire Tablets, Fire TV, and Echo. Our mission is to provide peace of mind for home owners while they are way from home using our wire-free, battery-operated smart home security cameras.

We are looking for motivated self-starters that can work in an extremely fast paced environment. The successful candidate can be a single-threaded owner of multiple facets of Blink’s data infrastructure, building out the infrastructure platform for Blink’s fast-growing Business Intelligence team.

Key job responsibilities
Design, implement and operate large-scale, high-volume, high-performance data structures for analytics and data science
Develop the end-to-end automation of data pipelines, making datasets readily-consumable by visualization tools and notification systems
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS technologies
Manage AWS resources including EC2, Redshift, Glue, Lambda, CloudWatch, S3, etc
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service modeling and production support for customers

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline, or equivalent work experience
Experience with Python
Preferred Qualifications
Master’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline
5+ years of experience as a Data Engineer or in a similar role in a company with large, complex data sources
Track record of data management fundamentals and data storage principles
Experience working with AWS technologies (Redshift, S3, Glue, Lambda, EC2)
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1928725"
192,Databricks Data Engineer (Remote Work),Dice,"Seattle, WA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Orgspire Inc, is seeking the following. Apply via Dice today!

Work Authorization Only Independent Applicants Key Skills SQL, Databricks, Tabular Cube, Power BI 9+ years development experience. 7+ of SQL Server development experience writing complex stored procedures, triggers, views, etc. Strong understanding of BI areas. Ability to work in large, complex development BI projects including the proactive identification of issues and coordination of resolutions. Expertise in T-SQL, DW Concepts, Tabular Cube. Must have strong experience with Azure Azure Data Lake Azure Data Factory. Experience with Azure DataBricks, Synapse will be a big plus. Experience with Power BI Strong Analytical and troubleshooting skills Excellent coding and debugging skills."
193,Data Science Engineer II,Nabors Industries,"Houston, TX","Company Overview

Nabors is a leading provider of advanced technology for the energy industry. With operations in about 20 countries, Nabors has established a global network of people, technology and equipment to deploy solutions that deliver safe, efficient and responsible hydrocarbon production. By leveraging its core competencies, particularly in drilling, engineering, automation, data science and manufacturing, Nabors aims to innovate the future of energy and enable the transition to a lower carbon world.

Nabors is committed to providing equal employment opportunities to all employees and applicants and prohibiting discrimination and harassment of any type without regard to race, religion, age, color, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This applies to all terms and conditions of employment including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. To learn more about our Fair Employment practices, please refer to the Nabors Code of Conduct.

JOB SUMMARY

Develop scalable machine learning components to generalize models to other data sources and product groups

Develop solutions for high volume, low latency applications and can operate in a fast paced, highly collaborative environment.

Data Science Engineer will help develop data science applications using machine learning and statistical techniques, help discover the information hidden in large and complex data sets

Develop processes and tools to monitor model performance over time and analyze data accuracy

Candidate must have experience using a variety of data mining and statistical methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations.

DUTIES AND RESPONSIBILITIES

Design and work with Machine learning processes and tools which would maintain and improve the model accuracy in an automated way
Understanding of statistics and hypothesis testing (ex. Confidence Interval, Regressions, Time Series, Clustering, Factor Analysis)
Conduct evaluations of new programs and initiatives to figure out what's working and what's not
Use predictive modeling and machine learning algorithms to solve complex problems
Stay abreast of state-of-the-art machine learning technologies; follow code standards and best practices
Develop and maintain forecasting models and tools using frameworks such as TensorFlow
Doing ad-hoc analysis and presenting results in a clear manner
Use Deep Learning frameworks like PyTorch, Tensorflow and MxNet is a plus
Strong written and verbal communication skills

Desired Skills and Experience


MINIMUM QUALIFICATIONS/SKILLS

• 3-5 years of experience in analytics with at least 2 year of experience in statistical analysis

• Bachelors or master’s degree in quantitative field (Computer Science, Mathematics, Machine Learning, AI, Statistics, or equivalent)

Familiar with the following software/tools:

Experience with data querying languages (e.g. SQL), scripting languages (e.g. Python), and/or statistical/mathematical software (e.g. R)
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
Ability to implement statistical models for forecasting, time series predictions
Knowledge and demonstrable skills in Deep learning architectures and implementation using Pytorch or TensorFlow.
Experience writing production quality software in Python or R to implement predictive models
Experience using web services: REST API, SOAP, WCF etc. is a plus


PREFERRED QUALIFICATIONS

Bachelors in Computer Science and a Master’s in Data Science or another quantitative field

PHYSICAL REQUIREMENTS / WORKING CONDITIONS

Travel to rig for domain knowledge and new product testing."
194,"Data Engineer, Spark",Deloitte,"Portland, OR","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
195,Staff Data Engineer - Remote,"TrueCar, Inc.","Santa Monica, CA","Job Description:

TrueCar is on a mission to revolutionize the way that consumers engage in the vehicle purchase and ownership experience. We’re building an end-to-end consumer journey that’s uplifting, empowering, and unrivaled in the marketplace, and we’re looking for the best and brightest to help us achieve our goals. We’re on the hunt for teammates who embrace challenge, relentlessly innovate, and reject the notion that ‘it can’t be done.’

TrueCar maintains a Dynamic Workplace, allowing employees to have their primary workstations at home, with office space in Santa Monica, CA and Austin, TX to be made available to individuals and teams to use as needed. Employees enjoy excellent benefits (100% employer-paid health/vision/dental premium, 401k with contribution matching, equity for eligible roles, etc.) as well as perks like monthly credits for at-home food delivery, internet/mobile phone service coverage, fitness expenses, and Caregiver support. In short, we care deeply about our teammates and build employee-centric programs that prove it.

About the Job:

The Data Engineering serves as the subject matter expert ingesting, analyzing, and validating the automotive data required from internal and 3rd party sources. Key responsibilities include building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.



What you'll do:
Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
Build complex workflows and orchestrate data dependencies.
Monitor and support data pipelines to honor internal and external SLA’s.
Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.
Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
Working with a team of engineers where mentorship is valued.
Ability to learn and adapt to continually evolving technologies in the big data ecosystem.
What you'll need:
5+ years of experience programming in Java.
2+ years of experience in Big Data technologies.
Experience in any of big data technologies: MapReduce, Spark, HBase,
Proficient in SQL and experience with RDBMS/NoSQL databases.
Experience working with Cloudera/Hortonworks/EMR distribution in AWS.
Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work.
Team-player with strong collaboration and communication skills, who is able to respond positively to feedback.
Bachelor degree (or Master) in Computer Science or related engineering field
While this position is open to remote work through TrueCar's Dynamic Workplace initiative, applicants may not reside in Colorado. Colorado candidates will be required to relocate. ***
Location(s):

Santa Monica, CA"
196,"Data Engineer, Specialist",Vanguard,"Malvern, PA","Join Vanguard’s Fund Investment Operations team! As a Data Engineer, you will provide advanced data solutions by using software to process, store, and serve data to others as well as test data quality and optimize data availability. This role ensures that data pipelines are scalable, repeatable, and secure and build a deep dive analytical skillset by working with higher level Data Engineers on a variety of internal and external data.

The Data Engineer Specialist:

Writes ETL (Extract / Transform / Load) processes, designs database systems, and develops tools for real-time and offline analytic processing.
Troubleshoots software and processes for data consistency and integrity. Integrates data from a variety of sources for business partners to generate insight and make decisions.
Translates business specifications into design specifications and code. Responsible for writing programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
Partners with internal clients to gain a basic understanding of business functions and informational needs. Gains working knowledge in tools, technologies, and applications/databases in specific business areas and company-wide systems.
Participates in all phases of solution development. Explains technical considerations at related meetings, including those with business clients.
Tests code thoroughly for accuracy of intended purpose. Reviews end product with the client to ensure adequate understanding. Provides data analysis guidance as required.
Provides tool and data support to business users and fellow team members.
Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
Participates in special projects and performs other duties as assigned.

What it takes:

Minimum of three years data analytics, programming, database administration, or data management experience.
Undergraduate degree or equivalent combination of training and experience.

Special Factors:

Tableau knowledge preferred
This role will fall into the Hybrid working model
Vanguard is not offering visa sponsorship for this position.

About Vanguard

We are Vanguard. Together, we’re changing the way the world invests.

For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.

We want to make success accessible to everyone. This is our opportunity. Let’s make it count.

Inclusion Statement

Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”

We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.

When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.

Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.

Future of Work

During the pandemic, we transitioned to a work from home model for the majority of our crew and we continue to interview, hire, and on-board future crew remotely.

As we have developed the path forward, we have taken a thoughtful approach that both maximizes the advantages of working remotely and the many benefits of coming together and collaborating in a shared workspace. We believe that in-person interactions among our crew are important for preserving our unique culture and advantageous for the personal development of our crew.

When our Crew return to the office, many will work in our hybrid model. A smaller proportion of our crew will operate in the Work from Home work model (for example, field sales crew); or in the Work from Office model (for example, portfolio managers).

The working model that your role falls into will be communicated to you in the interview process – please do ask if you are unsure. We encourage you to make the decision regarding your job interview and offer knowing which model your role will fall into. We will test and learn as our ways of working evolve and will continue to evaluate working models along the way."
197,Data Engineer - Advertising Analytics Data Pipeline,Amazon,"Boulder, CO","Description

Are you passionate about using Big Data to build customer trust and grow new business? Global advertisers rely on our team's performance insights to drive future investment in Amazon's Advertising Platform and improve the relevance of ads shown to customers. We are looking for passionate Data Engineers to own and optimize the big data pipeline that consumes the massive data sources we require to generate unique insights. Data is at the center of every product we will develop as we create brand new systems that serve the needs of our large and growing base of advertisers.

You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence. Members of this team will be challenged to innovate using big data technologies. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive profitability. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you!

Amazon is well positioned to grow its share of a fast growing online advertising industry due to its unique assets - e-commerce data, service oriented architecture, and startup culture. Be part of a team of industry leading experts that builds and operates one of the largest big data analytics platform at Amazon. Amazon is applying the latest machine learning and big data technologies available to change the way marketers purchase, track, measure, and optimize their advertising spend. We apply these technologies on terabytes of data (over 10B new events per day) and operate clusters that push scalability limits of the existing technologies. We seek to measure every possible signal indicating impact of advertising to provide the most objective result of marketing spends.

What we offer

We are a company of builders who bring varying backgrounds, ideas, and points of view to inventing on behalf of our customers. Our diverse perspectives come from many sources including gender, race, age, national origin, sexual orientation, culture, education, and professional and life experience. We are committed to diversity and inclusion and always look for ways to scale our impact as we grow. You can read more here.

Amazon has 13 affinity groups, also known as employee resource groups, which bring Amazon employees together across businesses and locations around the world. Some examples include the Black Employee Network (BEN), Amazon Women in Engineering (AWE), and Indigenous@Amazon.

This role is open to candidates sitting in Boulder, CO or Seattle, WA.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, Presto, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficiency in, at least, one modern scripting or programming language such as Python, NodeJS, Java, or Scala.
Preferred Qualifications
Experience working with and tuning AWS big data technologies (EMR, Redshift, S3)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Amazon is committed to a diverse and inclusive workforce. Amazon is an equal opportunity employer and does not discriminate on the basis of race, ethnicity, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit US Disability Accommodations.

The pay range for this position in Colorado starts at $ 113,000/yr; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon’s internal or external careers site.

#adsto #madsjob


Company - Amazon.com Services LLC

Job ID: A1675873"
198,"Data Engineer, Spark",Deloitte,"Dallas, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
199,"Data Engineer, Spark",Deloitte,"Irving, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
200,AWS Data Engineer,Deloitte,"San Diego, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
201,"Senior Data Engineer (Remote, United States)",The Job Network,"New York, NY","Job Description Our Data Engineering group builds and maintains the platform that delivers accessible data to power decision-making at Shopify for millions of merchants. We’re hiring high-impact developers across teams: The Engine group organizes all merchant and Shopify data into our data lake in highly-optimized formats for fast query processing, and maintaining the security + quality of our datasets.The Analytics group builds products that leverage the Engine primitives to deliver simple and useful products that power scalable transformation of data at Shopify in batch, or streaming, or for machine learning. This group is focused on making it really simple for our users to answer three questions: What happened in the past? What is happening now? And, what will happen in the future?

The Data Experiences group builds end-user experiences for experimentation, data discovery, and business intelligence reporting.The Reliability group operates the data platform efficiently in a consistent and reliable manner. They build tools for other teams at Data Platform to leverage to encourage consistency and they champion reliability across the platform.Qualifications While our teams value specialized skills, they've also got a lot in common. We're looking for a(n):

High-energy self-starter with experience and passion for data and big data scale processing. You enjoy working in fast-paced environments and love making an impact. Exceptional communicator with the ability to translate technical concepts into easy to understand language for our stakeholders. Excitement for working with a remote team; you value collaborating on problems, asking questions, delivering feedback, and supporting others in their goals whether they are in your vicinity or entire cities apart.Solid software engineer: experienced in building and maintaining systems at scale. A Senior Data Developer at Shopify typically has 4-6 years of experience in one or more of the following areas: Working with the internals of a distributed compute engine (Spark, Presto, DBT, or Flink/Beam)Query optimization, resource allocation and management, and data lake performance (Presto, SQL)Cloud infrastructure (Google Cloud, Kubernetes, Terraform)Security products and methods (Apache Ranger, Apache Knox, OAuth, IAM, Kerberos)Deploying and scaling ML solutions using open-source frameworks (MLFlow, TFX, H2O, etc.)Building full-stack applications (Ruby/Rails, React, TypeScript)Background and practical experience in statistics and/or computational mathematics (Bayesian and Frequentist approaches, NumPy, PyMC3, etc.)Modern Big-Data storage technologies (Iceberg, Hudi, Delta)#Senior Software Developer- Distributed Systems #Senior Software Developer- Data Engineering #Senior Data App Developer #Senior Data Reliability Engineer #Senior Software Developer- Reliability

Additional Information

Shopify is now permanently remote, and we’re working towards a future that is digital by design. That location you see above? Consider it merely an example of hundreds of potential locations Shopify is hiring. Learn more here: https://www.shopify.com/careers/work-anywhere

Our belief is that a strong commitment to diversity & inclusion enables us to truly make commerce better for everyone. We encourage applications from Indigenous peoples, racialized people, people with disabilities, people from gender and sexually diverse communities, and/or people with intersectional identities. Please take a look at our Sustainability Reports to learn more about Shopify’s commitments to our communities, and our planet.

At Shopify, we understand that experience comes in many forms. We’re dedicated to adding new perspectives to the team - so if your experience is this close to what we’re looking for, please consider applying. PandoLogic. Keywords: Data Engineer, Location: Dallas, TX - 75201"
202,Index Data Algorithm Engineer,"eVestment, A Part of Nasdaq","New York, United States","We are seeking an experienced financial algorithm / data engineer to join our effort in developing the next generation of index construction capabilities.

The vision for the Index Data Platforms and Models (DP&M) team is to improve client experiences by using data as a means of empowering the Nasdaq Index Business with data-driven insights and solutions. Within this organization, you will be focused on building modular functions for data transformation and index construction algorithms (e.g., security identification, factor screening, and weighting techniques) that support construction and rebalance of Nasdaq’s index suite.

In this role, you will be responsible for working with Research and Development and Index Portfolio Management while collaborating with our extended Data Platforms and Models team to deliver tangible value in the form of coded algorithms that translate a variety of input data into index portfolios by applying data transformation and portfolio construction methodologies, techniques and strategies.

The ideal person possesses a portfolio construction and data analytics acumen, a strong understanding of data technologies, and expertise in software engineering best practices.
Build, automate, maintain, and monitor index construction and maintenance algorithms using standardized methodologies, with reliability and scalability in mind
Improve the performance and reliability of our index research, historical simulation, portfolio construction, and rebalance and reconstitution processes and algorithms
Work in collaboration with Research and Development to translate their needs into scalable, standardized solutions
Develop standards and processes for onboarding new data transformations into research and production
Collaborate with other specialists, product managers, and engineers to identify and solve critical business problems, crafting a positive experience for internal and external clients
Bachelor’s degree in Computer Science, Mathematics, Statistics or related area
5+ years of experience in Software Engineering, Data Engineering, or Trading Strategy Modeling
5+ years working with relational databases and query languages, including building data pipelines and the ability to work across structured, semi-structured and unstructured data
5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
Experience working with cloud platforms (AWS preferred), development and operations technologies (e.g., Kubernetes, Gitlab), and data management utilities (Databricks, Apache Parquet, etc.)
Strong quantitative reasoning skills and an interest in working at the intersection of research and software engineering
Experience preparing data for analytics and following a data science workflow to drive business results
Practical experience with Agile principles, preferably with Scrum, Kanban, or SAFe frameworks
Promotion of a strong control environment, adherence to risk controls and procedures, and process discipline, while appropriately considering business priorities
Come as You Are

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation."
203,Data Engineer,Viking Global Investors,New York City Metropolitan Area,"The Data Engineer is a member of the Investment Data Engineering team and is primarily responsible for building data pipelines to support investment research. The role is focused on applying modern data engineering principles to rapidly deliver trustworthy and thoughtfully curated datasets to data analysts, data scientists, and the investment staff.
Responsibilities may include, but are not limited to:

Own the full data pipeline lifecycle, including gathering requirements, orchestrating tasks, writing performant Python and SQL code, implementing data validation, and providing ongoing support.
Work with stakeholders to translate their needs into a clearly defined technical implementation.
Work with cloud data warehouses, including creating, manipulating, and modifying objects.
Build complex Airflow DAGs, including creating custom operators.
Write efficient and modular data transformations using frameworks like data build tool (dbt).
Conduct data explorations in Tableau to discover anomalies and identify data inaccuracies.
Conduct code reviews and participate in architecture and systems design discussions.
Ensure timely delivery of projects and proactively communicate updates to stakeholders.

QUALIFICATIONS
The ideal candidate must have:
A minimum of 3 years of relevant work experience.
A degree in Computer Science or a related field, with a record of academic success.
Excellent computer science fundamentals and problem-solving skills, including an understanding of object-oriented and functional programming principles.
Experience in the fields of data warehousing, pipeline orchestration, and business intelligence, including familiarity with the extract-load-transform (ELT) data integration process.
Proficient in Python and SQL.
Experience with pipeline orchestration tools, such as: Apache Airflow, Luigi, Prefect, Dagster.
Experience with OLAP or cloud data warehouses, such as: Snowflake, Google BigQuery, Databricks SQL, Redshift.
Experience working in a cloud ecosystem, such as: AWS, Azure, GCP.
Experience working in a Linux environment.

The ideal candidate will also have:
Experience with Snowflake, Apache Airflow, Databricks, Apache Spark (PySpark), data build tool (dbt), Tableau, Retool, GitLab, and AWS.
Familiar with CI/CD patterns, Docker and containerization.
Familiar with data observability concepts and platforms.
Prior investment management or financial services industry experience.

Hybrid work schedule - 2 days in NYC, 1 day in Greenwich, CT, 2 days WFH"
204,Data Engineer - Dataworks - All Levels (Remote),FedEx Services,"Memphis, TN","Company: FedEx Services

Job Title: Data Engineer - Dataworks - All Levels (Remote)

Job Requisition Number: RC481202

Category: Information Technology

Pay Type: Exempt

Locations:

Memphis, Tennessee 38120

United States

Remote

United States

Colorado Residents Only – Compensation: Monthly Salary $6908.82 - $14524.25

The estimate displayed represents the typical salary range or starting rate of candidates hired in Colorado. Factors that may be used to determine your actual salary may include your specific skills, your work location, how many years of experience you have, and comparison to other employees already in this role. This information is provided to applicants in accordance to the Colorado Equal Pay for Equal Work Act.

Duties for this role include but not limited to: supporting the design, build, test and maintain data pipelines at big data scale. Assists with updating data from multiple data sources. Work on batch processing of collected data and match its format to the stored data, make sure that the data is ready to be processed and analyzed. Assisting with keeping the ecosystem and the pipeline optimized and efficient, troubleshooting standard performance, data related problems and provide L3 support. Implementing parsers, validators, transformers and correlators to reformat, update and enhance the data. Provides recommendations to highly complex problems. Providing guidance to those in less senior positions.

Additional Job Details

Data Engineers play a pivotal role within Dataworks, focused on creating and driving engineering innovation within Dataworks, helping define and build the Dataworks organization and facilitate the delivery of key business initiatives. S/he acts as a “universal translator” between IT, business, software engineers and data scientists, collaborating with these multi-disciplinary teams. Data Engineers will contribute to the creation of and adherence to technical standards for data engineering, including the selection and refinements of foundational technical components. S/he will work on those aspects of the Dataworks platform that govern the ingestion, transformation, and pipelining of data assets, both to end users within FedEx and into data products and services that may be externally facing. Day-to-day, s/he will be deeply involved in code reviews and large-scale deployments.

Essential Job Duties & Responsibilities
Understanding in depth both the business and technical problems Dataworks aims to solve
Building tools, platforms and pipelines to enable teams to clearly and cleanly analyze data, build models and drive decisions
Scaling up from “laptop-scale” to “cluster scale” problems, in terms of both infrastructure and problem structure and technique
Delivering tangible value very rapidly, collaborating with diverse teams of varying backgrounds and disciplines
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases
Interacting with senior technologists from the broader enterprise and outside of FedEx (partner ecosystems and customers) to create synergies and ensure smooth deployments to downstream operational systems
Skill/Knowledge Considered a Plus

Technical background in computer science, software engineering, database systems, distributed systems

Familiarity/fluency with distributed and cloud environments and a deep understanding of how to balance computational considerations with theoretical properties
Detailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferred
Experience with designing and deploying large scale technical solutions, which deliver tangible, ongoing value
Direct experience having built and deployed robust, complex production systems that implement modern, data scientific methods at scale
Ability to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being defined
Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value
An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact
Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews
Ability to conduct data analysis, investigation, and lineage studies to document and enhance data quality and access

Use of agile and devops practices for project and software management including continuous integration and continuous delivery

Demonstrated expertise working with some of the following common languages and tools:
Spark (Scala and PySpark), HDFS, Kafka and other high-volume data tools
SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch
Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools
Minimum Qualifications

Data Engineer II:

Bachelor's Degree in Computer Science, Information Systems, a related quantitative field such as Engineering or Mathematics or equivalent formal training or work experience. Two (2) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Strong knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Strong knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience as a member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Data Engineer III

Bachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Five (5) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Data Engineer Advisor

Bachelor’s Degree in Information Systems, Computer Science, or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Seven (7) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a leader or a senior member of multi-function project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Domicile / Relocation Information:

This position can be domiciled anywhere in the United States.

The ability to work remotely within the United States may be available based on business need.

Relocation assistance may be available based on business need.

Application Criteria / Deadline

Upload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire.

Notice: Sponsorship for this position is not available.

Employee Benefits: Medical, dental, and vision insurance; paid Life and AD&D insurance; tuition reimbursement; paid sick leave; paid parental leave, paid vacation, and additional paid time off; geographic pay ranges; 401K with Company match and incentive bonus potential; sales incentive compensation for selling roles.

FedEx. Where now meets next.

Our vision is to be the earth's most engaged advocates of connected commerce where open borders, new markets and fair, sustainable practices are the norm for the billions of personal supply chains being managed every day in our always on, mobile-first world. We stand for ease, access and opportunity. We lead purposeful innovation, champion entrepreneurs, advocate free trade and empower humans and their place in the era of autonomy and AI. We fight for our customers, a more sustainable planet and an ethical playing field.

FedEx inspires its more than 570,000 team members to remain focused on safety, the highest ethical and professional standards and the needs of their customers and communities. FedEx is committed to connecting people and possibilities around the world responsibly and resourcefully, with a goal to achieve carbon-neutral operations by 2040.

FedEx has been recognized on many different lists both for business success and for being a great employer:
Fortune ""World’s Most Admired Companies"" – 2021
Forbes ""Best Employers for Diversity"" - 2021
LinkedIn ""Top 100 Companies"" - 2021
TIME ""100 Most Influential Companies"" - 2021
World HRD Congress ""Best Gender Equality Workplace"" – 2021
InsiderPro ComputerWorld ""Best Places to Work for IT"" – 2021
Application Instructions/Deadline

Upload current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire by close of business (5:00pm CST) on the date below. If the date below is blank, there is no specified closing date for this requisition.

FedEx Services is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, genetics disability, or protected Veteran status.

FedEx Services does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, FedEx Services is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact recruitmentsupport@fedex.com."
205,Index Data Algorithm Engineer,"eVestment, A Part of Nasdaq","Pennsylvania, United States","We are seeking an experienced financial algorithm / data engineer to join our effort in developing the next generation of index construction capabilities.

The vision for the Index Data Platforms and Models (DP&M) team is to improve client experiences by using data as a means of empowering the Nasdaq Index Business with data-driven insights and solutions. Within this organization, you will be focused on building modular functions for data transformation and index construction algorithms (e.g., security identification, factor screening, and weighting techniques) that support construction and rebalance of Nasdaq’s index suite.

In this role, you will be responsible for working with Research and Development and Index Portfolio Management while collaborating with our extended Data Platforms and Models team to deliver tangible value in the form of coded algorithms that translate a variety of input data into index portfolios by applying data transformation and portfolio construction methodologies, techniques and strategies.

The ideal person possesses a portfolio construction and data analytics acumen, a strong understanding of data technologies, and expertise in software engineering best practices.
Build, automate, maintain, and monitor index construction and maintenance algorithms using standardized methodologies, with reliability and scalability in mind
Improve the performance and reliability of our index research, historical simulation, portfolio construction, and rebalance and reconstitution processes and algorithms
Work in collaboration with Research and Development to translate their needs into scalable, standardized solutions
Develop standards and processes for onboarding new data transformations into research and production
Collaborate with other specialists, product managers, and engineers to identify and solve critical business problems, crafting a positive experience for internal and external clients
Bachelor’s degree in Computer Science, Mathematics, Statistics or related area
5+ years of experience in Software Engineering, Data Engineering, or Trading Strategy Modeling
5+ years working with relational databases and query languages, including building data pipelines and the ability to work across structured, semi-structured and unstructured data
5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
Experience working with cloud platforms (AWS preferred), development and operations technologies (e.g., Kubernetes, Gitlab), and data management utilities (Databricks, Apache Parquet, etc.)
Strong quantitative reasoning skills and an interest in working at the intersection of research and software engineering
Experience preparing data for analytics and following a data science workflow to drive business results
Practical experience with Agile principles, preferably with Scrum, Kanban, or SAFe frameworks
Promotion of a strong control environment, adherence to risk controls and procedures, and process discipline, while appropriately considering business priorities
Come as You Are

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation."
206,"Market Intel, Data Engineer, WFS-MI",Amazon,"Washington County, OR","Job Summary

DESCRIPTION

The Market team for Workforce Staffing applies science, and insights to optimize hiring for Amazon’s largest candidate population – Tier 1 Associates. Amazon's hourly workforce brings the magic of Amazon’s industry-leading customer fulfillment to life. The pace at which job creation, hiring, and growth happen to support the scale and complexity of Amazon es is a problem Amazon is uniquely qualified to solve and innovate on. Workforce Staffing literally hires by the hundreds of thousands across multiple lines, job types, and shift configurations. The Market team in particular focuses on applying labor market, competitor, and candidate preference to enhance job offerings, mitigate operational risk, and sustain Amazon position in the market. Come join a team that is continually shaping and writing the future of the hourly worker landscape.

As a Data Engineer for the Market Intel team , you will excel in the design, creation, management, and use of large volumes of data that will become the core set for research and insights. You will be responsible for designing and implementing scalable ETL processes in the warehouse platform to support the rapidly growing and dynamic demand for across various sources, and use it to deliver the as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune to provide optimized solutions to the . You will work cross-functioning with analysts, scientists, and other data engineers to determine the best design for scale and maintenance.

Key Responsibilities Include
Building and migrating the complex ETL pipelines into cluster
Optimizing the performance of -critical queries and dealing with ETL job-related issues
Gather and understand requirements, work in the team to achieve high quality ingestion and build systems that can process the , transform the
Designing, implementing and supporting a data platform that can support business needs
Earn the trust of your customers by continuing to constantly obsess over their needs and helping them solve their problems by leveraging technology
Manage critical initiatives to enforce standard work and reduce waste

Basic Qualifications
Degree in Computer Science, Mathematics, or a related field or 2+ years industry experience
Coding proficiency in at least one modern programming language (Python, Java, etc.)
Demonstrated strength in modern , ETL development, lake design and implementation, and warehousing.
Warehousing Experience with Redshift
Query performance tuning skills
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Development Engineer , Data Scientist) with a track record of manipulating, processing, and extracting data from large data sets.
Experience with AWS technologies
Experience leading large-scale warehousing and projects, including using AWS technologies – S3, EC2, etc.
Experience building/operating highly available, distributed systems of extraction, ingestion, and processing of large data sets
Experience building products incrementally and integrating and managing sets from multiple sources
Experience in providing technical leadership and mentoring
A desire to work in a collaborative.
Ability to work on a diverse team or with a diverse range of coworkers.
The base pay range for this position in Colorado is $ 154,600 - $209,100 a year; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon’s internal or external careers site.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2027265"
207,AWS Data Engineer,Deloitte,"Chicago, IL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
208,Data Engineer - Internal Analytics,Datadog,"New York, NY","About Datadog:

We're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale—trillions of data points per day—allowing for seamless collaboration and problem-solving among Dev, Ops and Security teams globally for tens of thousands of companies. Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way.

The Team:

We are building out a first-class Internal Analytics department composed of Data Engineers, Software Engineers, Data Analysts, and Data Scientists. If you’re excited to work on a fast-moving team using cutting-edge open-source technologies to collect, store, transform, analyze, and model data, we want to meet you.

You Will:
Ship and process data at scale using Spark, Luigi, and other open-source technologies, with programming languages like Scala and Python, as well as SQL
Route that data between different sources and destinations, including internally managed datastores and externally managed third party APIs
Work closely with every department and domain at Datadog to collect critical data and insights and power decision making across the company
Join a highly collaborative team eager to solve hard problems the right way
Grow and scale with Datadog

You Are:
You are fluent in several programming languages, such as Python, R, or Scala, and are a SQL expert
You’ve built production ETL pipelines at scale using Spark, Hadoop, or a similar distributed framework and enjoy wrangling large amounts of data in different domains
You’ve worked with data stores and/or data warehouses, such as AWS S3, AWS Redshift, or Snowflake
You have a natural curiosity and an investigative mindset; you’re driven by the “why”
You’re able to explain advanced technical concepts in a simple manner and cater to your audience
You want to work in a fast, high-growth environment and thrive in autonomy

Equal Opportunity at Datadog:

Datadog is an Affirmative Action and Equal Opportunity Employer and is proud to offer equal employment opportunity to everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, veteran status, and more. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.

Your Privacy:

Any information you submit to Datadog as part of your application will be processed in accordance with Datadog’s Applicant and Candidate Privacy Notice."
209,Data Engineer 3 - Remote,"TrueCar, Inc.","Santa Monica, CA","Job Description:

TrueCar is on a mission to revolutionize the way that consumers engage in the vehicle purchase and ownership experience. We’re building an end-to-end consumer journey that’s uplifting, empowering, and unrivaled in the marketplace, and we’re looking for the best and brightest to help us achieve our goals. We’re on the hunt for teammates who embrace challenge, relentlessly innovate, and reject the notion that ‘it can’t be done.’

TrueCar maintains a Dynamic Workplace, allowing employees to have their primary workstations at home, with office space in Santa Monica, CA and Austin, TX to be made available to individuals and teams to use as needed. Employees enjoy excellent benefits (100% employer-paid health/vision/dental premium, 401k with contribution matching, equity for eligible roles, etc.) as well as perks like monthly credits for at-home food delivery, internet/mobile phone service coverage, fitness expenses, and Caregiver support. In short, we care deeply about our teammates and build employee-centric programs that prove it.

About the Job:

The Data Engineering team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.



What you'll Do:
Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
Build complex workflows and orchestrate data dependencies.
Monitor and support data pipelines to honor internal and external SLA’s.
Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.
Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
Working with a team of engineers where mentorship is valued.
Ability to learn and adapt to continually evolving technologies in the big data ecosystem.
What you'll Need:
Bachelor degree (or Master) in Computer Science or related engineering field
3-5 years of experience programming in Java (must have)
2+ years of experience in Big Data technologies
Experience in any of big data technologies: MapReduce, Spark, HBase
Proficient in SQL and experience with RDBMS/NoSQL databases.
Experience working with Cloudera/Hortonworks/EMR distribution in AWS
Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work
Team-player with strong collaboration and communication skills, who is able to respond positively to feedback
*** While this position is open to remote work through TrueCar's Dynamic Workplace initiative, applicants may not reside in Colorado. Colorado candidates will be required to relocate. ***

Location(s):

Santa Monica, CA"
210,Senior Data Engineer,Omnidian,United States,"Posted by
Amanda Swahn
Recruitment Professional | Strategic Relationship Builder | Organization Enthusiast
Send InMail
About Omnidian
Omnidian is building a more sustainable future for the planet. We assure that IoT-enabled distributed energy assets perform at their best through our passionate teams, our innovative technology, and by creating an amazing customer experience. We're a well-funded, growth-stage company offering the potential for enormous impact, both on our products and on the world.

The Job
A Senior Data Engineer on the Data Management team at Omnidian will act as steward for the strategic data that is at the heart of Omnidian’s service offering and will be an ambassador for data governance and security. They will manage and store important data regarding company operations, working closely with other data professionals and engineers to design and implement these systems across departments. They will use their knowledge of different programming languages to code and update data systems.

What You'll Do
Build and optimize datasets, ‘big data’ data pipelines and architectures
Perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Use analytic skills associated with working on structured and unstructured datasets
Build processes that support data transformation, workload management, data structures, dependency and metadata

Who You Are
Familiarity with Data Governance concepts such as Master Data Management, Data Lineage, Data Quality, Data Catalog, and Data Access Policies
Solid understanding of types of database platforms and use cases for each
Active interest in tying work and requirements back to business outcomes and ability to bring forward new ideas on how business outcomes might be improved
Experience building, maintaining and optimizing data acquisition pipelines
Experience working with structured and unstructured data
Excellent communication and collaboration skills
Active listening skills
Curiosity/growth mindset
Empathy and respect for coworkers, partners and customers
Ability to give and receive constructive feedback

Experience with the following is a plus, but not required
Experience supporting Time Series data
Experience working in AWS, GCP or Azure
Database administration experience for structured and/or unstructured database platforms
Experience with data migration projects, particularly for managed database platforms such as RDS
Experience designing, implementing and testing DR strategies for data hosted in the cloud
Experience supporting CICD pipelines

Work-Life at Omnidian
All of our roles offer the opportunity to work remotely
If you are in the Seattle, WA area, we offer a vibrant and creative workspace in the heart of downtown Seattle
We provide outstanding benefits that we are continually improving upon, including family medical, dental, vision, disability, parental leave, unlimited PTO, paid sabbatical leave, transit and 401(k)
We offer a competitive total compensation package that includes a bonus and equity for every employee
We are dog lovers and our offices and many of our events are dog-friendly
We are a passionate, mission driven team that believes in collaboration, mutual respect and trust


#LI-REMOTE

We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our customers. We are committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. We value diversity and inclusion and are committed to ensuring our hiring and retention practices as well as our office culture reflects this value.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.

Omnidian is an equal opportunity employer. We are committed to diversity in the workplace. We make employment decisions on the basis of merit and business need. We hire without consideration to age, ancestry, citizenship, disability, gender expression, gender identity, marital status, national origin, political activity or affiliation, race, religion, sexual orientation, veteran status, or any other basis protected by law."
211,AWS Data Engineer,Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.
Work You'll Do/Responsibilities
Creating/managing AWS services
Work with distributed systems as it pertains to data storage and computing
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The AI & Data Operations team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

AI & Data Operations will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually."
212,Python Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Saama Technologies, Inc., is seeking the following. Apply via Dice today!

Bachelors or Masterrsquos degree in relevant fields. 8+ yearsrsquo experience working in building and scaling systems 3+ years of experience of working on Python ETL Experience of working in Pandas, numpy or other scientific computing libraries in Python Experience of using scheduler like Airflow to schedule Experience of building packages and deploying using Docker. Rancher or Kubernetes experience is a plus. Experience of working in AWS. Experience working on Pharma related use cases will be a big plus Has a liking for analytics and is interested in learning about artificial intelligence and its use in process disruption today Is fearless, ambitious and can confidently brainstorm, whiteboard innovative ideas with executive sponsors"
213,"Data Engineer, Snowflake",Deloitte,"Las Vegas, NV","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
214,"Data Engineer, Treasury Technology",Amazon,"Austin, TX","Job Summary

DESCRIPTION

The Treasury Technology team has an opportunity to be part of something big! Come work with the Treasury Risk Management (TRM) teams, our expanding technology organization of Product Managers, TPM’s, BI & Data Engineers to support automation and build scale into our systems and processes. Our team builds productivity tools in lieu of hiring additional operational headcount. The Treasury Technology team is looking for a Data Engineer to develop and support Treasury business intelligence, operations reporting and management systems.

About The Team

The Treasury Technology team is responsible for innovating, architecting, and building, global and scalable technology solutions that transform the Treasury Risk Management (TRM) experience. This team and the solutions we build are a critical component in TRMs continued growth.

Innovation is at the center of this team. As a Data Engineer on the Treasury Technology Team you will have an opportunity to collaborate with a team of customers, product managers, data engineers, and software development engineers in developing automated data solutions that will scale with the growing Treasury Risk Management (TRM) organization.

As a Data Engineer, you should be experienced in the architecture of DW solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business analysts and engineers to determine how best to design the data warehouse for reporting and analytics. You will be responsible for designing and implementing scalable ETL processes in the data warehouse platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune SQL to provide optimized solutions to the business.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
System Engineering experience for tools like Tableau and RPA(UI Path) is a plus.
Infrastructure management of third party applications
Preferred Qualifications
Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
Experience in translating business needs into technical requirements
5+ years of industry experience as a Data Engineer in Treasury or Risk Management
Authoritative in ETL optimization, designing, coding, and tuning big data processes
Experience in designing and implementing data engineering solutions with AWS data technologies
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1995514"
215,AZURE DATA ENGINEER,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Amiti Consulting, Inc, is seeking the following. Apply via Dice today!

Azure Data Engineer - Full Time Position with TCS"
216,Data Engineer - Associate,Morgan Stanley,"Alpharetta, GA","Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management and wealth management services. The Firm's employees serve clients worldwide including corporations, governments and individuals from more than 1,200 offices in 43 countries.

As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence and strong team ethic. Morgan Stanley can provide a superior foundation for building a professional career - a place for people to learn, to achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.

Technology

Technology works as a strategic partner with Morgan Stanley business units and the world's leading technology companies to redefine how we do business in ever more global, complex, and dynamic financial markets. Morgan Stanley's sizeable investment in technology results in quantitative trading systems, cutting-edge modeling and simulation software, comprehensive risk and security systems, and robust client-relationship capabilities, plus the worldwide infrastructure that forms the backbone of these systems and tools. Our insights, our applications and infrastructure give a competitive edge to clients'

We are looking for an experienced hands-on data engineer who will lead the technical aspects of designing, implementing, and testing of our data integration and reporting platform within the Middle Office department. The role expects significant engineering knowledge in implementing core modules for the platform and experience in building solutions in a big data streaming and analytical space.

Responsibilities
Collaborate with business and product partners to understand the business problem to develop solutions that would mitigate the problem and enhance the customer experience
Participate in architecture and design discussions, whiteboarding the solution and effectively communicating the designs to other team members
Develop software solutions in a lean-agile methodology to bring the solutions quickly to the partners and improve, optimize, and repeat in an accelerated development cycle
Independently manage the delivery of the solutions within the deadline, adhering to the company’s compliance and development standards
Present the solution to the business/technology stakeholders and the management and communicate the design choices clearly
Create technical documentation for the product, business user guide, and other runbooks both for managing the applications and to onboard new users easily
Research and learn new technologies quickly and work in parallel balancing the ongoing tasks, research tasks, production support, and mentoring others
Skills Required
Experience in a large FinTech or similar highly regulated and data centric industry
5+ years of building distributed systems using modern programming languages (preferably Java)
Expertise in data integration using standard ETL tools (i.e. Informatica), Python and Spark
Extensive knowledge in RDBMS systems such as MS-SQL or Teradata
Proficiency in handling Big Data in Hadoop, HDFS, Hive and other tools
Experience working in an Agile, Lean, or Kanban environment with a focus on team success
Shell/Python/PySpark scripts for batch processing in Windows/Linux/Unix environment
Good communication and presentation.
Growth mindset, personal excellence, collaborative spirit.
Skills Preferred
SAP HANA (or any in-memory database) experience, Kafka messaging and Spring frameworks.
Azure or other cloud platforms such as Google Cloud, AWS, SnowFlake, etc.
Experience in integrating various systems using heterogenous data sources REST/SOAP Services, JMS, Database
Experience in Data Modeling, Query tuning, Stored Procedure and NoSQL exposure
Control-M, Autosys or any equivalent scheduling software
Educational Qualification
BA/BS in Computer Science, Math, Physics, or another technical field
Posting Date

Jul 28, 2021

Primary Location

Americas-United States of America-Georgia-Alpharetta

Education Level

Bachelor's Degree

Job

Development

Employment Type

Full Time

Job Level

Associate"
217,Remote Data Engineer,Insight Global,"Nashville, TN","A client in the Nashville, TN area is looking for a Remote Data Engineer to join their team. This person will work with the internal team- architects and visualization teams to develop data models to meet the needs of the organization's data systems, and will manage and support the flow of information between analytics solutions. They will also manage the Data & Analytics ETL and reporting platforms to cleanse, validate, and prepare the data for consumption, and maintain/enhance python scripts to help get data into their data lake."
218,Data Engineer,JPMorgan Chase & Co.,"Jersey City, NJ","As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
4+ years of strong programming experience with Python or Java/J2EE and Object-Oriented technologies
Experience with Cloud (AWS preferred)
Proficiency in multiple modern programming languages
Good understanding of the data modeling and database concepts and strong SQL
Good understanding of the computer science fundamentals, data structures, algorithms and performance turning
Good understanding of asset management business and asset classes
Should be a self-starter, driven and motivated to proactively tackle projects at hand
Experience working with private or public cloud platforms
Worked under Agile and Product based development model
Experience in application, data, and infrastructure architecture disciplines
Some knowledge of architecture and design across all systems
Knowledge of industry-wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
219,Data Engineer,Veeva Systems,"New York, NY","Veeva [NYSE: VEEV] is the leader in cloud-based software for the global life sciences industry. Committed to innovation, product excellence, and customer success, our customers range from the world’s largest pharmaceutical companies to emerging biotechs. Veeva’s software helps our customers bring medicines and therapies to patients faster.

We are the first public company to become a Public Benefit Corporation . As a PBC, we are committed to making the industries we serve more productive, and we are committed to creating high-quality employment opportunities.

Veeva is a Work Anywhere company which means that you can choose to work in the environment that works best for you - on any given day. Whether you choose to work remotely from home or in our New York City office - it’s up to you.

As a Data Engineer on the New York Analytics engineering team, you will be a core contributor in building out next-generation systems and processes that allow us to ingest advertising data and prepare it for health analytics at an ever-increasing scale.

You will be responsible for the design and implementation of major subsystems, and work in collaboration with your peers and the wider engineering organization.

What You'll Do
Handle and provide feedback on ad-hoc data loads for experimental/pre-contract advertising feeds before implementing in our proprietary ingest platform.
Assist other Data Engineers with complex, large, and time-sensitive data tasks across a modern and evolving technology stack.
Work with Software Engineers to migrate custom and one-off implementations into code, collaborating on feature and functionality required with product managers.
Keep up to date on emerging technology solutions that impact the data and cloud computing domains, in particular on AWS.
Provide technical guidance and support to members of other teams across the company in your areas of expertise.
Actively work to develop technical and soft skills through training, event attendance, accreditation, and industry knowledge.
Requirements
2+ years of hands-on, directly relevant, Data Engineering experience.
Technically proficient in: Python, Relational Databases / SQL, Redshift or another MPP Data Warehouse, Linux
Experience designing and implementing systems using modern frameworks for ELT/ETL such as Apache Airflow.
Experience using AWS Database and Big Data tools – primarily we are interested in Redshift, RDS (MySQL & Postgres), and EMR.
College degree in Computer Science, Math, Systems Engineering or a similar technical field.
Learn More
Engineer Perspective: 3 Reasons to Consider Veeva
Engineering at Veeva
Nice to Have
Proficiency some of the following technologies: Additional Languages: Java, Scala, Containerization: Docker, Kubernetes, NoSQL: MongoDB, ElasticSearch, AWS: S3, RDS, Lambda, EC2, EKS, VPC, SQL, SNS, ELB, CloudFront, Implementation of a Data Lake / Lakehouse design paradigm
AWS Associate Architect or Developer certification. Professional or Specialty is a big advantage.
Perks & Benefits
Office conveniently located in midtown Manhattan and close to several major transportation hubs.
Fully stocked kitchen with snacks and beverages.
Fitness/wellness reimbursement.
Allocation for continuous learning and development.
Private roof deck and flexible working space.
Weekly happy hours and other social activities.
Veeva’s headquarters is located in the San Francisco Bay Area with offices in more than 15 countries around the world. Veeva is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristics protected by local laws, regulations, or ordinances. If you need assistance or accommodation due to a disability or special need when applying for a role or in our recruitment process, please contact us at talent_accommodations@veeva.com."
220,AWS Data Engineer,Deloitte,"New York, NY","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines.
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business.
Establish solid design and best engineering practice for engineers as well as non-technical people.
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
5+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
221,"Data Engineer, Treasury Technology",Amazon,"Austin, TX","Job Summary

DESCRIPTION

The Treasury Technology team has an opportunity to be part of something big! Come work with the Treasury Risk Management (TRM) teams, our expanding technology organization of Product Managers, TPM’s, BI & Data Engineers to support automation and build scale into our systems and processes. Our team builds productivity tools in lieu of hiring additional operational headcount. The Treasury Technology team is looking for a Data Engineer to develop and support Treasury business intelligence, operations reporting and management systems.

About The Team

The Treasury Technology team is responsible for innovating, architecting, and building, global and scalable technology solutions that transform the Treasury Risk Management (TRM) experience. This team and the solutions we build are a critical component in TRMs continued growth.

Innovation is at the center of this team. As a Data Engineer on the Treasury Technology Team you will have an opportunity to collaborate with a team of customers, product managers, data engineers, and software development engineers in developing automated data solutions that will scale with the growing Treasury Risk Management (TRM) organization.

As a Data Engineer, you should be experienced in the architecture of DW solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business analysts and engineers to determine how best to design the data warehouse for reporting and analytics. You will be responsible for designing and implementing scalable ETL processes in the data warehouse platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune SQL to provide optimized solutions to the business.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
System Engineering experience for tools like Tableau and RPA(UI Path) is a plus.
Infrastructure management of third party applications
Preferred Qualifications
Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
Experience in translating business needs into technical requirements
5+ years of industry experience as a Data Engineer in Treasury or Risk Management
Authoritative in ETL optimization, designing, coding, and tuning big data processes
Experience in designing and implementing data engineering solutions with AWS data technologies
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1932597"
222,Data Science Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, DFND Security, is seeking the following. Apply via Dice today!

We are looking for a Data Science Engineer for a long term project expected to start the first week of May. The positions are expected to last at least 12 months with a possible contract to hire situation. It is expected to be all remote with PST working hours. Some more details are below. Please let me know if you are interested and when a good time is to get in touch with you. Thanks for your time. Define, Design and Implement Greenfield Analytics Platform on Azure. Key Skills DataBricks (expert), Snowflake, Azure Additional Skills Data+ Cloud Engineer, Python, Scala Ricky Grandy"
223,AWS Data Engineer,Deloitte,"Los Angeles, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
224,Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

The Data Engineer will join a team responsible for developing advanced analytics products; applying data visualization and statistical programming tools to enterprise data to advance and enable the key mission outcomes. In this role, they will support all phases of analytic work product development, from the identification of key business questions, through data collection and ETL, from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. This role requires special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree in STEM field
2+ years of experience with programming languages such as Python, R, SPSS, SAS, SQL
2+ years of experience with data visualization tools, such as Tableau, Qlik, PowerBI, or equivalent
2+ years of experience with ETL/ETL Pipeline, Data Warehouse Development and data modelling
Preferred:
Prior professional services or federal consulting experience
Experience with ETL, NoSQL Apache Hadoop, and cloud computing technology, especially Microsoft Azure
How You'll Grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
225,Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

We are looking for experienced Data Engineers to build and deliver innovative, game-changing mission-driven data pipelines. On this project, you will be responsible for leading the architecture and setup of hosted data lakes, as well as the ingestion pipeline and processing for large datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required clearance for this role
Travel up to 10%
2+ years of experience with extract, transform, and load (ETL) methods and tools
2+ years of experience with data modeling, data warehousing, and building ETL pipelines
2+ years of experience with SQL queries and JSON objects
1+ years of experience with both SQL and NoSQL databases, including PostgreSQL and MongoDB
Preferred:
Familiarity with microservice architectures
Interest in event streaming architectures, such as Apache Kafka
Prior professional services or federal consulting experience
Knowledge of data mining, machine learning, data visualization and statistical modeling
Ability to thrive in a fast-paced work environment with multiple stakeholders
Creativity and innovation - desire to learn and apply new technologies, products, and libraries
How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
226,Data Engineer/Remote/100-130kDoe,Motion Recruitment,"Philadelphia, PA","Data Engineer/Remote/100k-130k DOE

Our client is a Financial institution looking to hire a full time data engineer. They are developing a new application geared towards helping there clientele have an easier time navigate the stock market. The data engineer that they are looking to higher must be well versed in python, SQL, as well as aws. Familiarity with building pipelines is a plus as well as experience working with databases. This job will also be %100 remote.

Required Skills & Experience
3+ years of professional,
Python
AWS
SQL
+Building Piplines
+Database work
The Offer
Competitive Salary: Up to $100,000-130,000/year, DOE
You Will Receive The Following Benefits
Medical, Dental, Vision Insurance
401(k) with 3% matching
15 days of PTO
%100 remote
Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Posted By: Peter Degnan"
227,"Data Engineer, Search M5",Amazon,"New York, NY","Job Summary

DESCRIPTION

Enjoy Big Data, like to work on cutting edge technologies or create new, better, smarter ML algorithms layered on existing cool tech? Are you smart and eager and want to work with a team of senior engineers and the best Scholars, Economists & Machine Learning Scientists at Amazon? If yes, keep reading!

The Search M5 team at Amazon works with senior management on key business problems faced in retail, international retail, supply chain, traffic, search, pricing, cloud computing, third party merchants, Kindle and operations. We apply the frontier of AI thinking to market design, pricing, forecasting, online advertising, supply chain network planning and other areas.

As a Data Engineer Manager, you will collaborate with the top scholars, economists, machine learning scientists, software developers and other data engineers across the company to develop, test, and deploy services that implement a wide range of econometric and machine learning models for some of the most confidential projects at Amazon. This requires the use of sophisticated distributed systems, the processing and visualization of Big Data and application of advanced ML techniques. A successful candidate should have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
Bachelor's Degree in Computer Science, Information Systems Management, mathematics, or other related fields
5+ years of experience working directly with engineering teams on complex ML/DL/RL and infrastructure products
5+ years of technical program management experience
5+ years Architectural design or system design experience
5+ years experience managing programs cross functional teams, building processes, and coordinating milestones, online A/B and/or off-line testing, and prototype release schedules
Experience with software methodologies such as Scrum, Kanban.
Preferred Qualifications
Master’s degree in Computer Science, Computer Engineering or related technical discipline
Experience with Hadoop, MapReduce, Spark, Cassandra or other Big Data processing platforms
Database design experience (SQL/NoSQL)
Experience building complex software systems that have been successfully delivered to customers and had high impact
Knowledge of one or more of the following areas: econometrics, statistical modeling, machine learning, data mining
Broad ability to take a project from scoping requirements through launch and operations of the project
Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1946932"
228,Staff Data Engineer,Pave,"New York, NY","About Pave

At Pave, we believe the world of compensation is broken, and we’re going to fix it. Today, teams cobble together hundreds of messy spreadsheets and outdated surveys to determine how to compensate their employees. At best, they’re leveraging stale data from an industry that is quickly evolving past it. Add COVID, a new remote and distributed workforce, and you have an even blurrier picture of what “market compensation” is, how it’s evolving, and how to communicate it to employees.

That’s where we come in. Pave allows companies to benchmark compensation to leaders in their industry, analyze internal compensation data and make the right adjustments, then visually communicate compensation to their employees. We’re building the world’s largest real-time compensation data platform on the path to help employers and employees navigate the murky world of compensation with clarity, equity, and accessibility. And you don’t have to just hear it from us — you can hear it from our customers: Allbirds, Hover, Shopify, Discord and more.

Our Team

The mission of the Data team at Pave is to leverage the rich data produced by the Pave ecosystem to enable Pave to provide the most valuable insights to its users as well as to serve as the core optimization engine for the company. We build data infrastructure, define and evaluate metrics of success, derive actionable insights, and build predictive models to enable a culture of data-driven decision making and optimize every aspect of our business.

Your Primary Focus

Build and communicate a clear vision for how Pave should architect its data platform
Mentor and develop junior members of the team
Ensure that the Data team has a data infrastructure aligned with team and scalability needs
Be a role model for excellence across Data team
Work with stakeholders to ensure that what we build aligns with the needs of the business
About You

Bachelor’s degree in Computer Science, Math, Statistics, Engineering, or a related quantitative field
7+ years of experience in a data engineering or similar role
Expert knowledge of data warehouse design and management
Expert knowledge of core infrastructure such as orchestration tools, data warehouses, dashboarding platforms, cloud infrastructure
Strong communication skills
Comfortable working in fast-paced environments
A final note — we highly encourage you to apply for this role, even if you don’t feel entirely qualified, or entirely sure. You never know!

Our Mission

🎉 Make compensation open, transparent, and fair.

Our Values

💡Be Intellectually Honest

☀️ Be Transparent

🔥 Bring Your Fire

⏰ Focus On Impact

🤗 Hug Of Jawn

💪 Stretch The Rubber Band

Life at Pave

Pave is growing incredibly fast, and we have high ambition. We've complemented our ambitious goals with a world-class culture and a variety of amazing benefits. Some of these include:

Compensation: Competitive salary and startup equity (it's in our DNA)

Wellness: Top tier health insurance along with exercise & wellness benefits

Food: All meals while working, plus snacks. We take our snacking seriously

Commute: Commuter pre-tax benefit accounts + employer contribution

Tech: Choose your laptop and accessories of choice

Team: Retreats, happy hours, and events for our team, friends, and family

Flexible Time Off: Take the time you need - we encourage our team to unplug with unlimited PTO

WFH Wednesday: A dedicated work from home day each week + additional flexibility by team!

Location: Our company HQ is in San Francisco's SoMA district with a high energy in-person culture. We also have a hub in NYC!

Pave is committed to a diverse and inclusive workforce. We are an equal opportunity employer and do not discriminate on the basis of race, ethnicity, gender, gender identity, sexual orientation, protected veteran status, disability, age, or another legally protected status. For individuals with disabilities who would like to request accommodation, please email recruiting@pave.com."
229,"Data Engineer, Search M5",Amazon,"New York, NY","Job Summary

DESCRIPTION

Enjoy Big Data, like to work on cutting edge technologies or create new, better, smarter ML algorithms layered on existing cool tech? Are you smart and eager and want to work with a team of senior engineers and the best Scholars, Economists & Machine Learning Scientists at Amazon? If yes, keep reading!

The Search M5 team at Amazon works with senior management on key business problems faced in retail, international retail, supply chain, traffic, search, pricing, cloud computing, third party merchants, Kindle and operations. We apply the frontier of AI thinking to market design, pricing, forecasting, online advertising, supply chain network planning and other areas.

As a Data Engineer, you will collaborate with the top scholars, economists, machine learning scientists, software developers and other data engineers across the company to develop, test, and deploy services that implement a wide range of econometric and machine learning models for some of the most confidential projects at Amazon. This requires the use of sophisticated distributed systems, the processing and visualization of Big Data and application of advanced ML techniques. A successful candidate should have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
Bachelor's Degree in Computer Science, Information Systems Management, mathematics, or other related fields
5+ years of experience working directly with engineering teams on complex ML/DL/RL and infrastructure products
5+ years of technical program management experience
5+ years Architectural design or system design experience
5+ years experience managing programs cross functional teams, building processes, and coordinating milestones, online A/B and/or off-line testing, and prototype release schedules
Experience with software methodologies such as Scrum, Kanban.
Preferred Qualifications
Master’s degree in Computer Science, Computer Engineering or related technical discipline
Experience with Hadoop, MapReduce, Spark, Cassandra or other Big Data processing platforms
Database design experience (SQL/NoSQL)
Experience building complex software systems that have been successfully delivered to customers and had high impact
Knowledge of one or more of the following areas: econometrics, statistical modeling, machine learning, data mining
Broad ability to take a project from scoping requirements through launch and operations of the project
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1946931"
230,Data Engineer - Data Science Engineering,Datadog,"New York, NY","About Datadog:

We're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale—trillions of data points per day—providing always-on alerting, metrics visualization, logs, and application tracing for tens of thousands of companies.

Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way. We need you to design and build machine learning-powered products that help our customers learn from their data and make better decisions in real-time.

The team:

We extract and manage data and events from our core products and live systems to make them centrally available for our Data Science team in both batch and real-time ways. We enable Data Scientists to productionize their models and expose their data assets to the rest of the company.

If you’re excited to work on a fast-moving data engineering team with the best open-source data tools at high scale, we want to meet you.

You will:
Build distributed, real-time, high-volume data pipelines and work together with others to enable high-scale Data Science
Do it with Spark, Luigi, Kafka and other open-source technologies
Work all over the stack, moving fluidly between programming languages: Scala, Java, Python, Go, and more
Join a tightly knit team solving hard problems the right way
Own meaningful parts of our service, have an impact, grow with the company

Requirements:
You have a BS/MS/PhD in a scientific field or equivalent experience
You have built and operated data pipelines for real customers in production systems
You are fluent in several programming languages (JVM & otherwise)
You enjoy wrangling huge amounts of data and exploring new data sets
You value code simplicity and performance
You want to work in a fast, high growth startup environment that respects its engineers and customers
You are preferably familiar with Spark and/or Hadoop and know how to put machine learning models in production

Is this you? Send your resume and link to your GitHub if available.

Equal Opportunity at Datadog:

Datadog is an Affirmative Action and Equal Opportunity Employer and is proud to offer equal employment opportunity to everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, veteran status, and more. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.

Your Privacy:

Any information you submit to Datadog as part of your application will be processed in accordance with Datadog’s Applicant and Candidate Privacy Notice."
231,Data Engineer,Stitch Fix,"San Francisco, CA","About The Team

The data engineering team is a small, nimble group of data engineers that drive the company toward clean and informative data. As a member of the data engineering team, you’ll contribute toward a clear, concise data model to help power data science, ETLs and tools to make us efficient, as well as self-service data and tools to facilitate scalable decision-making. As a team, we are driven by the thrill of helping our colleagues use data with less friction, which ultimately increases the velocity at which the business can progress!

About The Role
Individual contributor position on the data engineering team, within our Algorithms organization, working with a team that focuses on our internal business partners
You will build and own large additions to our data engineering framework, contributing to a code framework that centralizes ETL logic and definitions
You will help to define, build and maintain a clear, concise data model, especially focused on scalable analytics infrastructure
You will build scalable data engineering solutions & frameworks to solve business and data problems
You will be involved in the day-to-day operations of the team, including maintaining and improving our current tools & scripts and supporting data that powers our business
You will have autonomy to help shape the future of data engineering at Stitch Fix by bringing your ideas on improving and automating what we do and how we do it

You’re Excited About This Opportunity Because You Will...
You will work with a variety of cross functional partners including product managers, analysts and data scientists to deliver up-to-date metrics to our partners in Merchandise, Styling, CX and Operations.
You will focus on our data infrastructure, optimization, and scalability
Be part of a fast-growing team which has high visibility across the organization
Contribute ideas and direct the team’s investment to impactful directions
Contribute to a culture of technical collaboration and scalable development

We Get Excited About Candidates Who Have…
5+ years of independent and significant project experience
Experience in building out data models and data engineering capabilities
Experience coding and designing extensible and reusable Python and SQL
Experience in working autonomously and taking ownership of projects.
Ability to think globally, devising and building solutions to meet many needs rather than completing individual projects or tasks
Strong prioritization skills with business impact in mind
Familiarity with using Spark to access an S3 data warehouse
Strong cross functional communication skills that help simplify and move complex problems forward with business partners

YOU’LL LOVE WORKING AT STITCH FIX BECAUSE…
We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!
We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation
We are a technologically and data-driven business
We are committed to our clients and connected through our vision of “Transforming the way people find what they love”
We love solving problems, thinking creatively and trying new things
We believe in autonomy & taking initiative
We are challenged, developed and have meaningful impact
We take what we do seriously. We don’t take ourselves seriously
We have a smart, experienced leadership team that wants to do it right and is open to new ideas
We offer competitive compensation packages and comprehensive health benefits
You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day

About Stitch Fix

At Stitch Fix, we’re about personal styling for everybody, and we believe in both a service and a workplace where you can be your best, most authentic self. We’re the first fashion retailer to combine technology and data science with the human instinct of a Stylist to deliver a deeply personalized shopping experience. This novel juxtaposition attracts a highly diverse group of talented people who are both thinkers and doers. All of this results in a simple, powerful offering to our customers and a successful, growing business serving millions of men, women, and kids. We believe we are only scratching the surface on our opportunity, and we’re looking for incredible people like you to help us carry on that trend.

Please review Stitch Fix's Recruiting Privacy Policy here:

https://www.stitchfix.com/privacy/usrecruitingprivacy"
232,Senior Data Engineer,Angi,"New York, NY","Angi® is transforming the home services industry, creating an environment for homeowners, service professions and employees to feel right at “home.” For most home maintenance needs, our platform makes it easier than ever to find a qualified service professional for most indoor and outdoor jobs, home renovations (or anything in between!). We are on a mission to become the home for everything home by helping small businesses thrive and providing solutions to financing and booking home jobs with just a few clicks.

Over the last 25 years we have opened our doors to a network of over 200K service professionals and helped over 150 million homeowners love where they live. We believe home is the most important place on earth and are embarking on a journey to redefine how people care for their homes. Angi is an amazing place to build your dream career, join us—we cannot wait to welcome you home!

The Team and Role

Angi is looking for a Senior Data Engineer to play a key role on the Data Engineering team. The successful candidate will develop and maintain strong relationships with teammates while ensuring delivery of high quality Engineering solutions. The ideal candidate will have outstanding communication skills, proven data infrastructure design and implementation capabilities, strong business acumen, and an innate drive to deliver results. He/she will be a self-starter, comfortable with ambiguity and will enjoy working in a fast-paced dynamic environment.

As a Senior Data Engineer, you will be responsible for:
Establishing and instilling innovative practices, patterns, and toolkits to deliver enterprise-grade data assets.
Interact closely with stakeholders to determine analytics needs and translate those into efficient and scalable data processes
Partnering with passionate counterparts to deliver awesomeness and continuously evaluate the best way to deliver short-term and long-term solutions
The folks in this role are usually successful when they have experience in:
Extensive hands on experience in developing reusable data integration and streaming platforms using Python or another comparable language
Broad knowledge of data infrastructure ecosystem
Experience with modern cloud database platforms, such as Snowflake or Redshift
Strong Analytical and SQL skills with demonstrated strength in data modeling, ELT development, and data warehousing
Experience with GitLab, CI/CD workflows, AWS services, containerization (Docker), Grafana, and orchestration tools
Proven track record of sharing outcomes through written communication, including an ability to effectively communicate with both business and technical teams
Compensation & Benefits
The salary band for this position ranges from 100k-210k, commensurate with experience and performance. Compensation may vary based on factors such as cost of living.
This position will be eligible for a competitive year end performance bonus & equity package.
Full medical, dental, vision package to fit your needs
Flexible vacation policy: work hard and take time when you need it
Pet discount plans & retirement plan with company match (401K)
The rare opportunity to work with sharp, motivated teammates solving some of the most unique challenges and changing the world
#BI-Remote"
233,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, 4dot5, is seeking the following. Apply via Dice today!

Data Engineer 6+ Months Contract Remote 4-6 yearsrsquo experience of IT experience Hands on experience with AWS Data Proficient and hands on experience with PySpark Knowledge of application development lifecycles, continuous integrationdeployment practices Experience in Data application development Experience working with Spark and real-time analytic frameworks Willingness to continuously learn share learnings with others Strong troubleshooting and performance tuning skills. Experience building multi-tenant, virtualized infrastructure a strong plus Required Skills ndash Glue, PySpark"
234,Data Engineer Fellowship - 1 Year,Dice,"Wilmington, DE","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Tech Impact, is seeking the following. Apply via Dice today!

TECH IMPACT - Data Innovation Lab Data Engineer Fellowship - 1 Year We are inviting you to be a part of our Fellowship cohort to bring your diverse technical talents together with us to make the world a better place. Full-Time Positionndash 12 Month Fellowship - June 2022 - May 2023 Benefits Offered Salary ndash 70,000 USD Medical, Dental, Vision - 90 cost paid by Tech Impact 15 Days Paid Time Off Job placement upon completion of the 12-Month Fellowship Location ndash New Castle County, Delaware Hybrid position (60 Remote) Participants expected to establish Delaware residency during the Fellowship The Data Innovation Lab at Tech Impact uses data for social good. The Lab was built to solve community challenges with data ndash from housing insecurity to economic instability. We develop models and create products that are used by community partners ndash from government agencies to nonprofit organizations ndash to directly address these challenges. Position Summary The Data Lab Fellowship is a year-long program intended for technologists with a deep technical skillset and who have a strong desire to serve their communities. As a Data Engineer on the Data Lab Fellowship, you will work with our partners to develop and deploy production-grade solutions to social good problems. You will be expected to collaborate across system integration projects, ideally with the creation of solutions that involve ETLELT and data modeling and implementing data warehouses that support our data narrative builds aiming to advance our mission. Your work will directly lead to improving the lives of people in communities in Delaware and beyond. Duties and Responsibilities Consult clientsstakeholders on the design and build of data warehouses Serve as our primary delivery resource for designing and building data warehouses Design, build, and deploy ETLELT solutions across different projects among our team Create and manage Cloud services (Azure, AWS) to support all aspects of our tech products including their delivery Organize and communicate personalized findings to clients andor stakeholders As part of the broader data team, propose innovative solutions to address sourcing and bridging datadata knowledge gaps amongst our team Create and develop new ways of applyingoptimizing analytics solutions to increase their valuecontribution Contribute to innovative projects, specifically to idea generation, idea incubation andor experimentation Serve as a liaison between government, corporate, community, and non-profit teams to effectively source and represent data When relevant, support authorship of academic papers, presentations, white papers, and other publications by conducting analyses and producing visualizations. Essential Skills for Success in Position A Masterrsquos degree or above in statistics, mathematics, computer science, psychology, public health, or another related quantitative field (those graduating by June 20th, 2022, are eligible to apply) A background andor strong commitment to leveraging data for social good A minimum of 2 years' experience in system integration projects, ideally with creation of solutions that involve ETLELT andor data modeling and implementing data warehouses that support interactive dashboardsdata visualizations Strong production-level experience with relational (SQL Server, Oracle) and columnar databases (Synapse, RedShift, Snowflake, Big Query) required Strong production-level SQL and PythonR coding skills required Ability to design and implement RESTful APIs Previous experience working with data science andor research teams Experience with implementing modern data pipelines (batch, real-time) using cloud based services or enterprise grade data integration toolsplatforms (Informatica, Talend, SSIS, Azure Data Factory, etc.) Experience with implementing and managing, Cloud based data services (preference given to those with AWS experience) Experience with DevOps and CICD processes (Git, Azure Pipelines, Jira, Confluence, etc.) Experience with Agile or Scrum methodologies highly desirable Experience handling confidential data and information with integrity in a shared cloud environment Excellent interpersonal and communication skills, with the ability to engage with colleagues on both technical and narrative build projects Proven ability to understand business processes in detail ndash both at global and countystate levels ndash and translate these into build data narratives for social good A self-motivated thinker that works well both independently and in collaborative teams. Excellent organizational skills, with the ability to navigate organizingprioritizing work efficiently and effectively Experience handling confidential data and information with integrity in a shared cloud environment Employment Policy As an Equal Opportunity Employer, Tech Impact embraces diversity and does not discriminate in employment based on age, race, color, creed, sex, gender, identity, religion, marital status, veteranrsquos status, national origin, disability, or sexual orientation. ADA Specifications Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position. Requires the ability to speak, hear, see and use a computer and other office-related equipment. Also requires the ability to sit for extended periods of time, stand, stoop, crawl and lift to 50 lbs. May require local and regional travel."
235,"Data Engineer, Spark",Deloitte,"Fort Worth, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
236,"Data Engineer, Spark",Deloitte,"Newtown, PA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
237,"Data Engineer, Snowflake",Deloitte,"New Orleans, LA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
238,"Data Engineer, Spark",Deloitte,"Atlanta, GA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
239,Data Engineer - Data Management Product Line.,Johnson & Johnson,"Titusville, NJ","Janssen Pharmaceuticals, Inc., a member of Johnson & Johnson's Family of Companies is currently recruiting for a Data Engineer - Data Managemetn Product Line for the Pharma Business Unit Data Science/Data Management Product Line. The primary location is in Titusville NJ and may require up to 10% domestic travel.

At the Janssen Pharmaceutical Companies of Johnson & Johnson, what matters most is helping people live full and healthy lives. We focus on treating, curing, and preventing some of the most devastating and complex diseases of our time. And we pursue the most promising science, wherever it might be found. Thriving on a diverse company culture, celebrating the uniqueness of our employees, and committed to inclusion. Proud to be an equal opportunity employer

You will play a key role building and deploying various data management solutions in realization of our Data Management strategy to transform the Pharma Commercial Regional Data Management capabilities. The DATA ENGINEERING & ENABLEMENT - IT LEAD will work with associates to implement data cataloging, data warehousing and governance capabilities/procedures to empower the Janssen Scientific and Medical Affairs business users to quickly identify insights and drive business performance. The IT Lead will collaborate with IT peers, Product Lines, Business Technology Leaders, Business Product Owners, Technology Services to apply Enterprise technology and development capabilities to deliver solutions that rapidly meet business needs. Key skills are needed to build data management capabilities in the realm of Big Data, Business Intelligence, Data Warehousing and Analytics, Metadata Management, Self Service.

Essential Duties And Responsibilities
Work with Business Partners, IT Application Services teams and Global Technology Services to manage solution development by leveraging SDLC and Agile methodology to plan, design and deliver projects.
Cognizant of Data Governance procedures and engage with cross functional teams to stay informed of the ways the organization uses its data and understand the potential impact to delivery of specific initiatives.
Maintain solid business understanding of entire application landscape, and be the DRI for all business and technical aspects of the application
Understand and articulate estimates for different levels of development effort to address the changing priorities and requirements in a fast-paced environment
Oversee operations by coordinating and communicating support activities between Business and TS/IT partners ensuring Service Level targets (SLAs) are met.
Ensure information security, data privacy and compliance requirements are met by participating in compliance reviews and other similar activities.
Understand and uses J&J enterprise architecture guidelines and best practices to accelerate our ability to meet current business needs, to scale and to adapt to dynamic business needs.
Take on data architecture ownership to develop data models, metadata solutions which meet the needs of the organization's information systems, manages data dictionary, data catalog, data quality and the flow of information using variety of data platforms.
Help formulate the next generation analytics environment, providing self-service, centralized platform for all data-centric activities which allows a full 360-degree view of customers from product usage to back office transactions.
Analyze internal and external data sources like Sales & Marketing, Medical Affairs, Contracting centric datasets to discover data and articulate the data scope and target models to application development teams and business.
Perform impact analysis for new releases/changes, data investigations and post-implementation troubleshooting of new applications and application upgrades.
Implement solutions leveraging AWS cloud stack components (S3, RDS, Redshift, EMR, Spark, Sqoop, Lambda, IAM, Glue, EKS), NoSQL, MySQL, Python, PySpark, Tableau and Qlik toolset or similar.
Understand upcoming technologies in the data management and data science space and be able to evaluate them and improve technical practices.
Understand the current technical delivery model for data platform development to enable ""economies of scope""
Monitor and optimize performance and cost of AWS Cloud infrastructure by tracking the usage metrics and exploring newer and cost-effective approaches.

Technology landscape at Janssen
Cloud Platforms -AWS (EC2, EMR, S3, Redshift, Spectrum, Athena, RDS, Lambda etc.). Knowledge of GCP, Azure, Veeva CRM, Veeva Vault is a plus
Data Integration Tools - EMR/Hadoop, Spark, Sqoop, Dataiku, APIs
Programming Languages - Python, PySpark
Database - Oracle, mySQL, Redshift
Automation Tools - Control-M, Jenkins(CI/CD), Bitbucket

Qualifications
Bachelors’ Degree in Computer Science, Math, Statistics, Information Systems, Information technology or similar.
Knowledge of pharma scientific and medical affairs business domain (CRM Interactions, Contact Center operations, Non-promotional and Scientific Knowledge Document review processes, EMR/EHR etc.
Demonstrated ability working with business partners and technology resources to translate business needs into data and analytics solutions using Agile methodology.
Experience in data profiling, data modeling, data ingestion and designing data pipelines to meet various analytical needs.
Hands-on experience in data analysis and connecting disparate data sources preferably CRM, RWE and Google Analytics data.
Demonstrated expertise in formulating business rules, design data quality framework and data cataloging.
Experience in distributed programming with Python, Sqoop, Spark(PySpark), NoSQL databases, Unix Scripting, MPP, RDBMS databases for data integration is required.
Hands on experience on with Amazon Web Services like S3, EC2, Redshift, RDS, EMR, Athena, Glacier, IAM is required.
Experience on Sales Force & Veeva object models is preferred.
Knowledge and hands-on experience with API security standards, design standards, patterns, and best practice implementation - Preferred

Do you strive to join an outstanding team that is dynamic and constantly evolving? Is career growth and opportunity appealing to you? Apply to this opportunity today.

Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability

At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional, and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.

For more information on how we support the whole health of our employees throughout their wellness, career and life journey, please visit www.careers.jnj.com.

Primary Location

United States-New Jersey-Titusville-1125 Trenton Harbourton Road

Organization

Johnson & Johnson Services Inc. (6090)

Job Function

Info Technology

Requisition ID

2206007418W"
240,Index Data Algorithm Engineer,"eVestment, A Part of Nasdaq","Maryland, United States","We are seeking an experienced financial algorithm / data engineer to join our effort in developing the next generation of index construction capabilities.

The vision for the Index Data Platforms and Models (DP&M) team is to improve client experiences by using data as a means of empowering the Nasdaq Index Business with data-driven insights and solutions. Within this organization, you will be focused on building modular functions for data transformation and index construction algorithms (e.g., security identification, factor screening, and weighting techniques) that support construction and rebalance of Nasdaq’s index suite.

In this role, you will be responsible for working with Research and Development and Index Portfolio Management while collaborating with our extended Data Platforms and Models team to deliver tangible value in the form of coded algorithms that translate a variety of input data into index portfolios by applying data transformation and portfolio construction methodologies, techniques and strategies.

The ideal person possesses a portfolio construction and data analytics acumen, a strong understanding of data technologies, and expertise in software engineering best practices.
Build, automate, maintain, and monitor index construction and maintenance algorithms using standardized methodologies, with reliability and scalability in mind
Improve the performance and reliability of our index research, historical simulation, portfolio construction, and rebalance and reconstitution processes and algorithms
Work in collaboration with Research and Development to translate their needs into scalable, standardized solutions
Develop standards and processes for onboarding new data transformations into research and production
Collaborate with other specialists, product managers, and engineers to identify and solve critical business problems, crafting a positive experience for internal and external clients
Bachelor’s degree in Computer Science, Mathematics, Statistics or related area
5+ years of experience in Software Engineering, Data Engineering, or Trading Strategy Modeling
5+ years working with relational databases and query languages, including building data pipelines and the ability to work across structured, semi-structured and unstructured data
5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
Experience working with cloud platforms (AWS preferred), development and operations technologies (e.g., Kubernetes, Gitlab), and data management utilities (Databricks, Apache Parquet, etc.)
Strong quantitative reasoning skills and an interest in working at the intersection of research and software engineering
Experience preparing data for analytics and following a data science workflow to drive business results
Practical experience with Agile principles, preferably with Scrum, Kanban, or SAFe frameworks
Promotion of a strong control environment, adherence to risk controls and procedures, and process discipline, while appropriately considering business priorities
Come as You Are

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation."
241,Data Engineer - Innovate on a New Platform in AWS,Amazon Web Services (AWS),"Seattle, WA","Description

Do you want to innovate on a new platform with Amazon Web Services (AWS)?

Come join our teams in AWS, you’ll help us design and deliver data engineering solutions that enables AWS to expand our operations into new data centers worldwide. Our green-field solutions will support programs in compliance and sustainability, and build a platform that will extend to first-class AWS services. You will collaborate with engineers designing data solutions at massive scale.

To succeed in this role, you have 4+ years of experience and you believe in high standards for writing SQL, building data pipelines with infrastructure as code, code quality, code reviews, testing, and operational excellence. You support your peers to deliver results in a space that operates much like a start-up, with the autonomy to build the right solution from Day One. As a DE with our team, you will collaborate with other engineers on technical direction, strategy with a career opportunity for growth that AWS can offer.

This role is located in Seattle, Portland, San Francisco, San Luis Obispo, Boston, New York City or Arlington. We have flexible work options, enabling you to work at home and in the office when the team collaborates. Our team also puts a high value on work-life balance, family-first approach. We care about your career growth and strive to assign projects based on what will help each team member grow.


Basic Qualifications
Bachelor’s Degree in Computer Science or related field, or equivalent experience
4+ years professional experience in data engineering, business intelligence, data science or related field
Experience with technologies including Big Data, EMR, EML or similar solutions
Demonstrated strength in data, development, and data warehousing - Experience in Python, Java or other similar languages
Experience with data management fundamentals and data storage principles
Experience with distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience with serverless computing, enterprise-wide systems, and AWS products
Understanding of Agile software engineering practices
Meets/exceeds Amazon’s leadership principles requirements for his role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, visit https://www.amazon.jobs/en/disability/us


Company - Amazon Web Services, Inc.

Job ID: A1761812"
242,"Data Engineer, Spark",Deloitte,"Huntsville, AL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
243,Data Engineer,MSCI Inc.,"New York, NY","Your Team Responsibilities

This is an exciting opportunity to join MSCI as a Data Engineer, to play a leading role in designing, architecting, and implementing world class data engineering solutions for commercial Real Estate market data.

What we will offer you: Depending on your location of your role, you can expect…

Competitive fixed and variable compensation, holiday/vacation allowance and retirement savings plans/pensions
A wide range of benefits including - healthcare, dental plans, risk insurances and (location dependent) - cycle-to-work schemes, gym benefits, retail discounts
A hybrid work environment, for the vast majority of employees, offering a more progressive way of working to give you the flexibility, accountability, and responsibility to empower you to perform at your very best. At the center of this way of working is a culture which is built on a strong foundation of trust. We trust our people and they trust our organization’s leaders to support them.
An inclusive and performance driven culture where you are empowered to maximize your potential in an environment where all individuals are respected and encouraged to bring their authentic selves to work
A purposeful approach to Wellbeing to provide you with all the resources you need to be your best at work and in your personal life. Our Here For You Employee Assistance Program provides confidential emotional support with local experts, financial and legal advice in a wide range of issues as well as access to online information, resources and tools. All the services are available for our employees free of charge.

Your Key Responsibilities

Operationalize our data lake platform built on S3 and Dremio
Optimize our data lake storage, leveraging file compaction and archiving strategies
Create and maintain our data catalog
Implement a layered architecture to enable self-service business intelligence and data science initiatives while maintaining query performance
In this capacity, you will be hands-on in the design and implementation of our new platform while working in a small, cross-functional team

Your Skills And Experience That Will Help You Excel

Strong expertise in advanced data modeling, schema and ETL process design, implementation and maintenance
Experience with numerous Data Lake and Analytics technologies (Dremio preferred, Snowflake, AWS Athena, Presto)
Strong knowledge of Data Lake best practices
Strong background in advanced SQL and focused on understanding, manipulating, processing and extracting values from large datasets and data streams
AWS experience (Glue, Lambda, Fargate)
Scripting and coding experience (Python, Typescript)
Experience with different variety of data types (JSON, Parquet, Excel, Flat files)
Experience with databases and data stores including: Microsoft SQL, Postgresql, Elasticsearch, Mongo, AWS S3

How We’ll Support You

Coaching, support and encouragement from experts in your team
Access to a wide range of learning opportunities to support your growth and development
Customizable learning journeys to help you navigate your professional development
Transparent performance-based compensation schemes
Employee Resource Groups: MSCI Pride, Black Leadership Network, Women’s Leadership Forum, Women in Tech, Asian Support Network, All Abilities, Hola! MSCI and Climate Action Group.

About MSCI And Our Teams

MSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.

Our values define the working environment we strive to create. We are inclusive, we champion bold ideas, we always pursue excellence, and always act with integrity. Personal accountability and responsibility are key to success, and we always work as a team to remain client centric.

At MSCI diversity is at our core and inclusion defines our culture. Our people are empowered to maximize their potential in an environment where all individuals are respected and encouraged to bring their authentic selves to work. We work, learn and grow together and we are committed to hiring and promoting qualified women, ethnic minorities and members of the LGBT+ community candidates, who have historically been underrepresented in financial services.

To learn more about life at MSCI please visit https://careers.msci.com or follow us on LinkedIn, Twitter, Glassdoor.

To all recruitment agencies: MSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.

MSCI Inc. is an equal opportunity employer committed to diversifying its workforce. It is the policy of the Firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected pregnancy/maternity leave), veteran status, or any other characteristic protected by law.

Annual"
244,Data Engineer,Dice,"Jersey City, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Kellton Tech, is seeking the following. Apply via Dice today!

Kellton Tech is a full-service software development company, offering end-to-end IT solutions, strategic technology consulting and product development services in Web, SMAC (Social, Mobile, Analytics, Cloud), ERP-BPM, and IoT space Our methodology of inventing infinite possibilities with technology helps us develop best in-class and cost effective solutions for our clients. Currently Kellton Tech is looking for talented resources for one of their listed clients. Below are further details on the positions Data Engineer Job Title ndash Data Engineer Number of Positions - 3 Job Location - NJ Job Description - Develop and maintain data pipelines and ETLrsquos, interacting with disparate data sources and multiple database servers. Bring analytical models and scripting to a production setting and push regular datamodel updates to in-production projects. Setup automation related to datamodel monitoring and performance evaluation Develop andor maintain client-facing applications including interactive dashboards and APIs Basic Requirements BS in Computer Science, Data Engineering, or equivalent 2+ years of professional software development or data engineering experience Strong CS and data engineering fundamentals Proven fluency in SQL and Python Understanding of ETLELT pipelines and Data Warehousing design, tooling, and support Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Lake) Strong communication skills in writing, conversation, Experience with tools we use every day Storage Snowflake, AWS Storage Services (S3, RDS, Glacier, DynamoDB) ETLBI Matillion, Looker Preferred Requirements Experience with ETL Dbt, Airflow Cloud Infrastructure AWS Kinesis, Lambda, API Gateway, Terraform Experience with tools we donrsquot use, but should, Proven passion and talent for teaching fellow engineers and non-engineers Proven passion for building and learning open source contributions, pet projects, self-education, Stack Overflow Experience in education or ed-tech Thanks Regards Nick Reddy"
245,Senior Data Engineer - Base Compute Data Analytics,JPMorgan Chase & Co.,"Plano, TX","We are looking for a savvy Data Engineer to join our growing team of platform engineers. We are building analytical tools and data products enabling observability of our cloud infrastructure. You will be responsible for building data solutions from the ground up-including new data pipelines to support business intelligence, enabling new valuable machine learning driven insights and observability--and optimizing data flow and collection from cross functional teams.

The ideal candidate is an experienced data pipeline builder, with a modern skillset, and data wrangler who enjoys building scalable data systems from the ground up. You must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or re-designing data architecture to support the delivery of value to our internal customers with next generation of products and data initiatives

Qualifications
Advanced working knowledge and experience working with a variety of data sources including relational databases, NoSQL stores, data API's--familiarity with graphQL and graph database engines is a plus
Experience building and optimizing 'big data' data pipelines, architectures and datasets
3+ years working with containerization technology (Docker, Kubernetes, etc) and cloud infrastructure providers
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured datasets
Experience building processes supporting data transformation, data structures, metadata, dependency, quality controls, and workload management
A successful history of sourcing, manipulating, processing, and extracting value from large disconnected datasets in a scalable manner
Working knowledge of message queuing, stream processing, and 'big data' data stores
Strong organizational skills-and a strong customer focus mindset
Experience supporting and working with cross-functional teams in a dynamic environment
We are looking for a candidate with 5+ years of experience in a Data Engineer role, preferably one who has also attained Graduate level degree in Computer Science, Statistics, Informatics, Data Science, Information Systems or another equivalent field
They Should Also Have Experience Using The Following Software/tools
Experience with big data tools: Hadoop, Spark, Kafka, etc
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc
Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Glue, etc
Experience with at least one object-oriented/object function scripting languages: Java or Python
Some experience with Data Science and Machine Learning workflows is a big plus
Responsibilities
Assemble large, complex data sets that meet functional / non-functional business requirements
Build analytics tools that utilize the data pipeline to provide actionable insights into operational efficiency and other key business performance metrics
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using internal data reservoirs, AWS 'big data' technologies, automating them via Airflow---and containerizing these solutions for scalability
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Assist in creating data tools for analytics and data scientist team members that assist them in building and optimizing our internal products to improve our ability to deliver new ones
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc .
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
246,Data Engineer - Amazon Distribution and Grocery Innovation,Amazon,United States,"Job Summary

DESCRIPTION

The ideal candidate relishes working with extremely large volumes of data, enjoys the challenge of highly complex technical contexts, and above all, is passionate about data and analytics. They should be an expert with Data warehouse architecture, data modeling, data processing using distributed compute solutions and passionately partners with the data scientists, business analysts and other business stakeholders to identify strategic opportunities where improvements in data infrastructure creates out-sized business impact. You should be comfortable in design and development of solutions using Big Data technologies like Hadoop, Spark on or off AWS. This position requires hands-on software development expertise used to ingest and process large scale datasets. The candidate is proficient in at least one programming language. They should be a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail), excellent communication skills and enjoys working in a fast-paced and global team.


Basic Qualifications
Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field
4+ Years industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer)
4+ Years Experience with at least one database technology such as Redshift, Oracle, MySQL or MS SQL
4+ Years of experience developing and operating large-scale data structures for analytics
Demonstrated strength in SQL, Query performance tuning, Data modeling, ETL development, and Data warehousing.
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)
Preferred Qualifications
Master’s Degree in Computer Science, Engineering, Math, Statistics or related discipline
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
Experience administering Tableau, Looker, Quicksight or other BI tools
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2031191"
247,Index Data Algorithm Engineer,"eVestment, A Part of Nasdaq","Philadelphia, PA","We are seeking an experienced financial algorithm / data engineer to join our effort in developing the next generation of index construction capabilities.

The vision for the Index Data Platforms and Models (DP&M) team is to improve client experiences by using data as a means of empowering the Nasdaq Index Business with data-driven insights and solutions. Within this organization, you will be focused on building modular functions for data transformation and index construction algorithms (e.g., security identification, factor screening, and weighting techniques) that support construction and rebalance of Nasdaq’s index suite.

In this role, you will be responsible for working with Research and Development and Index Portfolio Management while collaborating with our extended Data Platforms and Models team to deliver tangible value in the form of coded algorithms that translate a variety of input data into index portfolios by applying data transformation and portfolio construction methodologies, techniques and strategies.

The ideal person possesses a portfolio construction and data analytics acumen, a strong understanding of data technologies, and expertise in software engineering best practices.
Build, automate, maintain, and monitor index construction and maintenance algorithms using standardized methodologies, with reliability and scalability in mind
Improve the performance and reliability of our index research, historical simulation, portfolio construction, and rebalance and reconstitution processes and algorithms
Work in collaboration with Research and Development to translate their needs into scalable, standardized solutions
Develop standards and processes for onboarding new data transformations into research and production
Collaborate with other specialists, product managers, and engineers to identify and solve critical business problems, crafting a positive experience for internal and external clients
Bachelor’s degree in Computer Science, Mathematics, Statistics or related area
5+ years of experience in Software Engineering, Data Engineering, or Trading Strategy Modeling
5+ years working with relational databases and query languages, including building data pipelines and the ability to work across structured, semi-structured and unstructured data
5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
Experience working with cloud platforms (AWS preferred), development and operations technologies (e.g., Kubernetes, Gitlab), and data management utilities (Databricks, Apache Parquet, etc.)
Strong quantitative reasoning skills and an interest in working at the intersection of research and software engineering
Experience preparing data for analytics and following a data science workflow to drive business results
Practical experience with Agile principles, preferably with Scrum, Kanban, or SAFe frameworks
Promotion of a strong control environment, adherence to risk controls and procedures, and process discipline, while appropriately considering business priorities
Come as You Are

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation."
248,Data Engineer - AWS/Python/Glue,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Connecttel, Inc, is seeking the following. Apply via Dice today!

Data Engineer - AWSPythonGlue Fully Remote bull Tampa, FL bull IT Infrastructure Job Description Our client is looking for a talented data engineer to join our team. Data analytics stack is composed of PySpark, AWS Glue, AWS Redshift, AWS Quicksight, Terraform and a variety of other AWS services including ECS, S3, SQS. You will create data products that provide deep insights into data generated applications, deploy PySpark Glue pipelines processing models to enrich and categorize our data, and collaborate with members of our engineering and product teams to integrate your creations into our services architecture to be consumed across our ecosystem of products. ESSENTIAL FUNCTIONS AND RESPONSIBILITIES Broad comfort with data, from retrieval through analysis and presentation, from all kinds of sources Ability to operate as a part of an engineering organization, adhering to set source control, CICD, and security practices. Understanding of and willingness to learn the domains supported applications (Safety, EHS, LMS) Ability to take predictive models from RD to production, while building for accuracy, speed, consistency, and maintainability Definition and implementation of system, customer, SAAS metrics to better understand our platform. SUPERVISORY RESPONSIBILITIES This position does not have supervisory responsibilities. Requirements Required Expertise in building Data pipeline, data modeling and familiarity with data architecture Expertise that would involve mapping from existing platforms to canonical models and piping the models to Data warehouse Expertise in BI for reporting and dashboards MUST have AWS pipelinewarehouse experience MUST have experience in Pyspark programming of Glue Jobs MUST have experience in AWS Redshift Should be aware of data processing architecture like Lambda Amazon S3 SQL expertise with PostgreSQL, MySQL, or equivalent. Nice to Have skills AWS QuicksightSpice, AWS Lambda serverless2-3 yearsrsquo experience with Python, Scikit-Learn, and PostgreSQL"
249,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, HRK Solutions LLC., is seeking the following. Apply via Dice today!

Location Plano, TX (open to remote but once they are required on site, they will need to be in Plano) Quantity 1 Duration 23 months Job Description Data Engineering skill set with good experience in Python, Pyspark or Scala. Should have strong SQL skills with hands-on experience on AWS. Experience working on building data pipelines and data warehouses with preferred knowledge of Snowflake. Looking for someone with 10+ years of experience. IV 30-45 mins technical questions and experience review. Hands on with SQL (1 round) Intake Call -LOB Financial Services Dealer - Team oversees provisioning systems that stream data to them multiple data layers take them from Snowflake to data warehouse. -Data Engineer looking for 10+ YOE and able to build data pipelines with Python and PySpark. -Need very good hands on experience with Python, SQL (810), Cloud (AWS - General knowledge S3, EMR), Scala will work if not strong in Python (Python is preferred), Snowflake is nice to have, Tableau is nice to have -Need to be open to learning on the job, Data ecosystem has many frameworks and process so they need to be able to adapt and learn quickly. Ramp up period is one month. -Need to have some independence (once they are ramped up) -This is a backfill for someone who wasn't able to perform to expectations and did not have strong SQL skills -Has other rolesneeds as well and can use some of the candidates they like from this req to put against those. One is a DA role with same skillsets with SQL coding, Python, Analysis, and troubleshooting Thanks Regards, Mounica.K mailto"
250,AWS Data Engineer,Deloitte,"Jersey City, NJ","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines.
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business.
Establish solid design and best engineering practice for engineers as well as non-technical people.
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
5+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
251,Data Engineer II,Amazon,"Bellevue, WA","Description

Role Description

How often have you had an opportunity to build a big business, solving significant customer problems through innovative technology, from the beginning? The Amazon Halo team, within Amazon’s Device organization (Amazon Echo, Fire TV, Fire Tablets, and more), is looking for passionate, hard-working, and talented individuals to join our fast paced, start-up environment to help invent the future. We solve significant customer problems in customer health and wellness through devices, mobile applications, cloud services (AWS), computer vision, and machine learning.

As a Data Engineer, you will be a member of an algorithm team that is responsible for the delivery of ML based algorithms. You will build end-end-end data pipelines, design data collection experiments and perform data analysis.

You are an individual with outstanding analytical abilities and comfortable working with cross-functional teams and systems. You must be a self-starter and be able to learn on the go. Excellent written and verbal communication skills are required as you will work very closely with diverse teams.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us//


Basic Qualifications
1-2 Years of Experience in Python
2 Years of Experience in SQL
Visualization experience in Redash, Quicksight or at least one other tool
Experience building ETL pipelines comprising of Python and AWS services Like S3, Athena Lambda and Redshift or similar technologies.
Experience building metrics deck and dashboards for KPIs including the underlying data models
Preferred Qualifications
Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations

Company - Amazon.com Services LLC

Job ID: A1453697"
252,Data Engineer,RBC,"Raleigh, NC","Come Work with Us!

At RBC, our culture is deeply supportive and rich in opportunity and reward. You will help our clients thrive and our communities prosper, empowered by a spirit of shared purpose.

Whether you’re helping clients find new opportunities, developing new technology, or providing expert advice to internal partners, you will be doing work that matters in the world, in an environment built on teamwork, service, responsibility, diversity, and integrity.

Job Title

Data Engineer

Job Description
Creating software for retrieving, parsing and processing structured and unstructured data.
Defining and building scalable ETL/ELT workflows for reporting and analytics.
Supporting project team to scale, monitor and operate data platforms for very high availability and performance.
Developing scripts and programs for converting various types of data into usable formats.

Job Summary

Supports the development and maintenance of scalable data stores that supply big data in forms needed for business analysis. Applies complete knowledge, skills, and practices to perform assignments.

Address:

Raleigh, North Carolina, United States of America

City:

USA-NC-RALEIGH

Country:

United States of America

Work hours/week:

40

Employment Type:

Full time

Platform:

Technology and Operations

Job Type:

Staff Augmentation

Pay Type:

Salaried

Posted Date:

2022-04-08-07:00

Application Deadline:

2022-04-29-07:00

Inclusion and Equal Opportunity Employment

At RBC, we embrace diversity and inclusion for innovation and growth. We are committed to building inclusive teams and an equitable workplace for our employees to bring their true selves to work. We are taking actions to tackle issues of inequity and systemic bias to support our diverse talent, clients and communities.

We also strive to provide an accessible candidate experience for our prospective employees with different abilities. Please let us know if you need any accommodations during the recruitment process.

Join our Talent Community

Stay in-the-know about great career opportunities at RBC. Sign up and get customized info on our latest jobs, career tips and Recruitment events that matter to you.

Expand your limits and create a new future together at RBC. Find out how we use our passion and drive to enhance the well-being of our clients and communities at rbc.com/careers."
253,Data Engineer,TEKsystems,"Seattle, WA","Description

Experience working with Spark and other distributed data technologies for building efficient & large-scale data pipelines.

Programming efficiency and hands on experience in Scala/Java or Python. Software engineering rigor and ability to write elegant, modularized and well tested code.

Experience required in building data processing pipelines curating data for variety of stakeholders

Experience in schema design and data modeling, SQL skills to analyze and explore data, identify patterns and draw insights.

Strong communication and collaboration skills. Ability to work in a cross functional environment across multiple stakeholders and convert abstract requirements into concrete deliverables.

This is an opportunity to join one of world's most prestigious company with fastest growing groups, AI/ML (Artificial Intelligence and Machine Learning). This team supports the a product that is utilized by millions of users around the world. This program helps create the tools and technology to make this product the best voice assistant in the industry, and allows the company to continue to innovate how people can utilize this product on their devices. This role requires an on-site component in Seattle due to the data security and secrecy of the devices and software being supported.

Skills

Python, Sql, Data, Spark, Java

Top Skills Details

Python,Sql,Data,Spark,Java

Additional Skills & Qualifications

Due to the nature of the work, 100% on-site is required in Seattle

Experience Level

Entry Level, 2-5years of experience

About TEKsystems

We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.

The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law."
254,Data Engineer,Schneider Electric,"Boston, MA","Schneider Electric has an opportunity for a Data Engineer in our Franklin, TN location.

The Data Engineer will develop, maintain and improve data delivery through the North America (NAM) Data Excellence platform. The right person in this role will support overall improvements in data architecture, metric definition and processes to support business data and analytical initiatives. Our Data Engineers provide guidance and support to team members in the Data Services team and our business partners.

Schneider Electric creates connected technologies that reshape industries, transform cities and enrich lives. Our 135,000+ employees thrive in more than 100 countries. From the simplest of switches to complex operational systems, our technology, software and services improve the way our customers manage and automate their operations. Help us deliver solutions that ensure Life Is On everywhere, for everyone and at every moment.

http://www.youtube.com/watch?v=YtExntUe89c

Great People Make Schneider Electric a Great Company.

Key responsibilities include:
Collaborate with team members to conceptualize, design and deliver enterprise and departmental data solutions to support business intelligence, data warehousing and reporting and machine learning requirements.
Implement solutions that are reliable and scalable to meet the service levels associated with mission-critical solutions.
Participate in and enhance our DevOp practice to insure highly available solutions and quick issue resolution.
Translate business requirements into data pipelines and data stores to support of business requirements.
Perform assessments (Proof of Concepts) of the latest tools and technologies.
Work with Data and Solution Architects to define and implement migration strategies from legacy systems to cloud architecture and technologies.
Provide team feedback to optimize delivery of our solutions.

Qualifications

We know skills and competencies show up in many different ways and can be based on your life experience. If you do not necessarily meet all the requirements that are listed, we still encourage you to apply for the position.

Required qualifications:
5+ years of demonstrated experience with modern programming languages such as Python, Scala or Java.
3+ years of experience developing data-related solutions on cloud platforms such as: Amazon Web Services (AWS).
Demonstrated experience with primary AWS services such as EC2, Lambda, EMR, S3, IAM policies, Cloudwatch, Cloud Formation, SES
Demonstrated experience with Cloud Services for data handling and database technologies (DMS, Kafka, Spark, Redshift, Athena, Hadoop, Airflow, etc.).
Knowledge of Data Management, Integration and Data Quality tools, such as: Alteryx, Trifacta, Informatica Power Center, Informatica Cloud, and Oracle Data Integrator.
Command of advanced SQL queries and programming.
Experience contributing to and following architecture, design and implementation best practices.
Have an eye for operational transparency and resiliency at every layer of the application.
Proven analytical and problem-solving abilities. Ability to assimilate information and quickly discern the most relevant facts and recommend creative, practical design solution. Ability to think outside-the-box a real asset.
Experience with DEVOPs tools and processes and CI/CD are an asset.
Excellent communication, presentation, influencing, and reasoning capabilities.
Desire to take the initiative moving projects/ideas forward with clarity
Leadership skills to lead and mentor cross-functional teams towards common solutions.
Knowledge of legacy data warehousing tools and technology is an asset Examples: Dimensional Models, Informatica PowerCenter, Informatica Cloud, MS Integration Services, Alteryx, Oracle, MS SQL Server etc.
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more. Click here to find out more about working with us: http://se.com/us/careers.



We seek out and reward people for putting the customer first, being disruptive to the status quo, embracing different perspectives, continuously learning, and acting like owners. We’re recognized around the world for welcoming people as they are. We create an inclusive culture where all forms of diversity are seen as a real value for the company.  See what our people have to say about working for Schneider Electric.

https://youtu.be/C7sogZ_oQYg



Let us learn about you! Apply today.



You must submit an online application to be considered for any position with us. This position will be posted until filled.

As a federal government contractor, all Schneider Electric U.S. employees (including U.S. territories and Puerto Rico) must be fully vaccinated against COVID-19, subject to federal laws.



It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct.



Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.



Schneider Electric is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Schedule: Full-time

Req: 007KT4"
255,Data Engineer,Air Products,"Allentown, PA","Job Description And Qualifications

Air Products is a thriving Fortune 500 global company that is growing and looking for talented, driven Digital Technology professionals to join our team! With over 20,000 employees and operations in over 50 countries, Air Products is committed to its Higher Purpose of bringing people together to collaborate and innovate solutions to the world’s most significant energy and environmental sustainability challenges.

We are excited to share our Digital Technology team is expanding to meet growing business needs across the world. We’re making a significant investment in our people and systems to strengthen our digital foundation, drive business optimization and enhance our customer experience. If you are passionate about achieving these goals as well, we would like you to consider joining our team!

Our DT team has an immediate opening for a Data Engineer to help us build and maintain our Amazon Web Services (AWS) Data Lake. This position can be located at our global headquarters in Allentown, PA or can be available to those working remotely in the U.S. In the case of remote work, physical presence in the office may be required on occasion to engage in face-to-face collaboration among teammates.

What You’ll Do
Operationalize data pipelines that support analytics initiatives.
Design, build, manage, optimize, and document data flows from various sources into our enterprise Data Lake. Data ingestion timing can be near real-time or streaming. Delivery of high-quality data is a key item of focus.
Collaborate with data scientists, data analysts and other data consumers to review data models and algorithms to improve the overall efficiency of sophisticated analysis projects.
Ensure data quality, governance, and data security procedures are met while curating data for use in the Data Lake.
Optimize flexibility, scalability, performance, reliability, and future-proof capacity of DT services with a focus on cost.
Implement solutions, including infrastructure, scripts, database resources, permissions, and source control.
Design, implement, and analyze robust test plans and stress tests.
Contribute to improving the team's own internal processes of communications, documentation, workload planning.
What we are looking for:
Proficiency with Qlik (Attunity) Replicate & Compose, AWS Glue Scripting, Athena, Redshift, EMR, S3, PySpark, Python ETL, Java, Hive, and Scala.
Hands-on experience working with big data technologies (Hadoop, Hive, Spark, Kafka).
4-year BS degree in Information Technology field or related technical field.
Experience with processing large data sets in a time-sensitive environment while minimizing errors.
Experienced in maintaining infrastructure as code using Terraform or cloud formation.
Excels in SQL and NoSQL technologies such as MongoDB / DocumentDB.
Solid grasp of data warehouse design patterns and standard methodologies.
Ability to develop test plans and stress test platforms.
Experience with complex job scheduling.
Strength in process development, consistency, and improvement.
Able to prioritize and coordinate work through interpretation of high-level goals and strategy.
Effective team player with a positive attitude.
Strong oral and written English language communications skills.
At Air Products, we work in an environment where diversity is essential, inclusion is our culture, and each person knows they belong and matter.

To learn more, visit About Air Products .

We offer a comprehensive benefits package including paid holidays/vacation, affordable medical, dental, life insurance and retirement plans.

Air Products thanks all applicants in advance for their interest; however, only those applicants who are being considered for an interview, or are currently employed by Air Products, will be contacted.

We are an Equal Opportunity Employer (U.S.). You will receive consideration for employment without regard to race, color, religion, national origin, age, citizenship, gender, marital status, pregnancy, sexual orientation, gender identity and expression, disability, or veteran status.

Req No.

36991BR

Employment Status

Full Time

Organization

Corporate

Business Sector / Division

Information Technology

Region

North America

Country

United States"
256,Data Engineer (Remote),Signify Health,"Trenton, NJ","What We Do

At Signify Health, we provide care for those who need it most. We understand the benefits of a personalized approach to clinical encounters and the importance of conversation, understanding, and sitting knee-to-knee with the patient in the best setting for care delivery: the home.

A Software Engineer focusing on Data Engineering develops systems to manage data flow throughout Signify Health’s infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data.

What will you do?
Develop readable, well-tested applications, APIs, and libraries
Work with internal and external APIs for applications and data sources
Utilize cloud infrastructure and collaborate with SREs to build scalable systems
Implement application observability in the form of metrics, logging, and monitoring
Work with engineers across the organization
Collaborate closely with team members and product stakeholders
Requirements
4+ years of relevant software engineering work experience
Prior work with cloud-based systems
Expansive knowledge of RESTful API design and use
Familiarity with observability concepts and tooling
Meaningful experience with relational databases and SQL
Nice-to-have
Experience with Go or Python
Experience with Amazon AWS services
Experience with non-relational databases
Bachelor's degree in Computer Science or a related field
About Us

Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals’ clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we’re able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved."
257,Data Engineer - Data Management Product Line.,Johnson & Johnson,"Titusville, NJ","Janssen Pharmaceuticals, Inc., a member of Johnson & Johnson's Family of Companies is currently recruiting for a Data Engineer - Data Management Product Line for the Pharma Business Unit Data Engineering & Enablement Product Line. The primary location is in Titusville NJ and may require up to 10% domestic travel. At the Janssen Pharmaceutical Companies of Johnson & Johnson, what matters most is helping people live full and healthy lives. We focus on treating, curing, and preventing some of the most devastating and complex diseases of our time. And we pursue the most promising science, wherever it might be found. Thriving on a diverse company culture, celebrating the uniqueness of our employees, and committed to inclusion. Proud to be an equal opportunity employer

The DATA ENGINEER will play a key role building and deploying various data management solutions in realization of our Data Management strategy to transform the Pharma Commercial Regional Data Management capabilities. The DATA ENGINEER will work with associates to implement data cataloging, data warehousing and governance capabilities/procedures to empower the Janssen Scientific and Medical Affairs business users to quickly identify insights and drive business performance. The Data Engineer will collaborate with IT peers, Product Lines, Business Technology Leaders, Business Product Owners, Technology Services to apply Enterprise technology and development capabilities to deliver solutions that rapidly meet business needs. Key skills are needed to build data management capabilities in the realm of Big Data, Business Intelligence, Data Warehousing and Analytics, Metadata Management, Self Service.

Essential Duties And Responsibilities

▪ Work with Business Partners, IT Application Services teams and Global Technology Services to manage solution development by leveraging SDLC and Agile methodology to plan, design and deliver projects.

▪ Cognizant of Data Governance procedures and engage with cross functional teams to stay informed of the ways the organization uses its data and understand the potential impact to delivery of specific initiatives.

▪ Maintain solid business understanding of entire application landscape, and be the DRI for all business and technical aspects of the application

▪ Understand and articulate estimates for different levels of development effort to address the changing priorities and requirements in a fast-paced environment.
Oversee operations by coordinating and communicating support activities between Business and TS/IT partners ensuring Service Level targets (SLAs) are met.

▪ Ensure information security, data privacy and compliance requirements are met by participating in compliance reviews and other similar activities.

▪ Understand and uses J&J enterprise architecture guidelines and best practices to accelerate our ability to meet current business needs, to scale and to adapt to dynamic business needs.

▪ Take on data architecture ownership to develop data models, metadata solutions which meet the needs of the organization's information systems, manages data dictionary, data catalog, data quality and the flow of information using variety of data platforms.

▪ Help formulate the next generation analytics environment, providing self-service, centralized platform for all data-centric activities which allows a full 360 degree view of customers from product usage to back office transactions.

▪ Analyze internal and external data sources like Sales & Marketing, Medical Affairs, Contracting centric datasets to discover data and articulate the data scope and target models to application development teams and business.

▪ Perform impact analysis for new releases/changes, data investigations and post-implementation troubleshooting of new applications and application upgrades.

▪ Implement solutions leveraging AWS cloud stack components (S3, RDS, Redshift, EMR, Spark, Sqoop, Lambda, IAM, Glue, EKS), NoSQL, MySQL, Python, PySpark, Tableau and Qlik toolset or similar.

▪ Understand upcoming technologies in the data management and data science space and be able to evaluate them and improve technical practices.

▪ Understand the current technical delivery model for data platform development to enable ""economies of scope""

▪ Monitor and optimize performance and cost of AWS Cloud infrastructure by tracking the usage metrics and exploring newer and cost-effective approaches.

Technology landscape at Janssen
Cloud Platforms -AWS (EC2, EMR, S3, Redshift, Spectrum, Athena, RDS, Lambda etc.). Knowledge of GCP, Azure, Veeva CRM, Veeva Vault is a plus
Data Integration Tools - EMR/Hadoop, Spark, Sqoop, Dataiku, APIs
Programming Languages - Python, PySpark
Database - Oracle, mySQL, Redshift
Automation Tools - Control-M, Jenkins(CI/CD), Bitbucket

Qualifications
Bachelors’ Degree in Computer Science, Math, Statistics, Information Systems, Information technology or similar.
Knowledge of pharma scientific and medical affairs business domain (CRM Interactions, Contact Center operations, Non-promotional and Scientific Knowledge Document review processes, EMR/EHR etc.
Demonstrated ability working with business partners and technology resources to translate business needs into data and analytics solutions using Agile methodology.
Experience in data profiling, data modeling, data ingestion and designing data pipelines to meet various analytical needs.
Hands-on experience in data analysis and connecting disparate data sources preferably CRM, RWE and Google Analytics data.
Demonstrated expertise in formulating business rules, design data quality framework and data cataloging.
Experience in distributed programming with Python, Sqoop, Spark(PySpark), NoSQL databases, Unix Scripting, MPP, RDBMS databases for data integration is preferred.
Hands on experience on with Amazon Web Services like S3, EC2, Redshift, RDS, EMR, Athena, Glacier, IAM is preferred.
Experience on Sales Force & Veeva object models is preferred.
Knowledge and hands-on experience with API security standards, design standards, patterns, and best practice implementation - Preferred

Do you strive to join an outstanding team that is dynamic and constantly evolving? Is career growth and opportunity appealing to you? Apply to this opportunity today.

Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability

At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional, and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.

For more information on how we support the whole health of our employees throughout their wellness, career and life journey, please visit www.careers.jnj.com.

Primary Location

United States-New Jersey-Titusville-1125 Trenton Harbourton Road

Organization

Johnson & Johnson Services Inc. (6090)

Job Function

Info Technology

Requisition ID

2206028962W"
258,Data Engineer,Cheetah,San Francisco Bay Area,"About Cheetah

Restaurants and small businesses account for hundreds of billions of dollars in annual purchasing, yet most of it happens offline. From the humblest NYC slice and a cup of joe to museum-quality lattes (and yes, avocado toast), each order represents a long paper trail and a series of truck deliveries.

With a rapidly growing market presence, Cheetah is already streamlining the daily workflow of hundreds of restaurants and small business owners. Using Cheetah’s services, they manage millions of dollars in monthly purchasing, while Cheetah eliminates the burden of costly sales reps and inflexible, inefficient distribution centers. Partnering with the largest suppliers in the US, Cheetah is bringing the Silicon Valley user experience to an industry that hasn’t evolved in decades. In the process, we’re helping to reintroduce the small business owner to the small farmer, global supply chains to familiar faces, and delivery and logistics workers to increased mobility and new opportunities.

At Cheetah, a relentless executive team has set out to change this reality, armed with deep industry and entrepreneurial experience, personal conviction, and $130M in funding from Eclipse, Floodgate, Manna Tree, Sator Grove, and the early backers of Facebook, Lyft and Airbnb.

Unencumbered by the old-school business models and practices, Cheetah has introduced unprecedented transparency in pricing and inventory to the SMB marketplace. In doing so, Cheetah allows the people who form the backbone of local economies to focus on what they love: cooking great food and providing excellent service. From the convenience of their phones, business owners can now order whatever they need, discover new, rare or seasonal specialty products, and prioritize their customers. No more rushing to unload your double-parked van, or waiting anxiously for a truck to show up with your weekly allotment of boxes.


Data Engineer
Cheetah is looking for a data engineer with experience developing efficient, robust and scalable data solutions to enable data-driven decision making. As the founding member of the data engineering team, you will be responsible for leading Cheetah’s next generation data platform effort, owning the data ETL processes, and facilitating production deployment of data science modeling solutions. You will establish the data lake and build the data warehouse by ingesting and transforming hundreds of thousands inventory movement data from WMS (warehouse management system) tens of thousands transaction data from ERP (enterprise resource planning) on a weekly basis, along with task data from delivery and fulfillment platform and session data from APP logs, encompassing thousands of catalog items.

Responsibilities
Enhance the existing batch processing pipeline to reduce latency, gain visibility into cluster performance, and monitor pipeline performance
Work with the data science team to design and build the next generation of data environments in a scalable way to handle the rapid growth of catalog and APP event logs
Collaborate with backend engineers for seamless data ingestion, and with front end and mobile engineers for event-driven data collection
Establish a model deployment process for data scientists into production
Provide technical leadership / mentorship as you grow the data engineering team
Guide data users to most efficiently use the data platforms and tools

Qualifications
5+ years of hands-on experience in data engineering
Solid understanding of data structures on distributed systems, ETL processes, data schema design, orchestrating pipelines via JDBC / PostgreSQL / EDI, microservices, and query optimization
Expert knowledge in AWS products (e.g., Lambda, Batch, EC2, S3, RedShift, EMR)
Strong competency in programming languages (e.g., Python, Ruby, Scala, Java), data processing / streaming tools (e.g., Airflow, Hive, Spark, RabbitMQ, MongoDB, Hadoop, Zookeeper)
Knowledge of ERP (e.g., NetSuite) and WMS (e.g., JDA) software solutions
Knowledge of container applications (e.g., Docker) and container orchestration systems (e.g., Kubernetes)

Benefits and Perks
Unlimited paid time off
Competitive salary and stock options package
Comprehensive medical/dental/vision insurance
100% Employer-paid Life, AD&D, Employee Assistance Program, and Long-Term Disability benefits
Additional benefit options including accident protection, commuter and parking benefits, flexible spending options for heath and dependent care, 401K plan and pet insurance
Flexible work scheduling and location
Company sponsored learning and development, wellness support, team building, events and engagement opportunities
The opportunity to work alongside a visionary team during Cheetah’s rapid growth!"
259,"Data Engineer I, Data and Analytics Tech Applications",Amazon,"Austin, TX","Job Summary

DESCRIPTION

Team Summary

How often have you had an opportunity to be a member of a team that is tasked with solving customer needs through disruptive and innovative technology? Everyone on the team needs to be entrepreneurial and work in a fast-paced, ambiguous, and highly collaborative environment that is more startup than big company. If this sounds intriguing, then we would like to talk to you about a role on the Data and Analytics Tech Applications Tech Team. This team produces enterprise-wide data and analytics solutions touching all facets of the company. To continue evolving our capabilities, we seek a passionate, results-oriented Data Engineer (DE).

Role Summary

The DE will partner with other Data Engineers along with Software Developers, Business Analysts, Product Manager, and Program Managers inside and outside of the team to distill large, complicated datasets into automated, digestible reporting deliverables. The ideal candidate has strong business judgment, organization skills, backbone, and collaborates well with product managers. The operating environment is fast-paced and dynamic; however, we have a strong sense of team and a welcoming culture. To thrive, you must be detail-oriented, enthusiastic, and flexible, in return you will gain tremendous experience with the latest in technologies as well as exposure to several functional areas.

Responsibilities
Identify, extract, assess, manipulate, and analyze internal and external data from multiple sources including transactional and reference data
Leverage SQL and scripting skills (e.g., python) to help achieve business outcomes
Build data pipelines that support new and existing reporting and analytic solutions
Collaborate with both the internal and external teams to design new algorithms, drive optimization, and continuously improve our analytical capabilities
Work within multiple technology stacks and push to leverage AWS capabilities
Will work directly with business partners in addition to Product Managers to scope, size, and execute projects
Qualifications
Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field
1+ Years of experience with SQL and ETL development.
1+ Years of relevant work experience in a role requiring application of analytic skills
1+ Experience developing analytic insights and leveraging analytic tools such as Excel (or similar).
Effective verbal/written communication skills
Strong analytical and problem-solving skills
Demonstrated ability to learn new tools and concepts
Self-starter and motivated with a strong attention to detail and commitment to consistently meeting timelines and operating from a sense of urgency.
Focused on continuous improvement of yourself, the team, and the systems we build
Nice To Haves
Advanced degree in Computer Science, Engineering, Mathematics, or a related field.
3+ Years of experience
Key job responsibilities
Identify, extract, assess, manipulate, and analyze internal and external data from multiple sources including transactional and reference data
Leverage SQL and scripting skills (e.g., python) to help achieve business outcomes
Build data pipelines that support new and existing reporting and analytic solutions
Collaborate with both the internal and external teams to design new algorithms, drive optimization, and continuously improve our analytical capabilities
Work within multiple technology stacks and push to leverage AWS capabilities
Will work directly with business partners in addition to Product Managers to scope, size, and execute projects
A day in the life

The DE will partner with other Data Engineers along with Software Developers, Business Analysts, Product Manager, and Program Managers inside and outside of the team to distill large, complicated datasets into automated, digestible reporting deliverables. The ideal candidate has strong business judgment, organization skills, backbone, and collaborates well with product managers. The operating environment is fast-paced and dynamic; however, we have a strong sense of team and a welcoming culture. To thrive, you must be detail-oriented, enthusiastic, and flexible, in return you will gain tremendous experience with the latest in technologies as well as exposure to several functional areas.

About The Team

How often have you had an opportunity to be a member of a team that is tasked with solving customer needs through disruptive and innovative technology? Everyone on the team needs to be entrepreneurial and work in a fast-paced, ambiguous, and highly collaborative environment that is more startup than big company. If this sounds intriguing, then we would like to talk to you about a role on the Data and Analytics Tech Applications Tech Team. This team produces enterprise-wide data and analytics solutions touching all facets of the company. To continue evolving our capabilities, we seek a passionate, results-oriented DE.


Basic Qualifications
Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field
1+ Years of experience with SQL and ETL development.
1+ Years of relevant work experience in a role requiring application of analytic skills
1+ Experience developing analytic insights and leveraging analytic tools such as Excel (or similar).
Effective verbal/written communication skills
Strong analytical and problem-solving skills
Demonstrated ability to learn new tools and concepts
Self-starter and motivated with a strong attention to detail and commitment to consistently meeting timelines and operating from a sense of urgency.
Focused on continuous improvement of yourself, the team, and the systems we build
Preferred Qualifications
Advanced degree in Computer Science, Engineering, Mathematics, or a related field.
3+ Years of experience
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1757210"
260,Senior Data Engineer,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, OrangePeople, is seeking the following. Apply via Dice today!

Do you have a passion for innovation? Are you excited to leverage cutting edge technology to solve big business problems? If your response to those questions is ""yes"", we would love for you to join us! At OrangePeople we consult for some of the most prestigious brands in the world. But more importantly, our consultants have a voice in the vision and future of the company. At OrangePeople, our focus is people. It's right there in our name. Basic Qualification A minimum of 5 years of relevant work experience in data engineering, cloud-based infrastructure and programming. Domain expert in data engineering. Design and implement ETLELT data pipelines with Snowflake data warehouse, AWS S3 data lake, and RDBM systems. Understand data governance and monitoring techniques Practice DataOps and SysOps (e.g., automation, performance tuning, pipelines, ingestion, prep, orchestration, management and analytics) Domain expert in cloud-based architecture, SysOps tools and methods. AWS infrastructure and services (e.g., EC2, S3, IAM, CloudWatch, RDS, CLI). Design cloud-based services (e.g., IaaS, PaaS, SaaS). Familiar with Linux operating systems. Efficient practitioner in software engineering tools and principles. Open-source, Inner-source and cloud-native toolsets. Programming and scripting languages BashAWKSED, SQL, SnowSQL, Python, and other related system languages. Testing Automation Frameworks (e.g., Robot and PaBot Frameworks, Selenium, etc.). Understand DevOps principles (e.g., automation, CICD, containers, schedulers, version control, testing and deployment). Understand Agile SDLC using SCRUM methodology. Preferred Qualifications Has to know Cloud based infrastructure. Linux and AWS is a must as well. Need programming skills SQL Python(Must haves). Required Education BABS Degree Additional Responsibilities Participate in OrangePeople monthly team meetings, and participate in team-building efforts. Contribute to OrangePeople technical discussions, peer reviews, etc. Contribute content and collaborate via the OP-WikiKnowledge Base. Provide status reports to OP Account Management as requested. About us OrangePeople is an Enterprise Architecture and Project Management solutions company. Our most valuable asset is our people dynamic, creative thinkers, who are passionate about doing quality work. As a member of the OrangePeople team, you will have access to industry-leading consulting practices, strategies technologies, innovative training education. An ideal OrangePeople Person is a technology leader with a proven track record of technical achievements and strong processmethodology orientation."
261,Data Engineer,Ørsted,"Newark, NJ","Join us in this role where you’ll get the opportunity to architect and build end-to-end data engineering solutions that provide the foundation for a cloud-based analytics platform for our Onshore business unit, tapping into the enterprise solution. You’ll be a part of our agile delivery organization and will be working closely with product owners, subject matter experts, and business stakeholders. You’ll be advising the business on how to enable data in order to increase business value.

Welcome to Ørsted IT

You’ll be part of Ørsted IT where you, together with your colleagues, will be part of a newly established agile team that supports the Onshore business unit across the US and Europe. Together with our IT colleagues and the business, we’re committed to delivering on our company’s digital strategy and creating a strong platform for analytics, model management, etc.

You’ll play an important role in:
designing and building data pipelines for modern cloud-based analytics across the company
working with big data and complex and large datasets to be presented in a structured way to the business
ensuring that data formats and storage options are the best fit for end-user needs
monitoring and maintaining existing data pipelines
being part of PoCs in order to select and maintain relevant data engineering tools
linking the IT department with the Onshore business unit.
To succeed in the role, you:
have a passion for developing software to address data processing challenges, and have significant development experience and an interest in extensive use of Python
excel in taking responsibility for issues and clarifying them with peers
have experience working with SQL, Databricks, Spark/PySpark, and NiFi
have experience with data storage or engineering solutions in Azure
have experience with monitoring and orchestrating data pipelines in a DevOps context
have experience in Docker containers, such as Kubernetes.
Join a world leader in green energy

Ørsted is a global leader in offshore wind energy and ranked the world’s most sustainable energy company. To be a frontrunner, we invest significantly in employee development, and from the moment you join us, we’ll support your personal and professional growth. Here, you’ll get the opportunities that will unleash your full potential, and you’ll experience a collaborative, diverse, and dynamic work environment.

Shape the future with us

Send your application to us no later than 2022-05-16. We’ll be conducting interviews on a continuous basis and reserve the right to take down the advert when we have found the right candidate.

Please don’t hesitate to contact Talent Acquisition Operations US, Talent Acquisition Operations US, on talentacquisitionUS@orsted.com if you’d like to know more about the position.

If you need to request any adjustments to working practices, working patterns, or the assessment process we're happy to discuss alternative arrangements.

We offer a flexible workplace, and in this position you’ll be working in a hybrid (office and home-working) set-up.

Please note that for your application to be taken into consideration, you must submit your application via our online career pages and answer the screening questions relevant for your country.

Relocation is available."
262,Data Engineer,Dice,"New York, NY","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Perennial Resources International, is seeking the following. Apply via Dice today!

Our client is a top tier Alternative Asset Management firm seeking to add a Data Engineer to its Investment Technology Team Qualifications Demonstrated experience in large dataset design and modeling Demonstrated expertise in data engineering, including but not limited to data platforms, ETL ELT process engineering, data quality monitoring and machine assisted data discovery As a Data Engineer, you would need to be comfortable with the following tech stack C, .NET Core, Python, Entity Framework Core Database concepts SQL and no SQL (preferably in SQL Server environment) Tableau andor other data visualization tools AWS S3, RedShift, Lambda, Glue, SNS, SQS and parquet file format (or Azure equivalent) For Complete and Detailed Position Description please apply with Resume"
263,Data Engineer,Amazon,"New York, NY","Description

Are you passionate about Cloud technology? Are you motivated to drive innovation? Is the idea of enabling Amazon to continue to be the first and best user of AWS technology exciting to you? Then this role might be perfect for you. Discovery & Networking is building the largest data lake of service to service communication in Amazon. We are looking for a senior data engineer to play a critical role in taking our platform to the next level. You will be mining data from services across Amazon to enable the of network connectivity in the cloud. You will help us build the infrastructure to add machine learning capabilities to our platform to enable the build out of a truly autonomous network for Amazon services.

As an Amazon.com Data Engineer II you will be working in one of the world's largest cloud-based data lakes. You should be skilled in the architecture of data warehouse solutions for the Enterprise using multiple platforms (EMR, RDBMS, Columnar, Cloud). You should have extensive experience in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.


Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, or a related field and 4-5+ years industry experience
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)
Must have one year of experience in the following skill(s):
Developing and operating large-scale data structures for business intelligence analytics using: ETL/ELT processes; OLAP technologies; data modeling; SQL;
Experience with at least one relational database technology such as Redshift, Oracle, MySQL or MS SQL
Experience with at least one parallel processing data technology such as Redshift, Teradata, Netezza, Spark or Hadoop based big data solution
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
A desire to work in a collaborative, intellectually curious environment.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Query performance tuning skills using Unix profiling tools and SQL
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Experience with AWS
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1780495"
264,"PySpark Data Engineer, Consultant",Deloitte,"Stamford, CT","The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Qualifications

Required:
3+ years of relevant technology consulting or industry experience to include experience in Information delivery, Analytics and Business Intelligence based on data
3+ years experience in Python and/or R
3+ years experience in SQL
3+ years experience PySpark
2+ years of hands on experience with data core modernization and data ingestion.
1+ years experience leading workstreams or small teams
Bachelor's Degree or equivalent professional experience
Travel up 50% (While 50% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice
Limited immigration sponsorship may be available.
Preferred:
An advanced degree in the area of specialization is preferred.
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, and/or Google Cloud Platform (GCP)
Experience with Spark, Scala
Understanding of the benefits of data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design, data modeling, reporting process metadata, and ETL processes.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)."
265,Data Engineer,Dice,"Woodcliff Lake, NJ","Job Summary

Our Data Engineer will jump right into the action, working with data to solve business challenges, while builiding and maintaining the infrastructure to continuously improve processes and answer emerging questions. In addition to statistical expertise, our curious and creative Data Engineer will have great communication skills-and a passion for making data understandable and actionable all business units, propelling ambitious growth.

Experience, Skills, Knowledge Requirements

Bachelor's Degree preferred but not required. Extensive, progressive, work experience (6+ years of experience) Highly motivated to continuously learn and collaborate in a fast paced environment. Drive to build sustainable and reliable solutions. Knowledge of Python, Java, SQL, Git, AWS, Docker, Kubernetes, etc. Ability to effectively and concisely communicate with both business and technical audiences. Knowledge of ML pipeline frameworks like kubeflow, MLflow, etc. ML frameworks (sklearn, Tensorflow, PyTorch, MXNet or similar) ETL tools like Airflow, Celery, Luigi, etc.

AWS services like s3, Sagemaker, lambda, SQS, SNS, EMR, ECR, EKS, etc. MPP cloud data warehouses like Snowflake, Redshift, etc. An energetic, agile-minded, optimistic, pragmatic and collegial team-player.

Compensation And Benefits

We offer

A competitive starting salary based on experience, with achievement based opportunities for annual bonuses and increases. Opportunities for professional advancement. We value big thinking tied to practical, collaborative execution in a structured and growth oriented company.

Ongoing mentoring from senior staff and periodic opportunities to attend industry seminars and workshops. Starting on the first day of hire, all our employees can begin participating in our excellent Major Medical, Dental, Vision and Life Insurance plans. Paid holiday and vacation time, which starts in the first year of employment and increases with tenure. A modern, professional, suburban office space, concentrated work day (830-530) and business-attire environment. We enjoy a professional, collegial and positive work atmosphere, sharing camaraderie and rooting for individual and collective success.

location WOODCLIFF LAKE, New Jersey job type Permanent salary 150,000 - 180,000 per year work hours 8am to 5pm education Bachelors responsibilities

Job Functions, Essential Duties, And Responsibilities

Write production-grade code using standard software engineering methodologies. Implement data solutions using software engineering best practices. Take responsiblity for end to end ownership of data pipelines. Collaborate closely with business teams to design and implement solutions. Contribute to a data driven culture in which everyone's ideas are valued. Analyze large data sets and develop custom models to uncover trends, patterns and insights to advance business development. Provide critical thought leadership to enhance organizational capabilities by utilizing big and small data qualifications

Experience level Experienced Minimum 6 years of experience Education Bachelors skills

Data Analysis (6 years of experience is required) Docker (4 years of experience is required) AWS (6 years of experience is required) SQL (5 years of experience is required) Python (3 years of experience is required) MLflow (3 years of experience is required) ETL (5 years of experience is required) Lambda (3 years of experience is required) cloud (4 years of experience is required) Data Warehouse (6 years of experience is required) snowflake (2 years of experience is required) data engineer (7 years of experience is required) machine learning (5 years of experience is required)

Equal Opportunity Employer Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.

For certain assignments, Covid-19 vaccination andor testing may be required by Randstad's client or applicable federal mandate, subject to approved medical or religious accommodations. Carefully review the job posting for details on vaccinetesting requirements or ask your Randstad representative for more information."
266,Data Engineer II,Grubhub,"New York, NY","About The Opportunity

We’re all about connecting hungry diners with our network of over 300,000 restaurants nationwide. Innovative technology, user-friendly platforms and streamlined delivery capabilities set us apart and make us an industry leader in the world of online food ordering. When you join our team, you become part of a community that works together to innovate, solve problems, grow, work hard and have a ton of fun in the process!

Why Work For Us

Grubhub is a place where authentically fun culture meets innovation and teamwork. We believe in empowering people and opening doors for new opportunities. If you’re looking for a place that values strong relationships, embraces diverse ideas–all while having fun together–Grubhub is the place for you!

More About The Role

We are looking for data engineers to help us expand and improve our big data and machine learning pipelines powering Grubhub’s restaurant supply growth. You will work with smart, humble, and committed colleagues to create innovative, robust, automated solutions for Grubhub’s three-way marketplace. This is an unbeatable opportunity for data engineers who hope to work on and deliver world-class data products in a friendly and fun environment.

The Impact You Will Make
Work with high volumes of data and distributed systems using technologies such as Spark, Hive, AWS EMR, AWS S3, Azkaban, Presto, etc.
Work with data scientists and analysts to productionize data pipelines and machine learning models, so that they can scale and accommodate various business requirements.
Collaborate with technology, product and business stakeholders to deliver new features.
Actively contribute to the adoption of strong data engineering architecture, development practices, and new technologies.
Maintain up-to-date technical documentation while continuously delivering technical solutions and business impacts.

What You Bring To The Table
A bachelor's degree, preferably in a computer-related discipline.
5+ years experience with Python or another general-purpose programming language
Willingness to understand the larger business context
Ability to work and communicate within and across teams.
Excellent knowledge of SQL, data modeling, and patterns.
Background in ETL and data processing, familiar with how to transform data to meet business goals
Enthusiasm for a fast-paced, tech, and product-oriented environment, and the desire to work with a great team!

Got These? Even Better
5+ years of experience developing large data processing pipelines with Apache Spark.
Excellent communication skills, including the ability to crystallize and broadly socialize insights
Rigorous attention to detail and accuracy
Exposure to Amazon AWS or another cloud provider
Growth mindset.

And Of Course, Perks!
Flexible PTO/PTO. Grubhub employees enjoy a generous amount of time to recharge.
Health and Wellness. Excellent medical benefits, employee network groups and paid parental leave are just a few of our programs to support your overall well-being.
Competitive Pay. You’ll receive a competitive base salary with eligibility for generous incentives, bonuses, commission or RSUs (role-specific).
Learning and Career Growth. Your personal and professional development is a priority at Grubhub. We empower you to be a leader and grow your career through training, coaching and mentorship opportunities.
MealPerks. Get meals on us! Our employees get a weekly Grubhub credit to enjoy and support local restaurants.
Fun. Every Grubhub office has an employee-led Culture Crew that connects people through fun, meaningful events and initiatives like Wellness Wednesdays, Slack competitions and virtual happy hours!
Social Impact. At Grubhub we believe in giving back through programs like the Grubhub Community Relief Fund and donating $1 million to the Equal Justice Initiative in 2020. Employees are also given paid time off each year to support the causes that are important to them.

Vaccination Requirement: Grubhub employees are required to be fully vaccinated. Candidates must confirm vaccination status at time of hire, and must provide proof of full-Covid-19 vaccination within 2 weeks of starting employment. Fully vaccinated is defined as: “2 weeks have passed since your second dose in a 2-dose series, such as the Pfizer or Moderna vaccines, or 2 weeks after a single-dose vaccine, such as Johnson & Johnson’s vaccine.

Grubhub is an equal opportunity employer. We welcome diversity and encourage a workplace that is just as diverse as the customers we serve. We evaluate qualified applicants without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. If you’re applying for a job in the U.S. and need a reasonable accommodation for any part of the employment process, please send an email to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address."
267,"Data Engineer II, Amazon DSP Product Analytics",Amazon,"New York, NY","Job Summary

DESCRIPTION

Amazon Advertising seeks the world’s brightest and best leaders to drive next-generation solutions that enable advertisers to reach customers across our owned and operated (O&O) sites, on other high-quality publisher sites across the web, and on millions of desktops, mobile, and connected TV devices. Our mission is to offer the world’s most customer-centric advertising.

Amazon DSP (ADSP) is our programmatic advertising product for campaigns spanning Display, Video, and Audio across Amazon properties (e.g., Amazon.com, IMDb TV, Twitch, Fire TV, and Amazon Music) and tens of thousands of third-party (3P) websites and apps. We seek candidates who want to join a team focused on building large-scale systems that integrate across multiple sources to provide unique advertising products. These products give advertisers the controls and data necessary to improve the relevancy and performance of their ads.

As a Data Engineer, you will work on our BI team and partner closely with other Data Engineers, Data Scientists, Technical Program Managers, and Software teams to create data pipelines that are used to optimize data collection and processing. You will be responsible for synthesizing data across our organization in a data warehouse environment.

You have a passion for Excellence. You have a proven track record of building the right Data Engineering solutions that scale, provide high availability, and require minimal maintenance. You write code that is clean, readable, and straight forward.

You do whatever it takes to add value. You understand the importance of speed in business and have the technical ability to perform in a fast passed environment. You care passionately about your stakeholders and autonomously provide them the data they need.

You have good business and communication skills to be able to work with product owners to understand key business questions, and to build data sets that answer those questions.

You are comfortable juggling competing priorities and handling ambiguity. You thrive in an agile and fast-paced environment on highly visible projects and initiatives.

Key job responsibilities

Develop software for optimizing data annotation, data processing, and data metrics generation.

Work closely with engineers, scientists, and linguists specialized in AI data collection to improve and automate the end-to-end data collection workflow holistically.

Be the pioneer for creating a data pipeline, warehouse and dashboard that will scale and allow huge volume of data to flow fluently between internal and external teams.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
Industry experience as a Data Engineer, Software Engineer, or related specialty.
Experience building data products incrementally and integrating and managing datasets from multiple sources.
Experience in SQL.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1909296"
268,AWS Data Engineer,Deloitte,"Dallas, TX","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines.
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business.
Establish solid design and best engineering practice for engineers as well as non-technical people.
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
5+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
269,Data Science Engineer,Robert Half,"Waltham, MA","Description

Robert Half is currently working with a leading Software and Analytics company providing data-driven insights and technology to the foodservice, hospitality, healthcare and education industries. Right now they are actively looking for a Data Science Developer to further develop one of their machine learning algorithms. The algorithm is focused on matching similar items from a data pool of over 1 million items. You would be working both independently and across cross functional teams to work on their complex systems and algorithms.

This position would allow you to work FULLY-REMOTE and is paying up to 150K. If interested please apply!

No C2C or Sponsorship at this time

Requirements

Clojure, Big Data, MySQL

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
270,Data Engineer,Amazon,"New York, NY","Description

Are you interested in being a part of Amazon’s fastest growing business? Are you excited to work on cutting edge Big Data technologies to help shape the future of Grocery data? If the answer to both these questions is Yes, then come be a part of Amazon Fresh’s Central Data Infrastructure team (DASH). Our vision is to build a world class, centralized and secure data infrastructure for Amazon’s Worldwide Omnichannel Grocery Business. Our mission is to make it extremely convenient for Internal stakeholders, 3P partners and the Grocery engineering community to access timely and accurate data for all their reporting and analytics needs. The scale and the complexity of the data we manage requires constant innovation and pushing the boundaries on what’s possible. This role offers you an opportunity to work on VP level visibility initiatives and make a lasting impression on how grocery data is managed and distributed across the globe.

In This Role You Will
Help build the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data storage principles, and recent advances in distributed systems
Manage AWS resources.
Collaborate with Business Intelligence Engineers (BIEs) to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning
Drive the architecture and technology choices that enable a world-class user experience
Develop expertise in a broad range of Amazon’s data resources and know when, how, and which to use and which not to use
Encourage the organization to adopt next-generation data architecture strategies, proposing both data flows and storage solutions
Be comfortable with a degree of ambiguity and willing to develop quick proof of concepts, iterate and improve
Create extensible designs and easy to maintain solutions with the long term vision in mind
Have an understanding and empathy for business objectives, and continually align your work with those objectives and seek to deliver business value. You listen effectively.
Be comfortable with presenting your findings to large groups
We have a very flat team structure, and offer a unique opportunity for technical leaders who want to work closely with the business in defining, designing, building, and operating products that are rapidly growing.


Basic Qualifications
Bachelor's Degree in Computer Science, Math, Physics, or related discipline with 5+ years of relevant experience; or Master's Degree in the similar field with 2+ years of relevant experience.
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Demonstrated experience in relational database concepts with an expert knowledge of SQL
Demonstrated ability in data modeling, ETL development, and Data warehousing
Preferred Qualifications
Experience working with AWS Big Data Technologies
Experience working with Open Source Big Data tools
Proven track record of delivering a big data solution
Experience developing tools for data engineers and machine learning
Experience working with both Batch and Real Time data processing systems
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1607071"
271,Data Engineer - Real Time Data Streaming,Morgan Stanley,"Alpharetta, GA","Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management, and wealth management services. The Firm's employees serve clients worldwide including corporations, governments and individuals from more than 1,200 offices. As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence and strong team ethic. Morgan Stanley can provide a superior foundation for building a professional career - a place for people to learn, to achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.

Technology works as a strategic partner with Morgan Stanley business units and the world's leading technology companies to redefine how we do business in ever more global, complex, and dynamic financial markets. Morgan Stanley's sizeable investment in technology results in quantitative trading systems, cutting-edge modeling and simulation software, comprehensive risk and security systems, and robust client-relationship capabilities, plus the worldwide infrastructure that forms the backbone of these systems and tools. Our insights, our applications and infrastructure give a competitive edge to clients' businesses—and to our own.

We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data pipeline architecture for cross functional teams using on-prem and cloud (AWS/Azure) services. Must be self-directed and comfortable supporting the data needs of multiple teams and should be able to work under general guidelines in a fast-paced work environment

Responsibilities
Create and maintain optimal data pipeline architecture.
Build optimal ETL process from wide variety of data sources using SQL and AWS ?big data? technologies like AWS Glue, Lambda, Pyspark, Spark SQL, and/or Scala.
Develop, support, and document data pipeline for message queuing and real-time stream processing using existing on-prem frameworks like Logstash, Fluentd, Elasticsearch, Kafka, StreamSets, Python
Be able to automate application deployment and testing, including framework development and build visualization dashboards for various business monitoring and analytic needs.
Ensure performance, availability and scalability of data solutions including Kafka, AWS Kinesis, Elastic Search (ELK).
Experience performing root cause analysis to answer specific business questions and identify opportunities for improvement
Assist with data-related technical issues and support business logic mappings, and proactively test ETL data processing for on-prem and cloud jobs
Familiar with standard version control tools and be able to perform code reviews with the Data Integration
Required Skills
5 years of Information Technology experience
Python/Scala/Ruby
Cloud Technologies (AWS)
Data Lake Projects (AWS)
Relational Databases and SQL
Elasticsearch, Kafka, and/or Hadoop
SVN, Git
Excellent Communication skills
Excellent Analytical Ability
Skills Desired
Cloud Technologies (Azure)
Scrum/Agile Methodology
Big Data & Hadoop Related Technologies
Data Stage (IBM Information Server 11.x)
Data Pipeline Support
Tech/Product Tools/Open Source and Vendor products experience
Soft Skills
Financials, Marketing
Posting Date

Dec 14, 2021

Primary Location

Americas-United States of America-Georgia-Alpharetta

Other Locations

Americas-United States of America-California-Menlo Park

Education Level

Bachelor's Degree

Job

Engineering

Employment Type

Full Time

Job Level

Associate"
272,Big Data Engineer,Amazon Web Services (AWS),"Jersey City, NJ","Job Summary

DESCRIPTION

Amazon Web Services (AWS) is the leading platform for designing and developing applications for the cloud and is growing rapidly with hundreds of thousands of companies in over 190 countries on the platform. The Worldwide Revenue Operations (WWRO) team will be the authoritative source of customer metadata and the solutions team for applications that action AWS’ strategies to better serve our customers. We invest resources in information and solutions enabling AWS sales and business teams that yield increased customer adoption and an optimal customer experience.

Are you an experienced Big Data Engineer passionate about building scalable, enterprise-level systems? We are looking for a Big Data Engineer to play a key role in building next generation tools and solutions. In addition to technical expertise, you will invest time to understand the needs of the business, the data behind it, and how to transform information into technical solutions that allow the business to take action.

You should have deep expertise in the design, creation, management, and business use of large datasets, across a variety of data platforms. You should have excellent business and interpersonal skills to be able to work with business owners to understand data requirements, and to build ETL to ingest the data into the data lake. You should be an authority at crafting, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data lake. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive growth.

This position may be based in Seattle (WA), Dallas (TX), Boston (MA), New Jersey/New York, Atlanta (GA) or Arlington (VA). Relocation offered for US residents only.

About Us

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have twelve employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.


Basic Qualifications
This position requires a Bachelor's Degree in Computer Science or a related technical field, and 7+ years of relevant work experience.
5+ years of work experience with ETL, Data Modeling, and Data Architecture.
Expert-level skills in writing and optimizing SQL.
Experience with Big Data technologies such as Hadoop, Hive/Spark.
Proficiency in one of the scripting languages - python, ruby, linux or similar.
Experience operating very large data warehouses or data lakes.
Preferred Qualifications
Authoritative in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.
Experience with building data pipelines and applications to stream and process datasets at low latencies.
Demonstrate efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A1817825"
273,"DATA ENGINEER I, Training and Certification Data Engineering Team",Amazon Web Services (AWS),"Seattle, WA","Job Summary

DESCRIPTION

Amazon Web Services is seeking an experienced self directed, analytical and strategic Data Engineering for our AWS Training and Certification team that will drive projects and build data interface that improve the efficiency of our global operations. This is a unique opportunity to think big, insist on the highest standards, and invent and simplify the data architecture to scale.

Do you have deep expertise in the end to end development of large datasets across a variety of platforms? Are you great at designing data systems and redefining best practices with a cloud-based approach to scalability and automation? In this role, you will be responsible for scaling our existing infrastructure, incorporating new data sources, and building robust data pipelines for production level systems. In partnership with product and business teams, you will work backwards from our business questions to drive scalable solutions. You will be a technical leader owning the architecture of our data platform and influence best practices across multiple teams. Above all, you should be passionate about working with data to answer business questions and drive growth.

This job can be located near any AWS office in the United States.

Key Responsibilities Include
Design, implement, and maintain a cutting-edge cloud-based data-infrastructure for large data-sets.
Develop and optimize data tables using best practices for partitioning, compression, compaction, etc.
Develop and support ETL pipelines with robust monitoring and alarming
Maintain data integrity, availability, and auditability. Manage AWS resources. Drive the adoption of new technologies and new best practices
Mentorship

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our organizations members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.

Work life balance

Our team also puts a high value on work-life balance. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here, which is why we aren’t focused on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well-balanced life—both in and outside of work.

Inclusion

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.


Basic Qualifications
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline
2+ years of industry experience in Data Engineering or related field with experience manipulating, processing, and extracting value from large datasets
Ability to write high quality, maintainable, and robust code, often in SQL, Scala and Python. Strong in Data Structures.
2+ Years of Data Warehouse Experience with MPP systems like Redshift, PostgreSQL, etc. Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
Extensive experience working with cloud services (AWS or MS Azure or GCS etc.) with a strong understanding of cloud databases (e.g. Redshift/Aurora/DynamoDB), compute engines (e.g. EMR/Glue), data streaming (e.g. Kinesis), storage (e.g. S3) etc.
Experience/Exposure using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Preferred Qualifications
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Athena, Kinesis and Lambda for serverless ETL)
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Strong analytical skills, 1+ years’ experience with Python and an interest in Machine Learning
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Ability to adapt new solutions in a fast changing environment
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
The pay range for this position in Colorado is $101,300 - $160,000[yr]; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon’s internal or external careers site.

Pursuant to the San Francisco and Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A2013960"
274,AWS Data Engineer,Dice,"Portland, OR","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Tanisha Systems, Inc., is seeking the following. Apply via Dice today!

full time AWS Data Engineer Location - 1-2 Month remote then Onsite Portland OR Job RequirementsDescription 5+ years relevant work experience in the Data Engineering field 3+ years of experience working with Hadoop and Big Data processing frameworks (Hadoop, Spark, Hive, Flink, Airflow etc.) 2+ years of experience Strong experience with relational SQL and at least one programming language such as Python, Scala, or Java Experience working in AWS environment primarily EMR, S3, Kinesis, Redshift, Athena, etc. Experience building scalable, real-time and high-performance cloud data lake solutions Experience with source control tools such as GitHub and related CICD processes. Experience working with Big Data streaming services such as Kinesis, Kafka, etc. Experience working with NoSQL data stores such as HBase, DynamoDB, etc. Experience with data warehousesRDBMS like Snowflake Teradata Top Skills - Python - SQL - AWS - Spark Regards , Nirdosh Singh Account Manager Tanisha Systems Inc. Phone x 569 Email mailto Web www.tanishasystems.com httpwww.tanishasystems.com 99 Wood Ave South, Suite 308, Iselin, NJ 08830 LinkedIn - httpswww.linkedin.cominnirdosh-soami-rajput httpswww.linkedin.cominnirdosh-soami-rajput"
275,"Data Engineer, Snowflake",Deloitte,"Panama City, FL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
276,"Data Engineer, Snowflake",Deloitte,"Princeton, NJ","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
277,Senior Data Engineer - Supply Chain Analytics,HelloFresh,"New York, NY","At HelloFresh, we want to change the way people eat forever by offering our customers high quality food and recipes for different meal occasions. Over the past 10 years, we've seen this mission spread around the world and beyond our wildest dreams. Now, we are a global food solutions group and the world's leading meal kit company, active in 16 countries across 3 continents. So, how did we do it? Our weekly boxes full of exciting recipes and fresh ingredients have blossomed into a community of customers looking for delicious, healthy and sustainable options. The HelloFresh Group now includes our core brand, HelloFresh, as well as: GreenChef, EveryPlate, Chef's Plate, Factor, and Youfoodz.

Come see what's cookin' at HelloFresh!

At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people.

We are the industry leader in meal-kit subscription services and we're growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people.

The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result.

Job Description:

As a Senior Data Engineer at HelloFresh, you will be building and extending highly performant data products, using cutting-edge technologies. Within the Supply Chain Analytics team, you will work closely with fellow data engineers, data scientists, and analysts to teams across the Supply Chain to capture efficiencies. All in an effort to help deliver our vision, of delivering the perfect product, at the right time, to the doorstep of our customers.

You will ...
Build reusable technology that enables Supply Chain teams to capture, process, store, and serve their data products in an easy way
Build and maintain complex and scalable ETL pipelines
Work closely with the data infrastructure team and ensure and promote data quality and governance standards
Coaching and mentoring data engineers and developing internal talent

At a minimum, you have...
Extensive experience in Software Engineering and the ability to design, implement and deliver maintainable and high-quality code with Python
Expertise with Kafka, the Hadoop Ecosystem (Hadoop, Kafka, Spark)
Comfortable with cloud-based services & warehousing (AWS S3, EMR, Snowflake) and with containers (Docker, Kubernetes, ECS, EKS)
Demonstrated ability to design, build and maintain complex data products

You'll get…
Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 16 weeks & PTO policy
$0 monthly premium and other flexible health plans effective first day of employment
75% discount on your subscription to HelloFresh (as well as other product initiatives)
Snacks, cold brew on tap & monthly catered lunches
Company-sponsored outings & Employee Resource Groups
Collaborative, dynamic work environment within a fast-paced, mission-driven company

About HelloFresh

We believe that sharing a meal brings people of all identities, backgrounds, and cultures together. We are committed to celebrating all dimensions of diversity in the workplace equally and ensuring that everyone feels a sense of inclusion and belonging. We also aim to extend this commitment to the partners we work with and the communities we serve. We are constantly listening, learning, and evolving to deliver on these principles. We are proud of our collaborative culture. Our diverse employee population enables us to connect with our customers and turn their feedback into meaningful action - from developing new recipes to constantly improving our process of getting dinner to our customers' homes. Our culture attracts top talent with shared values and forms the foundation for a great place to work!

At HelloFresh, we embrace diversity and inclusion. We are an equal opportunity employer and do not discriminate on the basis of an individual's race, national origin, color, gender, gender identity, gender expression, sexual orientation, religion, age, disability, marital status or any other protected characteristic under applicable law, whether actual or perceived. As part of the Company's commitment to equal employment opportunity, we provide reasonable accommodations, up to the point of undue hardship, to candidates at any stage, including to individuals with disabilities."
278,"Data Engineer, Spark",Deloitte,"Foster City, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
279,"Data Engineer, Data Products Engineering",NBCUniversal,"New York, NY","Introduction

Responsibilities

At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.

About The Role

Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners’ strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered.

Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions
Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc)
2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas.
Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications.
Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data.
Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements
Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions
Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time.

Requirements

Qualifications/Requirements

Bachelor’s degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience.
Strong Computer Science/Engineering/Information Systems background
3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies.
Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas
Strong problem-solving skills.
Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences.

Desired Characteristics

Preferred Qualifications

1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research
Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc
Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills
Ability to design concise and visually appealing reports, user interfaces, mockups and documentation
Ability to read external API documentation and write pipelines to extract data from our partners’ systems
Ability to write and stand up internal API endpoints to share data with other internal teams.
Strong analytical focus, results-oriented and execution driven.
Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures.
Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed.
Flexibility to adjust to changing requirements, schedules and priorities.
Ability to work independently under minimum supervision and proactive in solving issues
Energetic, committed and solution focused with the ability to perform under pressure and meeting targets

Sub-Business

Engineering

Career Level

Experienced

City

See List Below

State/Province

Multiple Locations

Country

United States

Multiple Locations

New York - New York, Remote

About Us

NBCUniversal owns and operates over 20 different businesses across 30 countries including a valuable portfolio of news and entertainment television networks, a premier motion picture company, significant television production operations, a leading television stations group, world-renowned theme parks and a premium ad-supported streaming service.

Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. We strive to foster a diverse and inclusive culture where our employees feel supported, embraced and heard. We believe that our workforce should represent the communities we live in, so that together, we can continue to create and deliver content that reflects the current and ever-changing face of the world. Click here to learn more about Comcast NBCUniversal’s commitment and how we are making an impact.

Notices

NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.

NBCUniversal is an equal opportunity employer and will provide reasonable accommodations as required by applicable federal, state, and/or local laws."
280,Data Engineer II,Amazon,"New York, NY","Description

The Seller Partner Infrastructure and Core Experience Accelerator team seeks a talented, innovative and driven Data Engineer. Our team is foundational to the experience of our Selling Partners, managing the core infrastructure for Seller Central and Vendor Central including the site building features, accounts, and authorization models which fuel world class seller applications.

As an Amazon Data Engineer you will be working with one of the world's largest and most complex data processing environments. You will need expertise in the design, creation, management, and business use of extremely large datasets. From Day 1, you will be challenged with a variety of tasks, ranging from server administration to metadata modeling. You will interact with internal program and product owners, and technical teams to gather requirements, structure scalable and performant data solutions, and gain a deep understanding of key datasets. You will contribute to the evolution of your team’s data engineering practices, recommending changes in development, training, maintenance, and system standards. Above all you should be passionate about answering challenging and complex questions using data and your own dash and drive.

Come join our team and deliver amazing solutions that use data to delight both buyers and sellers and bring them together.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience working with AWS big data technologies (EMR, Redshift, S3)
Demonstrated strength in data modeling, ETL development, and data warehousing
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering

Company - Amazon.com Services LLC

Job ID: A1606947"
281,Data Engineer - AL/ML AWS,JPMorgan Chase & Co.,"Jersey City, NJ","As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Advanced knowledge of application, data, and infrastructure architecture disciplines
Understanding of architecture and design across all systems
Working proficiency in developmental toolsets
Knowledge of industry-wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals
Passionate about building an innovative culture
Proficiency in one or more modern programming languages
Understanding of software skills such as business analysis, development, maintenance, and software improvement
Proficiency in Oracle and data analytical skills highly preferred
Proficiency in Cloud technologies AWS or Azure or Google Cloud will be a preferred
Experience in AI/ML will be preferred
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
282,Data Engineer,CVS Health,"Washington, United States","Job Description

This Data Engineer role will collaborate with business partners to identity opportunities to leverage big data technologies in support of CVS.com Personalization with a common set of tools and infrastructure to make analytics faster, more insightful, and more efficient. You will build and architect next generation Big Data machine learning framework developed on a group of core Big Data technologies. You will design highly scalable and extensible Big Data platforms which enables collection, storage, modeling, and analysis of massive data sets from numerous channels. You will define and maintain data architecture, focusing on applying technology to enable business solutions. You will assess and provide recommendations on business relevance, with appropriate timing and deployment. You will perform architecture design, data modeling, and implement CVS Big Data platforms and analytic applications. You will bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies. You will develop prototypes and proof of concepts for the selected solutions, and implement complex big data projects. You will apply a creative mindset to a focus on collecting, parsing, managing, and automating data feedback loops in support of business innovation.

Required

Required Qualifications
Hands-on experience with “big data” platforms including Hadoop (preferably Azure or GCP) and Spark
Proficiency in “big data” technologies including Spark, Airflow, Kafka, Hbase, Pig, NoSQL databases, etc.
Experience and background on traditional relational data warehouse technologies like Oracle, Teradata, DB2
Proficiency in one of the following programming languages: PySpark, Scala, or Java
Good knowledge and experience with SQL, including Analytical SQL functions
BA degree required with 2+ years of relevant experience; or a Master's degree or equivalent in an analytic discipline and 1+ years of relevant experience

COVID Requirements

COVID-19 Vaccination Requirement

CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.

You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.

Preferred

Preferred Qualifications
Data Architect and Data Modelling experience in creating Semantic Layer Data Models
Ability to design and build a framework to orchestrate data pipelines and Machine Learning models
Proficiency with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)
Design and implement end-to-end solutions using Machine Learning, Optimization, and other advanced technologies, and own live deployments
Experience with frameworks for either Machine Learning or NLP (Scikit-Learn, SpaCy, Pytorch, Spark NLP)
Experience with developing Metrics and KPI data layers for broader business and analytic consumption
Experience with cloud computing environment (Microsoft Azure/GCP)
Experience working as a lead on a large scale Spark implementation
Productionalized one or more ML application on a big data platform
Education

Bachelor's Degree in Computer Science, Engineering, Statistics, Physics, Math or related fields.

Master's Degree Preferred – with coursework focused on advanced algorithms, mathematics in computing, data structures etc.

Business Overview

At CVS Health, we are joined in a common purpose: helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart.

We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, sex/gender, sexual orientation, gender identity or expression, age, disability or protected veteran status or on any other basis or characteristic prohibited by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
283,"Data Engineer, Analytics and Insights",Amazon,"New York, NY","Description

**We are open to hiring this role into New York NY, Chicago IL or Santa Monica CA!**

Amazon Advertising exists at the intersection of marketing and ecommerce and offers advertisers a rich array of innovative advertising solutions across Amazon owned properties as well as third party properties Amazon operates. We believe that advertising, when done well, can greatly enhance the value of the customer experience and generate a positive return on investment for our advertising partners.

We are looking for a passionate data engineer (DE) to own creation of a data lake to support Analytics & Media Managers (AMM) who need a broad set of data to develop insights for advertisers. Data is at the center of every insight we will develop as we create business analytics solutions that serve the needs of our advertisers. You will share in the ownership of the technical vision and direction for advanced analytics and insight products/ services. You will be a part of a team of top notch analytical professionals developing complex modeling/analyses for our customers and with a focus on sustained operational excellence. Members of this team will be challenged to innovate using big data technologies. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive their business. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.

Key job responsibilities
Build data pipelines and navigate permissions/compliance requirements to onboard new datasets while balancing performance and cost.
Optimization of existing pipelines.
Work with team to scale ad-hoc insights to thousands of customers. Via optimization of workloads and leveraging AWS.
Develop ETL to collect disparate data sources (via API, and AWS) to support critical business operations.

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in data modeling, data-pipelines development, and data warehousing (big data challenges)
Experience with a data warehouse technology
Experience using big data technologies (S3, MPP database, Spark/Scala, EMR, Redshift/Spectrum, Python, etc.)
Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
Experience working with cloud computing and infrastructure (AWS, Google Cloud, Azure)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Experience with writing design documentation
Comfortable working with SDEs
Preferred Qualifications
Experience working with AWS big data technologies (EMR, Redshift, S3, Athena, Glue)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Masters or PhD in computer science, mathematics, statistics, economics, or other quantitative field
Understanding of data retention policies and standards in accordance with data privacy laws
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1763017"
284,"Data Engineer, AWS Monetization",Amazon Web Services (AWS),"New York, NY","Job Summary

DESCRIPTION

Are you interested in building the next generation, cloud-based commerce system for AWS that’s used by millions of customers worldwide? Are you excited by the idea of building real-time stream processing systems that operate at Petabyte scale? Do you want to make an impact at a $10-billion-a-year business? Then we need to talk!

About The Hiring Group

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.

Job responsibilities

The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity problems, can multi-task, and can credibly interface between technical teams and business stakeholders.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.


Basic Qualifications
This position requires a Bachelor's Degree in Computer Science or a related technical field
5+ years of work experience with ETL, Data Modeling, and Data Architecture.
Expert-level skills in writing and optimizing SQL.
Experience with AWS products including Big Data Technologies (Redshift, RDS, S3, Glue, Athena, EMR, Spark, Hive, etc.)
Proficiency in one of the scripting languages - Python, Scala, Java or similar.
Experience working with very large data warehouses or data lakes.
Preferred Qualifications
Experience with data visualization using Tableau, QuickSight, or similar tools
Authoritative in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.
Experience with building data pipelines and applications to stream and process datasets at low latency.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Dev Center U.S., Inc.

Job ID: A2009652"
285,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Acadia Technologies, Inc., is seeking the following. Apply via Dice today!

RoleData Engineer Duration 1 Year Rate DOE Skills Skills CPTOPTCitizens are encouraged to apply who doesnt need sponsership need to apply"
286,Data Engineer,JPMorgan Chase & Co.,"Newark, DE","As a member of our Software Engineering Group, we look first and foremost for people who are passionate around solving business problems through innovation and engineering practices. You'll be required to apply your depth of knowledge and expertise to all aspects of the software development lifecycle, as well as partner continuously with your many stakeholders on a daily basis to stay focused on common goals. We embrace a culture of experimentation and constantly strive for improvement and learning. You'll work in a collaborative, trusting, thought-provoking environment-one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.

This role requires a wide variety of strengths and capabilities, including:
BS/BA degree or equivalent experience
Advanced knowledge of application, data, and infrastructure architecture disciplines
Understanding of architecture and design across all systems
Working proficiency in developmental toolsets
Knowledge of industry-wide technology trends and best practices
Ability to work in large, collaborative teams to achieve organizational goals
Passionate about building an innovative culture
Proficiency in one or more modern programming languages
Understanding of software skills such as business analysis, development, maintenance, and software improvement
Preferred Qualifications
Experience in working with/on data warehouses, data modelling, ETL/ ELT and SQL.
Experience with Spark and Hadoop ecosystem.
SQL performance tuning.
Experience with reporting tools like Tableau, Cognos etc.
JPMorgan Chase & Co., one of the oldest financial institutions, offers innovative financial solutions to millions of consumers, small businesses and many of the world's most prominent corporate, institutional and government clients under the J.P. Morgan and Chase brands. Our history spans over 200 years and today we are a leader in investment banking, consumer and small business banking, commercial banking, financial transaction processing and asset management.

We recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. We are an equal opportunity employer and place a high value on diversity and inclusion at our company. We do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with applicable law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.

The health and safety of our colleagues, candidates, clients and communities has been a top priority in light of the COVID-19 pandemic. JPMorgan Chase was awarded the ""WELL Health-Safety Rating"" for all of our 6,200 locations globally based on our operational policies, maintenance protocols, stakeholder engagement and emergency plans to address a post-COVID-19 environment.

As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. Employees are expected to follow the Firm's current COVID-19 or other infectious disease health and safety requirements, including local requirements. Requirements include sharing information including your vaccine card in the firm's vaccine record tool, and may include mask wearing. Requirements may change in the future with the evolving public health landscape. JPMorgan Chase will consider accommodation requests as required by applicable law.

Equal Opportunity Employer/Disability/Veterans"
287,Data Engineer *remote*,Robert Half,"Los Angeles, CA","Description
For immediate consideration, please message Ali Scott from Robert Half with your updated resume***
Functional Role: Data Engineer

Location: Anywhere in US or Canada *must be eligible to work in those countries*

Salary Range: $115,000-130,000 (DOE)

Technical Skills: Scala or Python, Postgres, SQL

Our client in the higher education industry is seeking two junior-mid level Data Engineers to join their growing team. This person either has experience with Scala or is willing to learn it, maintaining and developing code, working alongside the Data Architects building out data pipelines. If you have what it takes, then look no further!

Requirements

Top Requirements:
2-3 years experience
Scala or someone ready to learn it
Maintaining existing code
This role will be driven by the data architect building out the practice and building out data pipelines
SQL, Postgres, Python/Scala, eager to learn
For immediate consideration, please message Ali Scott from Robert Half with your updated resume***
Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
288,"Data Engineer, Snowflake",Deloitte,"Alexandria, VA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
289,Data Engineer - Opportunity for Working Remotely,VMware,"Nashville, TN","The Elevator Pitch: Why will you enjoy this new opportunity?

You want to be a part of an innovative company of 20000+ people working in 50+ locations worldwide and committed to building a community where great people want to work long term by living our values of passion, innovation, execution, teamwork, active learning and giving back. If you are ready to accelerate, innovate and lead, join us as we challenge constraints and problem solve for tomorrow today.

You are highly motivated and would love to be part of VMware Data Engineering team working to solve complex business problems and bring digital transformations

What is primary need, technical challenge, and/or problem you will be responsible for?

VMware Data Engineering team is seeking a highly motivated, experienced Data Engineer within the IT Data Engineering and Analytics group. This position is responsible for hands on development work on all aspects of Data Engineer, data provisioning, modeling, performance tuning and optimization. The candidate will work closely with both Enterprise and Solution Architecture teams to translate the Business/Functional requirements into technical specifications that drive Hadoop/HANA/BI solutions to the meet functional requirements.

Success in the Role: What are the performance goals over the first 6-12 months you will work toward completing?

Within the first few months you will spend time learning VMware’s coding standards, products, and increasing you know how of the technology landscape around data. We want you to be curious, learning both from team members and individual study. You will collaborate with other team members and participate in architecture reviews.

You will closely work with other data product owners/engineers towards taking ownership of few existing artifacts within the data landscape. You will be required to help in troubleshooting any upcoming production defects and perform production support. You will also work on delivering specific enhancements in an agile delivery model

What type of work will you be doing? What assignments, requirements, or skills will you be performing on a regular basis?

You will work in a fast paced and agile work environment.
You will communicate and engage with a range of stakeholders.
You will be responsible for hands on development work building scalable Data engineering pipelines and other data engineering/modelling work using one or more of Python, Kafka, Hadoop/Hive, Presto etc.
You will have to query data using SQL or other techniques. Excellent SQL & Analytical SQL functions knowledge will be needed
Understanding of SAP HANA and Knowledge of Data Integration Platforms - Informatica PowerCenter, SAP BODS, SDI, SLT (is desired but not mandatory) and will help you in understanding existing landscape
Bachelor’s degree or equivalent with 4+ years of Data Engineering experience in Big Data Solutions is required for this role. MS degree would be highly desirable
You will be owner of specific modules. You will collaborate with other team members on improving dev practices, do peer code reviews and provide production support

What is the leadership like for this role? What is the structure and culture of the team like?

This role reports into Sapan Bajpai who is a Senior Manager for IT Data Engineering and Analytics.

IT Data Engineering and Analytics team is spread across VMware offices in Bangalore, Chennai, Palo Alto(USA) , Austin(USA) , Cork(Ireland), Beijing(China) and Costa Rica

The team is headed by Director, IT Data Engineering and Analytics based in Palo Alto

What are the benefits and perks of working at VMware?

You and your loved ones will be supported with a competitive and comprehensive benefits package. Below are some highlights, or you can view the complete benefits package by visiting www.benefits.vmware.com.

Employee Stock Purchase Plan
Medical Coverage, Retirement, and Parental Leave Plans for All Family Types
Generous Time Off Programs
40 hours of paid time to volunteer in your community
Rethink's Neurodiversity program to support parents raising children with learning or behavior challenges, or developmental disabilities
Financial contributions to your ongoing development (conference participation, trainings, course work, etc.)
Healthy and local inspired snacks in all our on-site pantries

Category : Engineering and Technology

Subcategory: Software Engineering

Experience: Manager and Professional

Full Time/ Part Time: Full Time

Posted Date: 2022-04-19

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
290,"Data Engineer - Strategic Decisions, Topology",Amazon,"Austin, TX","Job Summary

DESCRIPTION

The Amazon Topology team does end-to-end supply chain estimation and optimization. This includes determining the number and placement of all types of facilities, how they should be configured, and how they should be connected to each other. There are thousands of facilities ranging from million-square-foot Fulfillment Centers with thousands of robots down to inner-city Prime Now facilities serving orders to be fulfilled within an hour. Each year we spend billions on these facilities and billions more on transportation costs between them. If you are interested in diving into a multi-discipline, high impact space this team is for you.

Handling data and making decisions on this scale requires a range of scientific tools. We use demand forecasting, optimization for various core decisions, machine learning to approximate the network, and simulation of how our choices will perform. The team is a mixture of Software Engineers, Operations Research scientists, Applied Scientists, Business Intelligence Engineers and Product Managers.

We are looking for a Data Engineer (DE) to help design the models, translate business questions into precise simulations, and generally investigate how the current network is functioning.

Watch this video to learn more about our organization: http://bit.ly/amazon-scot

Key job responsibilities

As a Data Engineer, you should be an expert in the architecture of DW solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business analysts and engineers to determine how best to design the data warehouse for reporting and analytics. You will be responsible for designing and implementing scalable ETL processes in the data warehouse platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune SQL to provide optimized solutions to the business.


Basic Qualifications
2+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
2+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Proficiency in SQL
A desire to work in a collaborative, intellectually curious environment.
Degree in Computer Science, Engineering, Mathematics, or a related field or 4+ years industry experience
Demonstrated strength in data modeling, ETL development, and data warehousing.
Data Warehousing Experience with Oracle, Redshift, PostgreSQL, etc.
Query performance tuning skills
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)
Experience with Cloud Technologies
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, etc.
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Linux/UNIX.
Experience with AWS
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1913191"
291,"Data Engineer, Amazon in the Community",Amazon,United States,"Job Summary

DESCRIPTION

If you aren't in Seattle but near a US Amazon corporate office and see this role as a fit, we would like to hear from you.

Do you love solving problems whose solutions make the world a better place? As the Amazon in the Community team, we are responsible for leveraging Amazon's assets for good, concentrating on the pillars of Education, Housing Equity, Disaster Relief, and Hunger. We design and build against what a child needs each day to show up at school, ready to learn and be curious: https://www.aboutamazon.com/impact/community

Our team is looking for a Data Engineer to deliver technical solutions to support Amazon’s charitable giving work around the world. As a Data Engineer, you will design, develop and enhance data systems and pipelines supporting Amazon's ability to scale our work with community partners, and to measure the activity and impact globally. You will be the subject matter expert for community impact and charitable giving data pipelines and usage that span 70+ data streams, while striving for efficiency by aligning data systems and data management approaches with organizational needs.

As a Data Engineer you will collaborate with other data engineers, software developers, business intelligence engineers as well as program and product partners to deliver end to end solutions that meet business goals. You will work with your internal customers to balance customer requirements with team requirements and develop data solutions to help your team and business evolve. A successful candidate will thrive in a highly collaborative, diverse, results-oriented environment that leverages agile planning techniques. We are looking for an engineer who is comfortable working through ambiguity and is a creative problem solver. And, of course, has a passion for the community.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
Experience using AWS-based technologies including S3, Athena, Redshift, Spectrum, and Glue ETL.
Track record of manipulating, processing, and extracting value from large and diverse datasets.
A desire to work in a collaborative, intellectually curious environment.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience providing technical leadership and mentor other engineers for the best practices in the data engineering space
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2009801"
292,Senior Data Engineer,Walmart,"Bentonville, AR","Position Summary... What You'll Do...

Data Strategy: Understands, articulates, and applies principles of the defined strategy to routine business problems that involve a single function.

Data Source Identification: Supports the understanding of the priority order of requirements and service level agreements. Helps identify the most suitable source for data that is fit for purpose. Performs initial data quality checks on extracted data.

Analytical Modeling: Selects the analytical modeling technique most suitable for the structured, complex data and develops custom analytical models. Conducts exploratory data analysis activities (for example, basic statistical analysis, hypothesis testing, statistical inferences) on available data. Defines and finalizes features based on model responses and introduces new or revised features to enhance the analysis and outcomes. Identifies the dimensions of the experiment, finalizes the design, tests hypotheses, and conducts the experiment. Perform trend and cluster analysis on data to answer practical business problems and provide recommendations and key insights to the business. Mentors and guides junior associates on basic modeling and analytics techniques to solve complex problems.

Model Assessment & Validation: Identifies the model evaluation metrics. Applies best practice techniques for model testing and tuning to assess accuracy, fit, validity, and robustness for multi-stage models and model ensembles.

Model Deployment & Scaling: Supports efforts to ensure that analytical models and techniques used can be deployed into production. Supports evaluation of the analytical model. Supports the scalability and sustainability of analytical models.

Code Development & Testing: Writes code to develop the required solution and application features by using the recommended programming language and leveraging business, technical, and data requirements. Test the code using the recommended testing approach.

Data Visualization: Generates appropriate graphical representations of data and model outcomes. Understands customer requirements to design appropriate data representation for multiple data sets. Work with User Experience designers and User Interface engineers as required to build front end applications. Presents to and influences the team and business audience using the appropriate frameworks and conveys clear messages through business and stakeholder understanding. Customize communication style based on stakeholder under guidance, and leverages rational arguments. Guide and mentor junior associates on story types, structures, and techniques based on context.

Problem Formulation: Translates business problems within one's discipline to data related or mathematical solutions. Identifies what methods (for example, analytics, big data analytics, automation) would provide a solution for the problem. Shares use cases and gives examples to demonstrate how the method would solve the business problem.

Applied Business Acumen: Provides recommendations to business stakeholders to solve complex business issues. Develops business cases for projects with a projected return on investment or cost savings. Translates business requirements into projects, activities, and tasks and aligns to overall business strategy. Serves as an interpreter and conduit to connect business needs with tangible solutions and results. Recommends new processes and ways of working. Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales. Provides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities. Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.

The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.

Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years’ experience in

software engineering or related field. Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related

field.

2 years' experience in data engineering, database engineering, business intelligence, or business analytics. Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering Primary Location...702 SW 8TH ST, BENTONVILLE, AR 72716, United States of America"
293,Senior Data Engineer - Data Platform,UrbanFootprint,"Berkeley, CA","Our Company

UrbanFootprint is the world’s first Urban Intelligence Platform. We provide critical intelligence to the institutions that are rebuilding the world's infrastructure. Where does the energy sector invest in electrification, decarbonization, and asset hardening in the face of climate threats? Where do cities and businesses invest to catch up with e-commerce, last-mile delivery, and new mobility? Where do governments deploy relief and new infrastructure to combat record hunger, homelessness, and hazard vulnerability? UrbanFootprint provides detailed and actionable answers to these questions and more.

UrbanFootprint is ‘Google Maps for the Modern Enterprise.’ We organize, normalize, and align thousands of urban, climate, and community metrics across the continental U.S.. The platform delivers targeted insights via dynamic data streams and collaborative web mapping applications. We enable our customers to answer complex questions in minutes versus weeks, months, or years. Our customers include some of the largest energy utilities, major financial institutions, critical government agencies, top urban planning firms, and fast-growing mobility companies.

We’re growing rapidly in a market with a TAM of $22B, and our competition is old-world manual consulting or outdated software tools that come without the data, the models, or the insights.

Our founders, Joe DiStefano and Peter Calthorpe, are urban planning pioneers who have spent decades providing critical urban intelligence to cities and enterprises across the globe. UrbanFootprint was named one of the World’s Most Innovative companies in 2021, and is on the GovTech 100 list. Our platform was awarded the top spot in FastCo’s Innovation by Design competition.

This is a fully remote position and can be based in any US location.

The role

As a socially-driven Senior Data Engineer, you will be responsible for building UrbanFootprint’s foundational data infrastructure. These internal tools enable Data Scientists to develop our proprietary models and algorithms that support wide ranging, real-world problems. By ensuring that data is accurate, accessible and up-to-date, this platform powers models that allow our customers to design environmentally conscious growth plans, target government relief dollars, and promote equitable community development.

You are an enthusiastic enabler; you love knowing that your data platform empowers others to thrive. You know that company growth and success means enabling others to unblock themselves by designing systems that meet internal user needs, crafting discoverable documentation, and building scalable infrastructure that is easy to use. As our Senior Data Engineer, you’re thoughtful and pragmatic; you understand the business needs of today and participate in design discussions for the future. You understand that data quality doesn’t mean ‘there’s no missing data’; it means understanding missing data happens. You know that you need to work with downstream consumers on how and when to address it. You are autonomous, not independent; you solicit feedback from end-users, clearly communicate progress, and take ownership of the reliability of the infrastructure and the accuracy of the data you produce.

What you’ll do:

Collaborate with data engineers and our Head Architect to design and implement our internal data infrastructure, focusing on data storage and versioning, data discoverability and data access.
Own the accuracy and integrity of our data and build automated systems and processes to ensure our systems stay current with the latest public data.
Collaborate with Data Scientists to define, monitor and automate data quality metrics reporting.
Mentoring team members to build a world-class “full stack” data science culture and support data scientist and solution analyst independence.

Your background most likely includes:

Work experience equivalent to an M.S. in Computer Science/Engineering or Data Engineering.
4+ years of relevant industry experience including designing, building and maintaining production ETLs.
High proficiency in at least one SQL dialect, Python, and at least one scalable data analytics framework such as Dask, PySpark, or Apache Beam.
Experience with Airflow.
Experience with Kubernetes.

Bonus qualifications:

Data engineering or machine learning-related certifications such as: GCP Professional Data Engineer or Machine Learning Engineer; Databricks Certified Associate Developer for Apache Spark, or similar.
Hands on experience with the GCP platform including GKE, DataProc, Cloud Composer and/or Vertex AI.
Experience working closely with data or machine learning scientists.
Experience with geospatial or spatio-temporal data including both raster and vector data.
Experience with open-source geospatial data such as Open Street Maps and US Census.
You are socially driven to leverage data to facilitate a more equitable and resilient society.


UrbanFootprint is committed to diversity in its workforce. We are committed to equal employment opportunity regardless of race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class."
294,Data Engineer / Python / SQL,Motion Recruitment,"Irvine, CA","A hedge fund with tech driven projects to implement data driven decision making is hiring a data engineer with strong Python OOP skills and strong SQL skills to join the team. In this position you will be responsible for data pipelines for Excel based models migrating to Python and SQL. This position also involves Pandas use on a day to day. This is an exciting challenge within a growing company, a senior team, and impact company performance with solutions.

This company is located in Irvine, California and open to hiring from any California based locations. This position does not offer sponsorship, candidates must be authorized to work now and in the future.

Required Skills & Experience
3+ years of backend Python Programming
experience building Data pipelines from scratch
strong fundamental and production level SQL skills
core understanding of relational databases
experience with migration to AWS
Experience with Pandas, numpy, Scki-kit learn
Experience with AWS Redshift
What You Will Be Doing

Daily Responsibilities
Build and manage financial models (valuation, monte carlo, etc.) for life settlement portfolios in Python
Manage the entire project development life cycle from requirements gathering, developing, testing, deployment, enhancements
Analyze and convert advanced Excel files (VBA, index/match, sumproduct) into production level code
Investigate bugs and reconcile differences between models
Explain model changes to a non-technical business audience
The Offer
Competitive Salary: Up to $160K/year, DOE
You Will Receive The Following Benefits
Medical Insurance & Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit
Remote options/capabilities
Posted By: Julie Bennett"
295,Index Data Algorithm Engineer,"eVestment, A Part of Nasdaq",Buffalo-Niagara Falls Area,"We are seeking an experienced financial algorithm / data engineer to join our effort in developing the next generation of index construction capabilities.

The vision for the Index Data Platforms and Models (DP&M) team is to improve client experiences by using data as a means of empowering the Nasdaq Index Business with data-driven insights and solutions. Within this organization, you will be focused on building modular functions for data transformation and index construction algorithms (e.g., security identification, factor screening, and weighting techniques) that support construction and rebalance of Nasdaq’s index suite.

In this role, you will be responsible for working with Research and Development and Index Portfolio Management while collaborating with our extended Data Platforms and Models team to deliver tangible value in the form of coded algorithms that translate a variety of input data into index portfolios by applying data transformation and portfolio construction methodologies, techniques and strategies.

The ideal person possesses a portfolio construction and data analytics acumen, a strong understanding of data technologies, and expertise in software engineering best practices.
Build, automate, maintain, and monitor index construction and maintenance algorithms using standardized methodologies, with reliability and scalability in mind
Improve the performance and reliability of our index research, historical simulation, portfolio construction, and rebalance and reconstitution processes and algorithms
Work in collaboration with Research and Development to translate their needs into scalable, standardized solutions
Develop standards and processes for onboarding new data transformations into research and production
Collaborate with other specialists, product managers, and engineers to identify and solve critical business problems, crafting a positive experience for internal and external clients
Bachelor’s degree in Computer Science, Mathematics, Statistics or related area
5+ years of experience in Software Engineering, Data Engineering, or Trading Strategy Modeling
5+ years working with relational databases and query languages, including building data pipelines and the ability to work across structured, semi-structured and unstructured data
5+ years writing clean, maintainable, and robust code in Python, Scala, Java, or similar coding languages
Experience working with cloud platforms (AWS preferred), development and operations technologies (e.g., Kubernetes, Gitlab), and data management utilities (Databricks, Apache Parquet, etc.)
Strong quantitative reasoning skills and an interest in working at the intersection of research and software engineering
Experience preparing data for analytics and following a data science workflow to drive business results
Practical experience with Agile principles, preferably with Scrum, Kanban, or SAFe frameworks
Promotion of a strong control environment, adherence to risk controls and procedures, and process discipline, while appropriately considering business priorities
Come as You Are

Nasdaq is an equal opportunity employer. We positively encourage applications from suitably qualified and eligible candidates regardless of age, color, disability, national origin, ancestry, race, religion, gender, sexual orientation, gender identity and/or expression, veteran status, genetic information, or any other status protected by applicable law.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation."
296,"Market Intel, Data Engineer , WFS-MI",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The Market team for Workforce Staffing applies science, and insights to optimize hiring for Amazon’s largest candidate population – Tier 1 Associates. Amazon's hourly workforce brings the magic of Amazon’s industry-leading customer fulfillment to life. The pace at which job creation, hiring, and growth happen to support the scale and complexity of Amazon es is a problem Amazon is uniquely qualified to solve and innovate on. Workforce Staffing literally hires by the hundreds of thousands across multiple lines, job types, and shift configurations. The Market team in particular focuses on applying labor market, competitor, and candidate preference to enhance job offerings, mitigate operational risk, and sustain Amazon position in the market. Come join a team that is continually shaping and writing the future of the hourly worker landscape.

As a Data Engineer for the Market Intel team , you will excel in the design, creation, management, and use of large volumes of data that will become the core set for research and insights. You will be responsible for designing and implementing scalable ETL processes in the warehouse platform to support the rapidly growing and dynamic demand for across various sources, and use it to deliver the as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune to provide optimized solutions to the . You will work cross-functioning with analysts, scientists, and other data engineers to determine the best design for scale and maintenance.

Key Responsibilities Include
Building and migrating the complex ETL pipelines into cluster
Optimizing the performance of -critical queries and dealing with ETL job-related issues
Gather and understand requirements, work in the team to achieve high quality ingestion and build systems that can process the , transform the
Designing, implementing and supporting a data platform that can support business needs
Earn the trust of your customers by continuing to constantly obsess over their needs and helping them solve their problems by leveraging technology
Manage critical initiatives to enforce standard work and reduce waste

Basic Qualifications
Degree in Computer Science, Mathematics, or a related field or 2+ years industry experience
Coding proficiency in at least one modern programming language (Python, Java, etc.)
Demonstrated strength in modern , ETL development, lake design and implementation, and warehousing.
Warehousing Experience with Redshift
Query performance tuning skills
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Development Engineer , Data Scientist) with a track record of manipulating, processing, and extracting data from large data sets.
Experience with AWS technologies
Experience leading large-scale warehousing and projects, including using AWS technologies – S3, EC2, etc.
Experience building/operating highly available, distributed systems of extraction, ingestion, and processing of large data sets
Experience building products incrementally and integrating and managing sets from multiple sources
Experience in providing technical leadership and mentoring
A desire to work in a collaborative.
Ability to work on a diverse team or with a diverse range of coworkers.
The base pay range for this position in Colorado is $ 154,600 - $209,100 a year; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon’s internal or external careers site.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2031897"
297,"Senior Data Engineer - Supply Chain Analytics (Boulder, CO)",HelloFresh,"Boulder, CO","At HelloFresh, we want to change the way people eat forever by offering our customers high quality food and recipes for different meal occasions. Over the past 10 years, we've seen this mission spread around the world and beyond our wildest dreams. Now, we are a global food solutions group and the world's leading meal kit company, active in 16 countries across 3 continents. So, how did we do it? Our weekly boxes full of exciting recipes and fresh ingredients have blossomed into a community of customers looking for delicious, healthy and sustainable options. The HelloFresh Group now includes our core brand, HelloFresh, as well as: GreenChef, EveryPlate, Chef's Plate, Factor, and Youfoodz.

Come see what's cookin' at HelloFresh!

At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people.

We are the industry leader in meal-kit subscription services and we're growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people.

The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result.

Job Description:

We are hiring a Senior Data Engineer to lead Data Infrastructure & Automation workstreams as a key member of the Data Engineering team within Operations Analytics.

We are growing our Data Engineering capabilities throughout our Supply Chain in order to take our data modeling and automation to the next level by leveraging big data tools and platforms (e.g. Airflow, Spark, Databricks etc.). Within the Operations Analytics team, you will work closely with fellow data engineers, data scientists, and analysts to build and maintain data products that improve the HelloFresh customer offering and enhance efficiency of our Operations.

Our vision is to develop and maintain a best-in-class automated data platform that supports our growing Supply Chain, in delivering the perfect product, at the right time, to the doorstep of all our customers. We are looking for someone who connects stakeholders to data through key projects, while simultaneously focusing on mentorship and developing internal talent.

You will ...
Data modeling: design, extend and update the HelloFresh's data model (create new schemas, fact tables, (mat) views, etc.)
Data infrastructure: Develop, deploy, monitor, and maintain ETLs on Airflow & Databricks
Data Cleaning: keep data clean and accurate with data sources (Internal & 3rd Party)
Data Transformations: implement the logic of the data pipeline (aggregations, projections, selections, etc …)
Design and implement end-to-end data products and Supply Chain automation flows: from data ingestions for data science modeling to creation of automated pipelines to external software (WMS, TMS, SAP etc.)

You are…
Familiar with data architecture, data governance and machine learning model implementations
Very organized and a structured problem solver with an articulated thought process
Able to develop an in-depth understanding of HelloFresh's core product and architecture, and act as an ambassador for state of the art software solutions and industry best practices, offering support and mentorship to colleagues
Comfortable operating in fast-paced, dynamic environments and working autonomously
Intellectually curious; don't mind questioning and challenging the status quo
A team player who efficiently collaborates with peers and stakeholders

At a minimum, you have...
MSc in a STEM discipline
5+ years' data engineering experience is required
Advanced Python (functional and OOP) and SQL skills (DDL, DML, CTEs, query optimization, ...), as well as experience with Apache Spark required
Knowledge of data structures (DataFrames, RDDs, Dataclasses) and data formats (CSV, JSON, Parquet, Avro, ORC)
Required experience installing data architectures on Cloud providers (e.g. AWS), using DevOps tools and automating data pipelines.
The ability to design, implement and deliver maintainable and high-quality code using best practices (e.g. git/github, secrets, configurations, yaml/json)
Experience with data modeling, design patterns, building highly scalable and secured solutions preferred
2+ years experience with job orchestration tools like Airflow or similar
Experience with end-to-end testing of data pipelines and implementing Data Quality checks (e.g. using frameworks AWS Deequ) is preferred.
Experience with Dash (and/or Shiny) is a plus

You'll get…
Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 16 weeks & PTO policy
$0 monthly premium and other flexible health plans effective first day of employment
75% discount on your subscription to HelloFresh (as well as other product initiatives)
Snacks, cold brew on tap & monthly catered lunches
Company sponsored outings & Employee Resource Groups
Collaborative, dynamic work environment within a fast-paced, mission-driven company

Salary Range: $120,000-$130,000

Position is eligible to participate, contingent on Supervisory Board Approval, in HelloFresh's equity program.

About HelloFresh

We believe that sharing a meal brings people of all identities, backgrounds, and cultures together. We are committed to celebrating all dimensions of diversity in the workplace equally and ensuring that everyone feels a sense of inclusion and belonging. We also aim to extend this commitment to the partners we work with and the communities we serve. We are constantly listening, learning, and evolving to deliver on these principles. We are proud of our collaborative culture. Our diverse employee population enables us to connect with our customers and turn their feedback into meaningful action - from developing new recipes to constantly improving our process of getting dinner to our customers' homes. Our culture attracts top talent with shared values and forms the foundation for a great place to work!

At HelloFresh, we embrace diversity and inclusion. We are an equal opportunity employer and do not discriminate on the basis of an individual's race, national origin, color, gender, gender identity, gender expression, sexual orientation, religion, age, disability, marital status or any other protected characteristic under applicable law, whether actual or perceived. As part of the Company's commitment to equal employment opportunity, we provide reasonable accommodations, up to the point of undue hardship, to candidates at any stage, including to individuals with disabilities."
298,DATA ENGINEER ASSOCIATE - HSB - JOIN A GLOBAL COMPANY,HSB - Hartford Steam Boiler,"Hartford, CT","Posted by
Debbie Dostal
Senior Talent Acquisition Partner / Hartford, CT & Springfield, MA
Send InMail
HSB is hiring now for a Data Engineer Associate - Hybrid work schedule in our Hartford, CT Office.

Our fantastic work culture and our employees makes HSB a great place to work!

In this position you will assist with the data engineering activities within the Data Office of HSB.

Help to create high quality modeling datasets and manage the technical environment for analytics model development.

Work with IT and Data Scientists to improve the analytics model development process.

In addition, work to create modeling datasets by combining internal and external data assets.

Education and Experience:
Master's degree in one of the following: Computer Science, Information Management, Business, Math, or Statistics, or equivalent work experience
1+ years of data & analytics experience
Prefer 1+ years' experience in the following:
Python programmer within a modeling/research environment
Azure Cloud experience
P&C Insurance Industries

Knowledge And Skills:
Python, R, SQL, Hadoop
Strong experience in big data environment.
Analytics model development experience.
Strong data management experience.
Strong analytical and reasoning capabilities.
Detail oriented with a commitment to documentation of work performed, assumptions and results.
Well-developed communication and presentation skills.
Aptitude for performing multiple tasks and dealing with changing deadline requirements.
Well organized with the ability to set and meet deadlines.
Desire to learn new software, data sources and skills to improve data management and analytics model deployment.

***The ability to commute into the Hartford, CT Office to work a Hybrid work schedule is required.***

At The Hartford Steam Boiler, a subsidiary of Munich Re, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services.
We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions."
299,AWS Data Engineer,Deloitte,"Atlanta, GA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
300,Data Engineer - AWS,Dice,"Newark, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, STS Consulting, is seeking the following. Apply via Dice today!

Title Data Engineer - AWS Location Remote Newark, NJ - 07102 Duration 12 months+ Years of Experience Mid-Senior Level Right to hire Yes Job Description Experience developing and deploying enterprise applications in AWS. Working knowledge of AWS Services such as Glue, Athena, Secrets Manager, Lamdba. Experience developing and deploying application code using Python, SQL, Pyspark, boto3, and Spark SQL. Experience developing data pipelines. Experience with securing data using IAM and Active Directory. Experience with persisting data in AWS ndash s3, RDS, Redshift. Experience with monitoring and logging techniques to be used in conjunction with Cloudwatch and Splunk. Working knowledge of version control tools and branching techniques using Artifactory, Jenkins, and Bitbucket."
301,Data Engineer,Amazon,"Denver, CO","Job Summary

DESCRIPTION

The Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Fire tablets, Fire TV, Amazon Dash, and Amazon Echo.

What will you help us create?

The Team: How often have you had an opportunity to be a founding member of a team that is solving a significant problem through innovative technology? Would you like to know more about how we are envisioning the use of data analytics, machine learning, AI and linear programming to solve these problems? If this sounds intriguing, then we’d like to talk to you about a role on a new Amazon team that's tackling a set of problems requiring significant innovation and scaling.

We are seeking a Data Engineer with strong analytical, communication and project management skills to join our team. This role will be a key member of a Science and Data technology team based in Seattle, WA. Working closely with business stakeholders, software development engineers and scientist colleagues, you will design, evangelize, and implement state-of-the-art solutions for never-before-solved problems, helping Amazon Device to provide customer great products and keep the data secure. You will work with the most complicated data environment, employ right architecture to handle big data and support various analytics use cases, including business reporting, production data pipeline, machine learning, optimization models, statistical models, simulation, etc. Your work will have a direct impact on the day-to-day decision making in the Amazon Devices Sales & Operations Technology, and end customers.

You are an individual with outstanding analytical abilities, excellent communication skills, good business understanding, and technically savvy. The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity problems, can multi-task, and can credibly interface between technical teams and business stakeholders.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
5+ years of experience with data warehouse technical architectures, ETL/ ELT, reporting/analytic tools and, scripting.
5+ years of demonstrated quantitative and qualitative data experience with data modeling, ETL development
Knowledge of data modeling and experience SQL with Redshift, Oracle, MySQL, and Columnar Databases
Experience managing competing priorities simultaneously and driving projects to completion
Preferred Qualifications
Desire to create and maintain data warehouse systems
Experience with big data technologies
Master's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or Engineering
Experience with AWS services including S3, Lambda, EMR, RDS, Data-pipeline and other big data technologies
Experience with scripting (Python experience is a strong plus).
Proficient in the composition of Advanced SQL (analytical functions) and query performance tuning skills
Ability to interact, communicate, present and influence with both business and technical teams
A self-starter who loves data and who enjoys spotting the trends in it!
Amazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age

#d2ctech tag

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1927464"
302,Data Engineer,Amazon,"New York, NY","Job Summary

DESCRIPTION

Amazon Advertising operates at the intersection of e-Commerce and Advertising, offering a rich array of digital display advertising solutions with the goal of helping our customers find and discover anything they want to buy. We help advertisers reach Amazon customers across our owned and operated sites, on other high-quality sites across the web, and on millions of Kindles, tablets, and mobile devices. We start with the customer and work backwards in everything we do, including Advertising. If you’re interested in joining a rapidly growing team working to build a unique, world-class advertising group with a relentless focus on the customer, you’ve come to the right place!

Our team's mandate is to launch and accelerate adoption of advertising measurement products and services that enhance advertiser experience on Amazon. We are responsible for defining and delivering a collection of self-service performance measurement products that enable Advertisers to understand the impact of their Advertising on consumers. Our products are strategically important to drive long term growth. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.

The ideal candidate will have excellent problem investigation abilities, and the ability to synthesize data into crisp and clear recommendations for scientists and product leaders. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business needs.

In the role, you will work closely with product managers, scientists and software engineers to build out infrastructure, data pipelines, and reporting mechanisms for our team.

Our Data Engineer Duties & Responsibilities Will Include

Design and deliver big data architectures for experimental and production consumption between scientists and software engineering

Develop the end-to-end automation of data pipelines, making datasets readily-consumable by visualization tools and notification systems.

Create automated alarming and dashboards to monitor data integrity.

Create and manage capacity and performance plans.

Act as the subject matter expert for the data structure and usage.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Excel in the design, creation, and management of very large datasets.
Detailed knowledge of cloud-based data warehouses, architecture, infrastructure components, ETL and reporting/analytic tools and environments.
Skilled with writing, tuning, and troubleshooting SQL queries
Experience with Big Data technologies such as Hive/Spark, AWS EMR, AWS Glue, AWS Lambda , Kinesis, Redshift.
Proficiency in one of the scripting languages - python, ruby, java or similar.
Excellent understanding of software development life cycle and/or agile development environment with emphasis on BI practices.
Strong organizational skills and planning prowess, with excellent attention to detail.
Preferred Qualifications
3-5 years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets
Experience with Massively Parallel Processing (MPP) databases - Redshift, Teradata etc
Experience with distributed systems and NoSQL databases
Experience leading medium to large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Excellent communication skills and able to work with business owners to develop and define key business questions and to build data sets that answer those questions
Experience providing technical leadership and mentorship of engineers and scientists on best practices in the data engineering space
Be self-driven and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2014860"
303,Remote Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, CNA Search, is seeking the following. Apply via Dice today!

CNA Search is seeking a talented Remote Data Engineer About the Job The Data Engineering team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion. What you'll do Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structuredunstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQLNoSQL. Build complex workflows and orchestrate data dependencies. Monitor and support data pipelines to honor internal and external SLArsquos. Work within standard engineering practices (i.e. SCRUM, unitintegration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients. Closely work with product owners analysts to understand business and functional requirements and contribute to the design and prioritization discussions. Working with a team of engineers where mentorship is valued. Ability to learn and adapt to continually evolving technologies in the big data ecosystem. What you'll need 5+ years of experience programming in Java. 2+ years of experience in Big Data technologies. Experience in any of big data technologies MapReduce, Spark, HBase, Proficient in SQL and experience with RDBMSNoSQL databases. Experience working with ClouderaHortonworksEMR distribution in AWS. Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work. Team-player with strong collaboration and communication skills, who is able to respond positively to feedback. Bachelor degree (or Master) in Computer Science or related engineering field"
304,Data Engineer (Financial experience Required),Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, NuWare Tech Corp, is seeking the following. Apply via Dice today!
Strong SQL experience -ETL -Data Analysis"
305,Data Engineer,Monstera Global,United States,"Posted by
Prashant Tyagi
Senior Recruiter
Send InMail
Data Engineer
Full Time, Remote US only
Experience: 4+ years

Skills: Python, Big Data, Data Pipeline, Postgres, PostGIS, Cloud computing, Google Cloud Platform, Geospatial Data

Qualifications:
A readiness to learn and broaden your horizons and is someone that takes initiative and is an active person in coming up with and delivering solutions.
An interest, and better yet experience, in the domains that the data relates to
Proficiency with Python, creating data pipelines and transformations, working with relational and spatial databases such as Postgres and PostGIS. Bonus if you have had experience with ETL.
Experience with Cloud computing, especially Google Cloud Platform, and familiarity with Docker, Kubernetes, Cloud Storage, Dataflow, Cloud Data Fusion, Dataproc, or related Data Lake technologies
Familiarity with statistical concepts and practices such as regression modeling, discrete choice, goodness-of-fit evaluation, clustering, and sampling. Understanding CNN, RNN, decision tree and other neural net methodologies a plus
A BS or higher degree in Computer Science or data analytics related field, or commensurate experience
2 or more years of industry and programming experience
BONUS: Previous experience with geospatial data and experience with data science tools such as Jupyter Notebooks, Pandas, GeoPandas, NumPy, SciPy, scikit-learn, Jax, and TensorFlow"
306,Data Engineer - Ad Platform,Amazon,"New York, NY","Job Summary

DESCRIPTION

The Amazon Advertising Platform team provides tools and services to create, deliver, and manage ads on Amazon.com and other Amazon properties. We innovate on a daily basis to meet these needs with new tools, products, and services designed to expand the capacity to deliver and maximize value to our customers and Amazon.

Amazon's Advertising Supply & Monetization team is looking for an experienced Data Engineer (DE) to create services that drive Display Advertising Monetization. The Advertising Supply & Monetization team has ownership of advertising placements (supply) that appear across Amazon’s owned and operated platforms and effective monetization of this supply. The team partners with other advertising technology teams to deliver an engaging and relevant ad experience to millions of customers each day. This team is tasked with maximizing the long-term value that Amazon creates across its Display Advertising Business.

You will help create products that utilize Big Data across supply, demand, inventory management, yield, and pricing to optimize value for Customers, Advertisers, and Amazon. You will own and optimize data pipelines that consume big data sources required to generate unique insights. You will own the technical vision and direction for building data lakes and integrations that drive advanced analytics and insight products, at scale. You will focus on developing operationally excellent systems that drive business profitability. You will leverage and apply interesting new machine learning technologies that will track, measure and optimize advertising performance. You will be responsible for our existing data pipelines and data lakes, and drive product and engineering partners to build and improve services based on the insights you determine and derive.

You will invent new ways to explore our data by collaborating with other data engineers, data scientists and product managers across teams, and be responsible for developing and deploying data infrastructure and reporting solutions to help our leadership make impactful business decisions. You will own the development and support of several key business metrics while continuing to raise the bar for creating scalable, efficient, and automated processes for large-scale data analyses. You will have regular exposure to senior leadership in this role.

This position requires excellent analytical abilities, deep knowledge of business intelligence solutions and data engineering practices, and the ability to collaborate with various teams across Amazon. The successful candidate will be a self-starter comfortable with ambiguity, capable of working in a fast-paced environment, possess a strong attention to detail, and able to collaborate with customers to understand and transform business problems into requirements and deliverables.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
Experience in machine learning/predictive modeling
Experience with AWS technologies and big data solutions (Infrastructure, Redshift, EMR/Hadoop/Spark, Athena, Glue, Quicksight etc.)
Experience with reporting tools like Tableau, Cognos, Business Objects etc.
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1861412"
307,Data Engineer I,Mid-Atlantic Permanente Medical Group,"Rockville, MD","Mid-Atlantic Permanente Medical Group

MAPMG is an Equal opportunity Employer. To learn more, view Equal Employment Opportunity is the Law in English or Equal Employment Opportunity is the Law in Spanish. We also participate in the E-Verify program, as required by law.

MAPMG is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans during our job application procedures. If you need assistance or a special accommodation, you may contact us at Careersatmapmg@kp.org



Job Category: Finance
Requisition Number: DATAE005185
Posted: 2022-03-10
Full-Time
Regional Office, 18801, 2101 East Jefferson St., Rockville, MD, 20852, USA


Who We Are:

The Mid-Atlantic Permanente Medical Group, (MAPMG) is one of the nation's premier multispecialty medical groups and is the largest integrated medical group in Maryland, Virginia and the District of Columbia. Founded in 1980, the medical group has more than 1,700 Permanente physicians spanning more than 50 subspecialties. Together, we serve approximately 800,000 Kaiser Permanente members in Maryland, Virginia, and the District of Columbia at 34 area medical centers, plus several community hospitals and skilled nursing facilities.

Job Summary:

We are seeking a Data Engineer I to join our Finance team at Mid-Atlantic Permanente Medical Group (MAPMG). The Data Engineer I will be responsible for using various programming languages to develop applications and automate processes in service of the analytics teams. The Data Engineer will also act as a technical resource for identifying areas for workflow improvement and replacement of existing systems.

Responsibilities:
Develop, deploy, and maintain sites and web services within the context of a microservice architecture, using Docker or other contain runtimes
Perform REST API calls to a variety of backends to perform automated administration (including access management, data recovery and restoration, and bulk updates) within web tools
Enable efficient data storage by designing and developing logical schemas and table structures, deploying and maintaining ad-hoc database systems, and performing database administration
Facilitate ETL (extract, transform and load) processes between different systems and database technologies, typically using Python and SQL
Develop and design reusable frameworks and libraries in Python that enable other team members to produce new functionality with minimal coding effort and a reduction in complexity
Troubleshoot production system failures
Clearly and regularly communicate with management and technical support colleagues
Perform other duties as required

Qualifications:
BS/BA required in computer science, engineering, mathematics, statistics, or related field
Prior experience in web and software development in Python or similar languages
Demonstrated knowledge of web technologies, such as HTML, CSS, and JavaScript
Demonstrated experience in end-to-end solution design required
Comfortable with working remotely in a CLI-based Linux server environment
Ability to communicate complex technical concepts to a non-technical audience
Expert in managing data structures, content and interpretation of specific data elements
Detail-oriented, naturally curious, and passionate about solving business problems through high-quality solutions
Demonstrated flexibility in meeting the needs of a challenging, fast-paced business environment
Adaptable and flexible due to varying deadlines and project goals
Rapid learner
Able to work effectively in complex, political and ambiguous situations
Ability to work independently and multi-task effectively
Demonstrated understanding of projects from the perspective of both client and business
Flexible and willing to accept a change in priorities as necessary
Strong attention to detail

Competitive Benefits:
Competitive compensation package
Comprehensive benefits including 100% employer-funded medical and dental insurance premiums for employees and families
Great work/life balance
Generous paid time off
Maternity and parental leave
Pension plan and 401(k) retirement plan
Life insurance, and short-term disability and long-term disability coverage
Education reimbursement

Equity, Inclusion, and Diversity:

MAPMG continuously works to identify and mitigate healthcare inequities, and that starts with providing an inclusive, supportive environment for our physicians. We encourage applicants of any race, color, religion, sex, sexual orientation, gender identity, or national origin who value diversity and will commit to practicing culturally competent healthcare.

External hires must pass a background check/drug screen.

We are proud to be an equal opportunity/affirmative action employer.

We value our diversity and E/O/E M/F/D/V.

Executive Order 11246 requires affirmative action and prohibits federal contractors from discriminating on the basis of race, color, religion, sex, sexual orientation, gender identity, or national origin. Contractors also are prohibited from discriminating against applicants or employees because they inquire about, discuss, or disclose their compensation or that of others, subject to certain limitations."
308,Jr. Data engineer,Gainwell Technologies,New York City Metropolitan Area,"Job description:

Job Description
Essential Job Functions

• Responsible for discussing data extraction needs with each individual account and ensuring that data extracted meets the needs of the GIO application in order to process it
• Responsible for validating that data ingested by connectors created by Java/Kafka Developers is ingested correctly and makes sense from a business perspective
• Familiarity with SQL is important in order to query GIO(T-K) to ensure data quality is high. Experience with design and documentation of ETL pipelines preferred.
• Works with the account teams to ensure that after initial data is ingested that data being displayed in the application appears appropriate and explores any anomalies that may exist


Requirements/Qualifications:
• Design and document data pipelines/transformations required for each unique account connector
• An ability to learn and understand the GIO(TK) data schema and how data is processed by TK in order to ensure that data designs developed work well with product.
• An ability to interact with and understand data extraction pipelines created by Gainwell account teams to suggest changes to them to meet product requirement needs."
309,Data Streaming Engineer,Robert Half,"New York, NY","Description

Robert Half is seeking a Senior Software Engineer for a team focused on the development of software used in Software as a Service (SaaS) applications. This person analyzes, designs, programs, debugs, and modifies software enhancements. Using common programming languages, the software engineer will write code, complete programming, and perform testing and debugging of applications. Completes documentation and procedures for installation and maintenance. May interact with users to define systems requirements and/or necessary modifications. Experience in object-oriented design, coding, performance tuning, and unit testing. Knowledge of logical data modeling. The role will be about 75% development and 25% system management and support. This person must have strong communication and team building skills and be able to participate in software design discussions and guide the architecture of the solution based on good object oriented design concepts, and both industry standard patterns and Microsoft best practices.

Key responsibilities
According to business requirements, develops, modifies, and implements Software as a Service (SaaS) hosted applications.
Comfortable creating and managing web services.
Experience in Relational Database.
Solid understanding of scripting library tools.
Knowledgeable of developing software utilizing open-source languages.
Translates business requirements to conceptual solution architecture and high-level project estimates.
Requirements
Logical data modeling design and implementation
Completes test review and analysis, test witnessing, and certification of software
Skilled at time management
Correct application and knowledge of standards and techniques used during the Software Development Life Cycle process (SDLC, security)
Research, extraction and entry of complex data
Completes proper unit testing and software code written (including automated unit testing)
Reviews technical requirements, ensuring compliance with business requirements
Ability to work on multiple tasks simultaneously with various groups both internally and externally
Object oriented design, coding, performance tuning, and unit testing
Complete such other duties as may be assigned by management
Participates in peer code reviews
Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
310,Data Engineer,Dice,"Berkeley Heights, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Axis Group, LLC, is seeking the following. Apply via Dice today!

Title Data Engineer Location Berkeley Heights, NJ Atlanta, GA, 100 Remote Compensation Full-Time Competitive Salary, Health Benefits, 401K Our Company Axis Group is the Analytics Enablement company. With more than twenty years of experience providing data and analytics consulting services and innovative solutions to enterprises and government organizations, we unleash analytic potential throughout the data lifecycle from data management, governance, visual design and platform migration to digital transformation, predictive analytics, automation and beyond. Axis Group combines business acumen, leadership, and industry-specific experience with technical expertise to tackle tough data problems to enable better business outcomes. In addition, we partner with leading data and analytics technologies to deliver the right solutions at the right time. We have experts in Qlik, ThoughtSpot, Snowflake, Power BI, Dataiku, VoiceBase and StreamSets, among others. We have regional offices in Atlanta, GA, Berkeley Heights, NJ and Plano, TX. Data Engineer Position We are looking for engineers who share our passion for modern data platforms. We deliver solutions using a diverse, modern stack and use a collaborative, team-based approach. Our consultants think critically about business, process, and data, and thrive in ambiguity. Responsibilities Develop and automate large scale, high-performance data solutions (batch and streaming) to enable Axis Group clients to successfully execute their data strategies Build scalable data pipelines leveraging best-in-class tools (e.g., StreamSets, Spark) that integrate with high-level orchestration frameworks such as Prefect and Apache Airflow Design and implement multi-tier distributed data platforms in cloud, on-prem and hybrid environments (AWS, Azure, GCP) using infrastructure as code and automation tools (e.g., Terraform, Ansible) as well as container technologies (Docker, Kubernetes, Helm) Develop complex processing logic, custom services and reusable libraries using Python or Java Incorporate best practices from DataOps, MLOps, DevOps and Agile software development into these solutions, including builddeployment automation (CICD), testing automation, and operational monitoringalerting Contribute to shared Data Engineering standards and tooling to establish best practices, to improve productivity, and to increase the quality and repeatability of our solutions Support a wide range of dataanalytics use cases across many industries, including cloud data warehousing, streaming, and machine learning Qualifications 4+ years of experience as a Data Engineer or Software Engineer Bachelor s degree in Computer Science or a related field, or equivalent experience Programming expertise in at least one of the following languages Python, Java, Scala Familiar with the full software development lifecycle including unit testing, integration testing, etc. Production deployment experience to at least one of the following platforms Snowflake, Databricks, Hadoop, AWS, Azure, GCP Experience with data ingestion tools (e.g., StreamSets, Apache NiFi, Azure Data Factory), data processing frameworks (e.g., Spark, Beam) and distributed storage (e.g., Kafka, HDFS, S3ADLSGCS, MongoDB, Cassandra, Elastic Search) strongly preferred Strong working knowledge of SQL and the ability to write, debug and optimize queries Excellent communication skills, both written and verbal This is a full-time position. Some overnight, onsite travel to clients is occasionally required. Axis Group offers a competitive compensation package that includes salary, bonuses, as well as full benefit package, including 401K. No third parties. Axis Group offers competitive salaries, excellent benefits, including, medical, dental and 401K and a positive work environment. Axis Group is an Equal OpportunityAffirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age or, any other characteristic protected by applicable law. All candidates must be legally authorized to work in the United States. Axis Group will not consider Visa Sponsorship for this position."
311,Data Engineer [Remote],Braintrust,"Boston, MA","ABOUT US:

Braintrust is the only network that gives in-demand talent all the freedom of freelance with all the benefits, community and stability of a full-time role. As the first decentralized talent network, our revolutionary Web3 model ensures the community that relies on Braintrust to find work are the same people who own and build it through the blockchain token, BTRST. So unlike other marketplaces that take 20% to 50% of talent earnings, Braintrust allows talent to keep 100% of earnings and to vote on key changes to improve the network. Braintrust is working to change the way freelance works – for good.
JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION: Remote (TimeZone: EST | Partial overlap)
HOURLY RANGE: Our client is looking to pay $30-$50 /hr
ESTIMATED DURATION: 40h/week - long-term, ongoing project

THE OPPORTUNITY
Web scraping
S3/AWS
Python and SQL (for dashboards)
Snowflake

Apply Now!

ABOUT THE HIRING PROCESS:

Qualified candidates will be invited to do a screening interview with the Braintrust staff. We will answer your questions about the project, and our platform. If we determine it is the right fit for both parties, we'll invite you to join the platform and create a profile to apply directly for this project.

C2C Candidates: This role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.

Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
312,Data Engineer,Apple,"Austin, TX","Summary

Imagine what you could do here!


We’re a diverse collective of thinkers and doers, continuously reimagining our programs, practices, and products to help people do what they love in new ways. That innovation is inspired by a shared commitment to great work — and to each other. Because learning from the people here means we’re learning from the best.


As a Data Engineer on the Retail Store Analytics team, you will create robust and scalable data to help Apple Retail make strategic decisions.


Key Qualifications

5+ years of professional experience with Enterprise Data Warehousing systems, pipelines, data processing, and reporting


Fluency in SQL and Python, including schema design and data modeling


Experience working with Teradata, Snowflake, and pipeline tools like Airflow and dbt


Strong experience in building robust and scalable data pipelines


Some experience with CI/CD and containerization tools (e.g. Jenkins, Docker, Kubernetes)


Comfortable picking up, embracing, and driving adoption of new technologies and languages


Ability to initiate, refine, and complete projects with minimal guidance


Ability to think critically and collaborate cross-functionally with other data engineering, data science and analytics partners distilling abstract requirements into clear data products


Experience designing data ingestion processes and working with unstructured data a plus


Description

Design, create, refine, and maintain complex data pipelines used for modeling, analysis, and reporting. Collaborate with other data scientists, analysts, and engineers to build full-service data solutions. Work with multi-functional business partners and vendors to acquire and transform raw data sources. Support data access requests through various sharing platforms. Perform automated quality checks and maintain detailed documentation. Communicate and educate consumers on access, usage patterns and changes to maintained datasets and pipelines, managing transparency in logic definitions. Develop a deep understanding of our retail customer base and purchase choices and contribute to the development of tools to improve business efficiency and productivity. Bring best practices and technical leadership in supporting production data analytics products.


Education & Experience

Bachelor’s degree, preferably in related quantitative field


Role Number: 200368619"
313,Data Engineer,Bayer,"Whippany, NJ","At Bayer we’re visionaries, driven to solve the world’s toughest challenges and striving for a world where ,Health for all, Hunger for none’ is no longer a dream, but a real possibility. We’re doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining ‘impossible’. There are so many reasons to join us. If you’re hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there’s only one choice.

Data Engineer

Your Tasks And Responsibilities

The primary responsibilities of this role, Data Engineer, are to:
Participate in and occasionally lead design sessions with Enterprise Data Stewards, Data Engineering teams, Data Scientists, Product Managers, business and IT stakeholders, that result in tangible documentation for data processing, data hierarchies, data storage and solution delivery;
Understand business capability needs and processes as they relate to IT solutions through partnering with Product Managers and business and functional IT stakeholders and apply this knowledge to influence the decision-making process driving business goals;
Support evaluation of implementing new technologies, like Reltio, Snowflake, LiveRamp or AWS Native, with established products such as Veeva Network, including performing POCs and presenting results to others, with a goal of providing optimal technical recommendations;
Implement data solutions according to design documentation using a variety of tools and independent from programming languages;
Collaborate with Product Owners, other engineers and data stewards within the team and across data, technical platforms and product teams on executing and aligning roadmaps, delivery dates and integration efforts;
Provide ontologies and terminologies for Pharma core areas;
Hold third party data providers like IQVIA, Veeva Open Data, DRG, IRA, ProMetics, Lash etc accountable for their data deliverables before consumption into Bayer systems and processes;
Guide and orient small teams of external/internal stewards to shepherd the day to day tasks of pre-mastering, review candidate records business critical data identification, KPIs the measure quality and sharpen the Standard Operating Procedures (SOPs) for stewardship that manage data change request, while building alignment on the data strategy vision.
Relocation as well as visa sponsorship may be available.

Who You Are

Your success will be driven by your demonstration of our LIFE values. More specifically related to this position, Bayer seeks an incumbent who possesses the following:

Required Qualifications:
Bachelor’s degree with at least six years of applicable business experience;
Experience in participating in the build and support of MDM, Data Platforms and design principles;
Experience leading data modeling and architecture design efforts, including designing both logical and physical models for datasets;
Proficiency in working with relational databases such as Postgres, MySQL, Oracle, etc.;
Experience modeling large datasets in distributed databases such as Snowflake;
At least two years of experience with Veeva Network;
Experience providing reliable estimates for large scale projects;
Experience contributing to open source projects;
Strong interpersonal skills and desire to work in a highly collaborative environment;
Familiarity with the relevant industry trends.
Preferred Qualifications:
Pharmaceuticals, life sciences, bioinformatics or related disciplines;
Cloud-native solutions, Veeva Network and MDM technologies, such as Reltio;
Knowledge of machine learning or other data science practices.
YOUR APPLICATION

Bayer offers a wide variety of competitive compensation and benefits programs. If you meet the requirements of this unique opportunity, and want to impact our mission Science for a better life, we encourage you to apply now. Be part of something bigger. Be you. Be Bayer.

To all recruitment agencies: Bayer does not accept unsolicited third party resumes.

Bayer is an Equal Opportunity Employer/Disabled/Veterans

Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below.

IMPORTANT NOTE for POTENTIAL US CANDIDATES: Bayer active employees are required to disclose their vaccination status and if fully vaccinated, provide proof of vaccination status before participating in approved activities for fully vaccinated employees. Bayer defines fully vaccinated in alignment with CDC which is two weeks after completing the two-dose vaccine regimen or two weeks after completing the one-dose regimen. Additionally, if candidates are invited on site as visitors for in person interviews, they must provide proof of being vaccinated and comply with all safety protocols.

Location: United States : New Jersey : Whippany

Division: Enabling Functions

Reference Code: 559572

Contact Us

Email: hrop_usa@bayer.com"
314,Data Engineer (Snowflake),Major League Soccer,"New York, NY","As a Data Engineer, you will be part of our Data & Engineering team and work with a passionate data team to take MLS fan data to the next level. You will bring your expertise in data management and technical expertise with tools like Snowflake and Salesforce Marketing Cloud to support the League’s overall data strategy goals.

Responsibilities
Understand MLS goals and requirements and how that impacts the Fan enterprise data model and fan customer record
Help to define and implement the League data management process including establishing data quality metrics and benchmarks & tools, improving the fan golden record process data onboarding and data transformation rules
Working with League and Club marketing, data, technology, and other key stakeholders to support day-to-day business requirements utilizing MLS data warehouse and Salesforce tools like Marketing Cloud and DMP

Qualifications
Bachelor’s degree in Computer Science, Engineering, Data Science or equivalent experience
5+ years hand-on-cloud-based software application development
Snowflake SnowPro Core certification required, data engineer certification is desirable
Strong SQL Skills and Data Modeling skills
Experience setting up new data sources and migrating data as well as establishing or fine tuning ETL processes in Snowflake
Track record of building & maintaining products involving complex data systems & APIs as well as recommending practical solutions
Ability to work effectively in a fast paced, team environment
Strong communication and interpersonal skills, with the ability to effectively communicate, both verbally and in writing
Demonstrated decision-making and problem-solving skills
Detail-oriented with the ability to multi-task and meet deadlines with minimal supervision
High-level of commitment to a quality work product and organizational ethics, integrity and compliance
Ability to work non-traditional hours, including evenings, weekends, and holidays as needed

Desired Skills
Prior experience working with Data Warehouse environments
Prior experience with API Integrations between SFMC and external services using SFMC SDK, Mulesoft, Apache Kafka or equivalent
Knowledge of the sport of soccer"
315,Senior Data Engineer (contract),Capgemini,"Jersey City, NJ","Duration: 8+ months

Job Description
Experience with Python is preferred, but not required.
Deep Experience and understanding of SQL (MySQL, Postgres, Redshift).
Experience with ETL (Airflow, Boomi) tools is a plus.
Experience with frontend BI Tools is a plus (such as Periscope, Sisense, Tableau).
Experience with AWS.
A minimum of 4 years of software engineering experience is required.
Work on a small team of passionate data scientists and data engineers.
Manage the day to day operations of running our Data Warehouse.
B-uild data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.
Assist in building scalable data models to support reporting and tracking of key business and product metrics.
Build, analyze and manage reports and dashboards for business stakeholders.
Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.
Experience with running and managing a data warehouse.
Demonstrated understanding of data quality principles and practices.
Experience with Business Intelligence technologies.
Comfortable working in a fast-paced, changing environment.
Ability to communicate and collaborate effectively with business users and other developers.
Creative thinker who can quickly develop innovative ideas across a wide variety of business functions.
The Capgemini Freelancer Gateway is enabled by a cutting-edge software platform that leads the contingent labor world for technology innovation. The software platform leverages Machine Learning and Artificial Intelligence to make sure the right people end up in the right job.

A global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50 year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion."
316,Data Engineer,Intel Corporation,"Phoenix, AZ","Job Description

Looking for an opportunity to help Intel meet customer commitments and transform our planning data to improve our leaders ability to better decisions for Intel? The search is over. Join our Data Solutions and Analytics team. We are searching for two talented Data Analysts/Architects (per the industry, a Data Engineer) to help us on our journey. Our incredibly talented team has the goal of developing and deploying an integrated and governed data, technology and infrastructure in support of Corporate Planning Group's planning transformation strategy to reduce our monthly reset planning process throughput time by half. Data transformation will play an essential and critical role.

Responsibilities May Include But Not Be Limited To
Provide expertise on database management and or design to drive integration of business data, functions and systems.
Establish standards, guidelines, procedures and other infrastructure necessary to support the objective of process creation, re-engineering and integration. This could be data models, business glossary, core data domains, data cleansing and general data stewardship.
Deliver work product such as designing comprehensive, yet concise datasets; partner with IT to drive goal-state data lineages and low fidelity data prototypes
Resolve performance problems for data foundation, modeling and or consumption typically data extracts, transformations of data, and or loads not meeting stakeholder expectations.
Demonstrating leadership for data quality.
Plans and schedules daily tasks, uses judgment on a variety of problems requiring deviation from standard practices.
Gathering requirements by performing stakeholder analyses, understanding key gaps, time objectives, user stories with a focus on business process integration.
Communicating with project team and subject matter experts or key partners to successfully collaborate cross-functionally, embodying a One Intel approach.
Fully leading or supporting Test Lead in UAT Execution for newly developed or enhanced applications through partnership with IT systems analyst and subject matter experts.
Supporting Deployment Lead through go-live by adjusting business process documentation based on go-live changes, supporting post go-live stabilization, and training users on escalation processes post go-live.
In addition to the qualifications a successful candidate will demonstrate:
Strong problem-solving skills.
Natural curiosity and desire to understand and predict problems rooted in the data ecosystem.
Comfortable with ambiguity in the context of work planning and establishing commitments with stakeholders.
Attention to detail and a passion for quality data.
Communication effectiveness with the ability to simplify explanations of complex concepts.
Skills to context shift and move in and out of team collaboration and individual work settings.
Are you a non-commissioned employee considering this role? If so, please review the Grade Reclassification Q&A and the How Grade Reclassifications Work video for more information on how compensation changes when moving between these roles. Additional information can be found here on our Internal Transfer’s page.

Qualifications

You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.

This Position is not eligible for Intel immigration sponsorship.

Minimum Qualifications
Bachelor's Degree in Supply Chain Management, Industrial Engineering, Logistics, or related field of study
5+ years of relevant experience in:
Ingesting and analyzing data at Intel for business intelligence purposes by constructing queries and joining disparate data for prototyping BI-related solutions
Experience with Extract, Transform, and Load (ETL) solutions
Knowledge of data management and governance philosophies
1+ year experience understanding data quality philosophies and/or building, managing data quality solutions
1+ year experience on supply chain data such as master data (Item, Bom) or transactional data (Supply, Demand)
Preferred Qualifications
Supply chain, Manufacturing, Divisional/BUs and/or SMG prior experience
Proficient knowledge of demand planning, supply planning, procurement, manufacturing processes and related Intel applications
Proficient knowledge of Product Master Data, Bill of Materials (BOM), Intel's reporting systems and Intel's financial costing systems (e.g. ICOST)
Experience querying Hana, EDW, or a similar data warehouse
Denodo, MSFT SSAS Tabular, and or MSFT SSIS
Inside this Business Group

Intel's Sales and Marketing (SMG) organization works with global customers and partners to solve critical business problems with Intel based technology solutions. SMG works across business units to amplify the customer voice and deliver solutions that accelerate their business. We work across numerous industries, including retail, enterprise and government, cloud services and healthcare as examples. The operations team focuses on forecasting, driving alignment with factory production and delivering efficiency tools and our marketing capability drives demand and localized marketing in locations around the globe. Our sales force navigates a complex partner and customer ecosystem while shaping product roadmaps, driving value for our customers, and collaborating to harness emerging technology trends to deliver comprehensive solutions.

Other Locations

US, California, Folsom;US, California, Santa Clara;US, Oregon, Hillsboro

Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.

Posting Statement

All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.

Work Model for this Role

This role will be eligible for our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site."
317,Jr-Mid Level Data Engineer *Fully remote*,Robert Half,"Alexandria, VA","Description
For immediate consideration, please message Ali Scott from Robert Half with your updated resume***
Functional Role: Data Engineer

Location: Anywhere in US or Canada *must be eligible to work in those countries*

Salary Range: $115,000-130,000 (DOE)

Technical Skills: Scala or Python, Postgres, SQL

Our client in the higher education industry is seeking two junior-mid level Data Engineers to join their growing team. This person either has experience with Scala or is willing to learn it, maintaining and developing code, working alongside the Data Architects building out data pipelines. If you have what it takes, then look no further!

Requirements

Top Requirements:
2-3 years experience
Scala or someone ready to learn it
Maintaining existing code
This role will be driven by the data architect building out the practice and building out data pipelines
SQL, Postgres, Python/Scala, eager to learn
For immediate consideration, please message Ali Scott from Robert Half with your updated resume***
Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
318,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Decision Minds, is seeking the following. Apply via Dice today!

Data Engineer 5-7 years of data analytics exp 2+ years of python data engineering exp with Spark"
319,"Data Engineer, Amazon Publisher Services, Publisher Data Lake",Amazon,"New York, NY","Job Summary

DESCRIPTION

Want to work on one of Amazon’s fastest growing advertising businesses? Amazon Publisher Services (APS) helps digital publishers around the world build and grow thriving businesses. We provide services and advanced technologies to web, mobile app and advanced TV publishers of all sizes, including many of comScore’s global top 100, to help them monetize their content with demand from multiple programmatic buyers. Our server-side header bidding solutions are fast and reliable across devices, handling billions of queries per day, delivering ads in milliseconds. The result is more profitable advertising for publishers and more relevant ads for customers.

We're building turn-key, fully integrated data solutions for our publisher customers that process, analyze and derive actionable insights on data from various 1P and 3P data sources in a privacy-safe environment. We process terabytes of data every day and use a variety of technology like Hive, Spark, Airflow, SQL, and AWS Big Data technologies like EMR, Redshift, S3, Glue, Kinesis and Lambda.

We are seeking a Data Engineer to build the tools and infrastructure necessary to process data in real-time for our customers. You'll also be responsible for developing the data architecture components that scale with ever-evolving data needs.

Key job responsibilities
Design, implement and support an analytical data infrastructure
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using AWS Big Data technologies
Must possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Experience working with AWS Big Data technologies including: EMR, Redshift, S3, Glue, Kinesis and Lambda.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
Proven success in communicating with users, other technical teams, and management to collect requirements, describe data modeling decisions and data engineering strategy.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1931611"
320,Jr. Data Engineer - TAP,Chubb,"Jersey City, NJ","By Joining The Technology Associate Program, You Will Be Offered The Following

Chubb is looking for individuals passionate about technology to join our Technology Associate Program (TAP). The Technology Associate Program is a two-year program designed for early career IT professionals to help them develop expertise and technical acumen as a technologist.
On-the-job and educational technical training (offered through Pluralsight) to enhance skills within your chosen discipline
Business acumen and professional development training specific to IT and an understanding of our broader industry
Networking opportunities with IT and Business leaders and TAP associate peers
Hackathons
Position Summary

The Data Engineer will work with the business to understanding data requirements and will become a data platform expert in designing and building data solutions focused on Cloud-based Big Data ecosystems. You will work closely with other data science teams and take ownership of your projects and deliver high-quality data-driven advanced analytics applications. You will solve diverse business problems by utilizing a variety of different tools, strategies, algorithms and programming languages.

What about you?
You are highly collaborative, creative, and intellectually curious individual who is passionate about data engineering and supporting cutting-edge computing capabilities.
You are able work well, both individually and within a team.
You are adaptable and able to overcome technical challenges.
You are a self-starter and motivated to learn and succeed.
You are data driven and are able to identify and solution problems as they arise.
Responsibilities
Collaborate and work with global data management stakeholders to identify requirements for complex business problems that may be loosely defined.
Work with the business, applications owners, solutions architects, and with technical architects to understand the implications of respective data architectures to maximize the value of information across the organization.
Build the enterprise conceptual and logical data models for analytics, operational and data mart structures in accordance to industry best practices models.
Identify, evaluate and implement leading edge data management frameworks required to address complex large-scale data challenges.
Work within multi-functional agile teams with end-to-end responsibility for product development and delivery.
Provide architectural support by building proof of concepts & prototypes.
Desired Qualifications
Working knowledge of Excel, PowerPoint, and Word is required.
Energetic, able to build and sustain long-term relationships across a multitude of stakeholders in a fast paced, multi-national work environment.
Strong time management and organizational skills.
Possess strong verbal and written communication skills and ability to present, persuade and influence peers.
Internship or Job experience in software development
Bachelor's degree in Computer Science, Data Science, or related field with GPA of 3.0+ required.
Excellent data analysis skills.
Experience in performing analysis and design for data management and data driven projects.
Exposure or knowledge of tools such as T-SQL on Spark SQL, ANSI SQL etc.
Experience or exposure Python, Jupyter Notebook, etc.
Experienced in programming languages such as python, scala or Java.
Familiarity with data science and analytic tool sets such as jupyter hub.
Bonus Skills
Exposure or experience with Cloud Platforms
Experience in designing and leading the conceptual, logical and physical design for distributed databases.
Experience with operating system command languages such as bash or ksh
Experience with development tools such as git and integrated development environments
Understanding of the SAFe Agile development methodology
EEO Statement

At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.

344764"
321,Data Engineer,Underdog.io,"New York, United States","We're looking for a Data Engineer to join a company in the Underdog.io network.

The Underdog.io network is a curated group of some of the fastest growing startups and tech companies in the country. We actively turn away more than 50% of companies that attempt to join.

We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active.

Our companies look for Data Engineers proficient in Python, Java, SQL, Hadoop, C++ and more. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many. Many of our companies are looking for mid-to-senior level talent, both individual contributors and managers.

To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free.

If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer.

Apply today!

Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.

C++,Python (Programming Language),Java,Data Products,Pipelines,Hadoop,MapReduce,SQL,Data Warehousing,Data Architecture"
322,"Senior Data Engineer I ( Remote - NV, CA, FL, TX )",MGM Resorts International,"Nevada, United States","VACCINATION REQUIREMENT: MGM Resorts now requires that all new hires who do not exclusively work from home to provide proof of vaccination against COVID-19 before beginning work effective August 30, 2021 (excluding hourly employees for properties located in Michigan, Mississippi, New Jersey or Ohio).

Location:

US, Nevada

Become one of the stars behind The SHOW and become part of the world’s most powerful entertainment brands. Our Company has one exciting mission: To entertain the human race.

The Senior Data Engineer I is responsible to lead, manage and execute the designing, development and operationalization of data integration, engineering and data platform services to support enterprise data programs. This will include developing production- grade code through vision, design, development, test, deployment and operational support. All duties are to be performed in accordance with departmental and MGM Resorts policies, practices and procedures.

Principal Duties And Responsibilities
Lead, develop and execute production-grade data integration and engineering code end-to-end through vision, definition, development, deployment and sustainment
Partner with Product Management to ensure data design, engineering and implementation is in accordance with design intention and meets business needs
Technical design documentation in adherence to business, source system, target system, and solution architecture requirements
Work with technology leadership to identify, implement new technologies and techniques that can improve our overall platform performance and ultimate customer experience
Mentor junior engineer and lead analysis of product tradeoffs and deliver simple and intuitive products that just work
Develop and maintain relationships with diverse teams spanning all organizational levels to effectively articulate technology solution and gain trust and buy-in
Perform other job-related duties as assigned
Minimum Requirements
Bachelor’s degree in Computer Science, Engineering or a related technical field; or equivalent experience
Five (5) + years of experience on delivering data integration, data engineering service and data platform solutions
Preferred
Previous experience with data warehouse integration and data engineering using real-time messaging, batch ETL/ELT, etc. in data management discipline
Previous working experience using cloud environments and various cloud services
Previous experience using various scripting tools and programing languages and experience working in a DevOps culture
Previous experience working in the hospitality industry
Knowledge, Skills, And Abilities
Strong understanding of best practices of various data integration, data engineering and platform engineering mechanisms
Strong Understanding of various choices of data platforms, ETL tools, Cloud IAAS/PAAS, scripting tools
Understanding of data architecture and data modeling
Understanding of data governance, data quality, data security and compliance
Excellent customer service skills
Interpersonal skills to effectively communicate with all business contacts
Ability to effectively communicate in English, in both oral and written forms"
323,Data Engineer,Apex Systems,"Skillman, NJ","Apex has an opportunity for a Data Engineer for a leading medical devices, pharmaceuticals, and consumer packaged goods client.

One hundred percent remote.

If you interested in discussing the position further, please send an MS Word version of your resume to Ronnie Miller, romiller@apexsystems.com

Need experience with Python (data side – Pandas or similar data framework experience)

Should have experience with cloud (Microsoft Azure or GCP)

Must Have:

Python experience with Cloud experience (Azure or GCP)

Project need - They support the cloud data layer and data operations across the company - Multi-cloud environment or Digital Marketing, Google cloud (Big Query, Google Marketing cloud, etc.)

You will be working to upgrade and fix data architecture with best practices. Will be about 10-15% hands-on data engineering (Building out POCs and helping with implementation) but primarily will be focused on building out architecture design and plans and then directing data engineering teams on implementation portion. Need to be able to be vocal and push the team on architecture best practices. Will work across the cloud environments and teams supported.

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please"
324,"Data Engineer, Spark",Deloitte,"Hattiesburg, MS","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
325,Data Engineer,AArete,"New York, NY","Description

Data Engineer

AArete is one of a kind when it comes to consulting firm culture.

You will be at the forefront of a development process to build a high quality product using AI and machine learning on the cloud. In this role, you will have the opportunity to create a world class product to help reduce costs for our healthcare payers and ensure claims are paid accurately. You will have an opportunity to foster a culture of collaboration and technical expertise with your team and AArete leadership as a subject matter expert.

Why AArete? We are a fast-growing strategy, operations and technology consulting firm – due to this the bar is set high at AArete. AArete is guided by our deeply embedded principles: Excellence, Passion, Loyalty to Clients, Stewardship, Family, Community, Sustainability, and Inclusion. We strive to continually improve our work environment to allow for high team member engagement and the ability for all team members to be successful.
Exciting variety of clients and business challenges – from the largest firms in the world to the largest health plans in the country to colleges and universities
A collaborative culture focused on career and professional development, abundant opportunities with meaningful and impactful experiences.
Competitive Pay and Benefits – competitive salary, unlimited PTO, 401K match, Employee Stock Ownership Plan
At AArete we’re successfully embracing working in a flexible work environment. We have offices across the globe in Chicago, New York, Los Angeles, Dallas, Denver, DC, London and India. Based on preference, AAretians have the opportunity to work remotely, visit nearby offices or client locations.

AArete is proud to have earned a Great Place to Work Certification™. We are named in Vault’s Top 50 Firms to Work For, Crain's Chicago Business Fast 50 for a 3rd year, Inc 5000’s Fastest Growing Firms list for 4 consecutive years, Consulting Magazine's Fastest Growing Firms for the 4 consecutive years, and Forbes Magazine’s Best Consulting Firms in America list of 2021.

Work you’ll do
Understand product needs to define optimal system specifications
Plan/ design the technology solution, build to scale and remain compliant to security standards
Communicate system requirements to product engineering teams
Provide information architecture, data engineering pipeline design, application software, API design and participate in development and production rollout of the solution
Evaluate and select appropriate tech stack and suggest integration methods & business impact
Ensure the implementation of agreed architecture and infrastructure
Work closely with product managers, engineering, and data science teams to oversee product build/ release
Resolve project challenges involving scope, design, and technical problems when they arise
Address technical concerns, ideas, and suggestions
Monitor systems to ensure they meet both user needs and business goals
Requirements
Degree in Computer Science, Information Technology, or related field
Minimum of 8 years of hands-on experience with data/system integration/analytics/software life cycle as a Data Engineer within a product development environment
Experience in conceiving/designing/ architecting/developing data platforms using Big data and cloud technologies in AWS is a plus
Proficient in designing DWH/ ETL solutions and In-depth knowledge of data models/ structures
Strong experience on AWS Glue, EMR, Redshift
Programming skills for data engineering
Documentation of critical artifacts such as Logical/Conceptual design, Source to Target mapping, System architecture, etc.
Problem-solving attitude, with a willingness to work in a fast-paced product development environment and a hands-on mentality to do whatever it takes to deliver a successful product
Experience with working with an on-shore/off-shore model
Understanding data standardization practices in the pharma domain, integrating Healthcare data, and security related to HIPAA is a plus
Experience with data and system integration using RESTful APIs, web services, JSON, and XML will be a plus
Familiarity with data visualization and BI tools is a plus"
326,"Data Engineer II, Amazon Fuse",Amazon,"Miami, FL","Description

Amazon Fuse is one of the fastest-growing and strategic new business areas inside Amazon. We are looking for a talented Data Engineer (DE) who is passionate about using data to drive crucial business decisions regarding our digital subscriptions. This role will focus on building world class processes around designing, collecting, organizing and presenting data to analyze traffic, understand customer behavior, feature performance and marketing efforts from business partnerships. As a Data Engineer working on the analytics platform, you will develop and support analytic capabilities to give flexible and structured access to data that is timely, accurate and actionable.

Responsibilities
Develop and maintain data sets to measure the performance of the program. This will include building metrics for High Value Actions, engagement, acquisition and landscape.
Build, maintain and optimize scalable self-service solutions that empower stakeholders to address their data needs.
Liaising with technology teams across Amazon to influence data model designs and data sharing. This may include writing papers to influence executive decisions.
Assist business stakeholders in creating and implementing business requirement documents to drive projects, working backward from customer needs.
Implement standardized, automated operational and quality control processes for difficult datasets to deliver accurate and timely data and reporting to meet or exceed SLAs
Communicate timely information to all stakeholders on current and future initiatives, prioritizing based on the organization’s needs

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Economics, Finance, Mathematics, Statistics, Engineering).
5+ years of relevant experience one of the following areas: data science, data engineering, business intelligence or business analytics.
Strong analytical and problem-solving skills.
Expertise in the design, creation and management of large datasets/data models.
Expert-level proficiency in writing complex, highly-optimized SQL queries across large datasets.
Ability to work with business owners to define key business requirements and convert to technical specifications.
Ability to manage competing priorities simultaneously and drive projects to completion.
Preferred Qualifications
7+ years of experience as a data engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in an internet-based company with large, complex data sources.
Experience with AWS services including S3, Redshift, EMR and RDS.
Knowledge with statistical and/or econometric modeling.
Experience in BI/DW as a change leader providing strategic research, recommendations, and implementations.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1569739"
327,Data Engineer - JD,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Advent Global Solutions, Inc., is seeking the following. Apply via Dice today!

ONLY W2 Key Skills Strong experience building robust and scalable data processes and pipelines for modeling, analysis, and reporting Experience with Enterprise Data Warehousing systems including Snowflake and Teradata (or equivalent) Fluency in SQL and Python, including data wrangling and schema design Experience working with pipeline tools like Airflow and dbt Experience with BI processes and some experience with dashboard tools like Tableau Some experience with CICD and containerization tools (e.g. Jenkins, Docker, Kubernetes) Ability to initiate, refine, and complete projects with minimal guidance and some experience working in a scrum or release cycle environment Ability to clearly communicate technical concepts, definitions, logic, and processes to a non-technical audience Ability to think critically and collaborate cross-functionally with other data engineering, data science and analytics stakeholders distilling business requirements into clear data products Experience designing data ingestion processes and working with unstructured data a plus"
328,Lead Data Engineer.,Johnson & Johnson,"Bridgewater, NJ","Johnson & Johnson is currently recruiting for a Lead of Data Engineering located in Bridgewater, New Jersey.

Caring for the world, one person at a time has inspired and united the people of Johnson & Johnson for over 130 years. We embrace research and science -- bringing innovative ideas, products and services to advance the health and well-being of people.

With $82.6 billion in 2020 sales, Johnson & Johnson is the world's most comprehensive and broadly-based manufacturer of health care products, as well as a provider of related services, for the consumer, pharmaceutical, and medical devices markets. There are more than 260 Johnson & Johnson operating companies employing over 135,000 people and with products touching the lives of over a billion people every day, throughout the world. If you have the talent and desire to touch the world, Johnson & Johnson has the career opportunities to help make it happen.

Thriving on a diverse company culture, celebrating the uniqueness of our employees and committed to inclusion. Proud to be an equal opportunity employer.

The Johnson & Johnson Supply Chain Digital & Analytics (“JJSC DNA”) group is seeking a Lead of Data Engineering to enable Johnson & Johnson Operating System (JJOS) to make better business decision using unified data and democratizing the data model.

Responsibilities
Lead portfolio of multiple data projects and work closely with business and IT functions to deliver data projects in timely and efficient manner
Lead development of provisioning data pipes for reporting and data science
Map the base data elements which needs to be transformed and provisioned
Work with business to get understanding of their data requirements
Help Business improve process and help business to make data driven decisions

Qualifications
A bachelor’s is required. A Master’s Degree in engineering or related field, e.g. statistics, operations research, computer science, mathematics; computer science degree is highly preferred
6+ years of work experience in data analysis or Data Engineering is required.
Expertise with SQL and backend ETL jobs written in SQL and other scripting languages like UNIX Shell Script etc. is required.
Proven experience in setting up and handling data in cloud computing environment (AWS or Azure) highly preferred.
Solid grasp of data lifecycle required around data ingestion, data contextualization, data insights and analytics is required.
Familiarity with various types of databases and business applications of distributed computing (e.g., MapReduce, HIVE, HADOOP) is required.
Experience of working with IT and application development functions
People leader experience required.
Ability to travel up to 25% of the time, including international travel.

Preferred
Knowledge of ERP systems
Experience in BI software tools like Tableau, Qlik, Power BI, etc.

At Johnson & Johnson, we’re on a mission to change the trajectory of health for humanity. That starts by creating the world’s healthiest workforce. Through cutting-edge programs and policies, we empower the physical, mental, emotional, and financial health of our employees and the ones they love. As such, candidates offered employment must show proof of COVID-19 vaccination or secure an approved accommodation prior to the commencement of employment to support the well-being of our employees, their families and the communities in which we live and work.

For more information on how we support the whole health of our employees throughout their wellness, career and life journey, please visit www.careers.jnj.com.

Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Primary Location

United States-New Jersey-Bridgewater-430 Route 22

Organization

Johnson & Johnson Services Inc. (6090)

Job Function

R&D

Requisition ID

2206016259W"
329,Expert Cloud Data Engineer,Dice,"Waltham, MA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Stamford Technology Solutions LLC, DBA Infinity, is seeking the following. Apply via Dice today!

Role Expert Cloud Data Engineer Location Waltham, MA and Atlanta, GA Contract Prerequisites Bachelor'sMasterrsquos degree in Computer Science or equivalent subject 12+ years of experience in engineering ndashhands-on leader with deep engineering track record in modern tech stacks, frameworks, and programming languages Strong command of English language (both verbal and written) Qualifications middot Extensive track record in engineering large-scale cloud, big data and analytics implementations in complex usage scenarios and migration paths middot Expert hands-on knowledge in public cloud products, cloud-native architectures, serverless frameworks, DockerKubernetes, data security, and integration services middot Deep expertise in building big data ecosystems, analytics and AIML platforms and accelerators, data management systems using HadoopSpark, Tensorflow, Jupyter, HBase, BI tools, and public cloud ML tools and services middot Experience building enterprise-grade data and analytics platforms that are highly scalable, compliant and secure with robust data quality, data governance, data discovery, catalog and visualization capabilities. middot Proficient in building large scale systems using Python, Java, or other high-performance languages, developer tools, CICD, DevOps, monitoring tools and engineering cloud migration solutions. middot Proven track record in enterprise wide API management, Microservices, Event-driven architectures and complex integrations. middot Experience in building and leading highly effective and agile data analytics engineering teams. middot Collaborative -able to build strong relations that enable robust debate and survive periodic disagreements regarding priorities. middot Influential without direct line management control across multiple cultural environments (AsiaIndiaEuropeAmerica), other internal McKinsey units, and external third parties. middot Ability to communicate effectively with technical and non-technical audience middot Degree in computer science or engineering or related discipline industry recognized cloud certifications, patents, open source contributions are preferred Non-technical competencies middot Ability to work with multiple teams to provide technical guidance and oversight middot Strong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions middot Distinct customer focus and quality mindset Excellent interpersonal, leadership and communication skills middot Ability to work under pressure with a solid sense for setting priorities middot Comfortable working in a team based and collaborative environment middot Coach and mentor team members to follow good engineering best practices Best Regards, Kanika Malik Infinity, a Stamford Technology Company Accelerating Customer Growthhellip Global Locations USA-Canada-India-EMEA Phone Email www.infinitysts.com httpwww.infinitysts.com"
330,Data Engineer - Advertising Analytics Data Pipeline,Amazon,"Boulder, CO","Description

Are you passionate about using Big Data to build customer trust and grow new business? Global advertisers rely on our team's performance insights to drive future investment in Amazon's Advertising Platform and improve the relevance of ads shown to customers. We are looking for passionate Data Engineers to own and optimize the big data pipeline that consumes the massive data sources we require to generate unique insights. Data is at the center of every product we will develop as we create brand new systems that serve the needs of our large and growing base of advertisers.

We are open to hiring for this position in Seattle or New York City.

You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence. Members of this team will be challenged to innovate using big data technologies. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive profitability. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you!

Amazon is well positioned to grow its share of a fast growing online advertising industry due to its unique assets - e-commerce data, service oriented architecture, and startup culture. Be part of a team of industry leading experts that builds and operates one of the largest big data analytics platform at Amazon. Amazon is applying the latest machine learning and big data technologies available to change the way marketers purchase, track, measure, and optimize their advertising spend. We apply these technologies on terabytes of data (over 10B new events per day) and operate clusters that push scalability limits of the existing technologies. We seek to measure every possible signal indicating impact of advertising to provide the most objective result of marketing spends.

What we offer

We are a company of builders who bring varying backgrounds, ideas, and points of view to inventing on behalf of our customers. Our diverse perspectives come from many sources including gender, race, age, national origin, sexual orientation, culture, education, and professional and life experience. We are committed to diversity and inclusion and always look for ways to scale our impact as we grow. You can read more here.

Amazon has 13 affinity groups, also known as employee resource groups, which bring Amazon employees together across businesses and locations around the world. Some examples include the Black Employee Network (BEN), Amazon Women in Engineering (AWE), and Indigenous@Amazon.

The pay range for this position in Colorado is $113,000 - $160,000 [yr]; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Amazon is an Equal Opportunity-Affirmative Action Employer - Female/Minority/Disability/Veteran/Gender Identity/Sexual Orientation.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
5+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, Presto, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficiency in, at least, one modern scripting or programming language such as Python, NodeJS, Java, or Scala.
Preferred Qualifications
Experience working with and tuning AWS big data technologies (EMR, Spark/Hive, S3). Experience working with SQL based systems and building data pipelines.
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

#adsto #madsjob


Company - Amazon.com Services LLC

Job ID: A1519389"
331,Data Engineer/Scientist,Dice,"Reston, VA","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Maania Consultancy Services, is seeking the following. Apply via Dice today!

Clearance Level Secret (TSSCI Eligible) ship Required Years of Experience 5-7 years Education Level Bachelor of Science Degree Required Skills Developing and operating data processing systems Exploratory data analysis (EDA), ETL, statistical analysis and data processing using Python and Pandas, Matplotlib, NumPy, and Scikit-Learn Experience designing, implementing, maintaining and scaling PostgreSQL databases Virtualization systems (e.g., Docker) Experience working on classified automated information systems An understanding and ability to implement data hygiene methods (e.g., filtering and imputation) Experience using multiple data formats (e.g., JSON, Protobuf, FlatBuffer, SQL, CSV) BS in a relevant area (e.g., CS, Math, EE, Statistics)"
332,"Data Engineer, WW Installments Competitive Pricing",Amazon,"Arlington, VA","Job Summary

DESCRIPTION

WW Installments is one of the fastest growing businesses within Amazon and we are looking for a Data Engineer to join the team. This group has been entrusted with a massive charter that will impact every customer that visits Amazon.com. We are building the next generation of features and payment products that maximize customer enablement in a simple, transparent, and customer obsessed way. Through these products, we will deliver value directly to Amazon customers improving the shopping experience for hundreds of millions of customers worldwide. Our mission is to delight our customers by building payment experiences and financial services that are trusted, valued, and easy to use from anywhere in any way.

As the Data Engineer within WW Installments, you will be responsible for designing and implementing the WW Installments Competitive Pricing metrics and data pipeline roadmap. This core capability is fundamental to the success of the business. You will have high visibility across Amazon businesses and leadership.

Key job responsibilities

This Role Will Be Responsible For
Build the WW Installments Competitive Pricing Analytics data platform that will be used by teams worldwide to access reporting, deep dives, and installment related insights.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.
Design, implement, and maintain production data pipelines including DQ, SLA, and data agreements across data, ML, and partner teams.
Continually improve the reporting and analysis pipeline, automating and simplifying whenever possible, and enabling self-service support for stakeholders.
Engage business stakeholders in constructive dialogues to convert business problems into data pipeline logic. Identifying new opportunities to influence business strategy and product vision using data.
Support fellow engineers and scientists to deliver analytical projects and build proof of concept applications. Partner with fellow Data/Applied Scientists to implement scalable, automated infrastructure for data extraction, processing, computation, and delivery.
Learn new technology and techniques to support product and process innovation.
Work through significant business and technical ambiguity delivering on analytics roadmap across the team with autonomy.

Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Spark, EMR, etc.)
Basic/Intermediate proficiency in programming language (Python and Scala) for automation of data extraction/processing, statistical computation, and/or web scraping.
Ability to deal with ambiguity and competing objectives in a fast-paced environment.
Preferred Qualifications
Master's degree in computer science, engineering, mathematics, or a related technical discipline
A desire to work in a collaborative, intellectually curious environment.
5+ years of industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist, and Applied Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Coding proficiency in at least one modern programming language (Python, Scala, Java, etc)
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Query performance tuning skills using Unix profiling tools and SQL
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Experience with AWS, cloud computing
Prior experience in tech, ecommerce, retail, or finance/banking industry
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1883027"
333,"Data Engineer, Snowflake",Deloitte,"Horsham, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
334,"Data Engineer, Student Programs",Amazon,"Arlington, VA","Job Summary

DESCRIPTION

Do you love data? Do you get a thrill out of fixing complex problems? Do you want to work with AWS applications for computing and storing data?

Join the Student Programs Engineering, Analytics, and Research (SPEAR) team to help us design business intelligence solutions for Recruiting Teams across the globe. SPEAR is responsible for hiring the best student talent from all backgrounds to build the future of Amazon’s global workforce. Our team’s job is to equip our recruiting teams and partners with data and insights on the addressable marketplace of talent, the global hiring demand projections, and process efficiency gains.

SP’s role is to provide forecasts and insights to our global recruiting organization from our various recruiting and HR systems. Your goal will be to help us build out our data warehouse. As our Data Engineer, you can shape the future data roadmap for all of our global HR teams. You will help us leverage AWS technologies such as Redshift, S3, EDX, and QuickSight.

You won’t be alone. Our team includes other business intelligence engineers and analysts — all of whom will be partners for you in realizing our vision and enabling the growth at Amazon that’s critical for our customers. You’re just right for this role because you like diving into data, developing innovative solutions and learning new tools!


Basic Qualifications

3+ years experience in dimensional data modeling, designing and implementing scalable ETL design and mappings, database query construction and data warehousing skills

Excellent knowledge of SQL and Linux OS

Server management and administration including basic scripting

Proficient in Stata, R, Python, Java or some combination

Self-starter with the ability to work independently Bachelor’s degree in CS or related technical field


Preferred Qualifications

Master’s degree in Computer Science

Familiar with Redshift, Hadoop, Java

Experience with various BI Platforms (QuickSight, OBIEE, Tableau, MicroStrategy)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1939115"
335,Python - Data Engineer,Dice,"Spring, TX","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Digital Intelligence Systems, LLC, is seeking the following. Apply via Dice today!

Python - Data Engineer with Strong ETL Skills Product Statement Kepler is the general service data platform for our clients' Upstream Company. As the platform has grown over the course of the past two and a half years, the need to enable and onboard new business use cases onto the strategic data platform has grown substantially. The Kepler team supports an in-depth data analytics technology stack ranging from ETL tooling to cloud data warehousing. Members of our Kepler team deliver new full stack functionality to our platform and provide consulting to the business consumers of the platform. Our team is composed of Database Administrators, Data Pipeline ETL Developers, Python Developers, and Full Stack Developers to provide a holistic data and analytics ecosystem. Data Pipeline ETL Developer The Data Pipeline ETL Developer will focus on tasks related to Extraction, Transformation, and Loading of data (ETL) for existing and new data pipelines within our platform. Scope of Deliverables Support in developing and owning ETL pipelines within our data platform Data extraction and transformation pipeline Automation using PythonAirflowAzure Data FactoryQlikFivetran Data Engineering support for Upstream Digital Data Foundation and Upstream use cases Delivery of task monitoring and notification system for data pipeline status Supporting additional data ingestion and curation enrichment tasks to enable ongoing business use cases Supporting additional data cleansing, enrichment, and curation enrichment tasks to enable ongoing business use cases Developing and delivering data pipelines through a CICD delivery methodology Developing monitoring around pipelines to ensure uptime of data flows Required Proficiencies Expertise in Python and Python data libraries, Azure Data Factory, Airflow, and Databricks (Expert in ETL Pipelines) Expertise with Snowflake snowflake data Python development specifically experience with ETL and data libraries such as Pandas, Interaction with API's ANSI SQL Query Expert and some experience with Database Administration Typical azure services (currentfuture) used (ADX, ADF, Databricks) ADLS use in data ETL and storage Familiarity with Airflow (and other data pipeline experience) Digital Intelligence Systems, LLC (DISYS) is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status."
336,Intern - Data Engineer (Remote),Stryker,"Chicago, IL","Why engineering at Stryker?

At Stryker we are dedicated to improving lives, with a passion for researching and developing new medical device products. As an engineer at Stryker, you will be proud of the work that you will be doing, using cutting-edge technologies to make healthcare better. Here, you will work in a supportive culture with other incredibly talented and intelligent people, creating industry-leading medical technology products. You will also have growth opportunities as we have a culture that supports your personal and professional development.

Need another reason to apply? Check out these 8 reasons to join Stryker's engineering team: https://www.strykercareersblog.com/post/8-reasons-to-join-strykers-engineering-team

We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com

Our benefits include bonuses; commissions; healthcare; insurance benefits; retirement programs; stock based plans; paid time off plans; family and parenting leaves; tuition reimbursement; wellness programs; onsite fitness centers and cafeterias; discount purchase programs; and service and performance awards - not to mention various social and recreational activities.

Our Data Engineers are responsible for developing the architecture to analyze and process data according to business needs. The Data Engineer will assist in the design, build, and management of data assets that will deliver accurate, reliable data to the right place, in the right format, at the right time. In this position you will be engaged on projects that will ultimately use the data for advanced analytics techniques, such as optimization, forecasting, machine learning, predictive maintenance, and statistical analysis to develop solutions that help deliver significant value to customers.
Work with clinical, image, device, wearable, and demographic data to perform a variety of data transformations and data analysis
Build ETLs and data models using a variety of data tools (Python, SQL, Data Factory, Databricks) on Azure
Assist in decision making on database architecture and design for multiple use cases
Run existing ETLs and monitor data pipelines for errors
Perform root cause analysis on errors/bugs and provide solutions to address
Implement ETLs and other code into production data pipelines and analytics solutions that can be used for products in addition to research initiatives
Identify and act upon opportunities to improve the system process flow, cost, performance, and technical efficiencies
Know someone at Stryker?

Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program at https://careers.stryker.com/referrals/

About Stryker

Stryker is one of the world’s leading medical technology companies and, together with our customers, is driven to make healthcare better. We offer innovative products and services in Orthopaedics, Medical and Surgical, and Neurotechnology and Spine that help improve patient and hospital outcomes.

We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com"
337,Data Engineer,Brookhaven National Laboratory,"Upton, NY","Why Work at BNL?

Brookhaven National Laboratory (www.bnl.gov) delivers discovery science and transformative technology to power and secure the nation’s future. Brookhaven Lab is a multidisciplinary laboratory with seven Nobel Prize-winning discoveries, 37 R&D 100 Awards, and more than 70 years of pioneering research. The Lab is primarily supported by the U.S. Department of Energy’s (DOE) Office of Science. Brookhaven Science Associates (BSA) operates and manages the Laboratory for DOE. BSA is a partnership between Battelle and The Research Foundation for the State University of New York on behalf of Stony Brook University.

Organizational Overview

Brookhaven National Laboratory is entering an exciting new chapter with one of the newest and most advanced synchrotron facilities in the world. National Synchrotron Light Source II (NSLS-II) enables the study of material properties and functions with nanoscale resolution and exquisite sensitivity by providing world-leading capabilities for X-ray imaging and high-resolution energy analysis.

This facility is open to users from academia and industry and its operations are at a time when the world enters a new era with a global economy fueled largely by scientific discoveries and technological innovations. NSLS-II provides the research tools needed to foster new discoveries and create breakthroughs in critical areas such as energy security, environment, and human health.

Position Description

NSLS-II is seeking an exceptional candidate to contribute to a multi-disciplinary project where they will build a Data repository working with BNL, participating universities and other partners to design, develop, and implement a data repository solution to facilitate data sharing and enhance coordination across the center.

The candidate will work closely with the C2QA Center’s Cross-cutting co-design team, Thrust Leaders and NIST partners to coordinate project milestones and deliverables. You will be based at BNL and part of National Synchrotron Light Source II's Data Science and Systems Integration (DSSI) Program, which provide supports and expertise to meet the scientific computing needs of the NSLS-II

Essential Duties And Responsibilities
Spearhead a focused effort to develop a user-friendly data management tool/database which will be used by center members to support data sharing, coordination and collaboration
Collaborate with the Center’s scientists to identify and implement strategies for data handling, sharing, management and analysis
Provide documentation, training and support for the database and software solutions developed.
Collaborate with NIST partners to develop the public facing interface for the data repository
Develop protocols and tools for data validation and publication
Position Requirements
Bachelor's or higher level degree in Computer Science, Physical Sciences, Applied Mathematics or related field and a minimum of one (1) years of relevant experience
Demonstrated experience of data and code management best practices
Ability to implement agile software engineering methodologies
Experience in collaborative development of software and cloud infrastructure, especially in distributed teams.
Demonstrated experience in multiple programming languages, including Python and script
Excellent communication skills
Preferred Knowledge, Skills and Abilities (experience in one or more of the following):
Software design, development and deployment with version control
Working with NoSQL and/or Relational databases
Use of infrastructure systems (storage, networked accessible systems, collaborative tools)
Containerized development and deployment (e.g. AWS, Docker)
Design and use of standard APIs
Developing training resources, including documentation, manuals, and tutorials of software tools for users.
Providing user support (i.e. center members)
Experience with Linux operating systems.
Other information: Candidate will be placed at appropriate level contingent upon depth and breadth of experience.

Our Benefits Program Includes, But Is Not Limited To

At Brookhaven National Laboratory we believe that a comprehensive employee benefits program is an important and meaningful part of the compensation employees receive.
Medical, Dental, and Vision Care Plans
Flexible Spending Accounts
Paid Time-off and Leave Programs (vacation, holidays, sick leave, paid parental leave)
401(k) Plan
Flexible Work Arrangements
Tuition Assistance, Training and Professional Development Programs
Employee Fitness/Wellness & Recreation: Gym/Basketball Courts, Weight Room, Fitness Classes, Indoor Pool, Tennis Courts, Sports Clubs/Activities (Basketball, Ping Pong, Softball, Tennis).
Brookhaven National Laboratory and the Energy and Photon Sciences Directorate are committed to your success. We offer a supportive work environment and the resources necessary for you to succeed.

Brookhaven Science Associates requires proof of a COVID-19 vaccination for all employees. Proof of full vaccination as recognized by the CDC and/or WHO, inclusive of the two-week waiting period, is required at the start of your employment.

Brookhaven National Laboratory (BNL) is an equal opportunity employer that values inclusion and diversity at our Lab. We are committed to ensuring that all qualified applicants receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, status as a veteran, disability or any other federal, state or local protected class.

BNL takes affirmative action in support of its policy and to advance in employment individuals who are minorities, women, protected veterans, and individuals with disabilities. We ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
VEVRAA Federal Contractor"
338,"Data Engineer, PSAS Data Engineering and Analytics",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The Amazon e-commerce platform supports hundreds of thousands of sellers who fuel the rich buying experience that Amazon is today. From artisans selling handmade goods to some of the world's largest retailers, Amazon's merchants are quickly becoming the heart of the Amazon shopping experience. Join Amazon Services, the fastest growing division of Amazon.com, in creating the next evolution of the internet's finest selling experience.

The team builds Self-Service analytic solutions for Selling Partner and Seller Experience teams that enables business users to make quick and data driven business decisions. Our tools are strategically important to our leadership, finance, economists, analysts, machine learning scientists, SDEs and BI partners to drive long-term growth. We are highly motivated, collaborative, and fun loving with an entrepreneurial spirit and bias for action. With a broad mandate to experiment and innovate, we are growing at an unprecedented rate with a seemingly endless range of new opportunities.

As an experienced member of the team, in this role, you will:
Contribute to evolving the technical direction of SPS analytical Systems and play a critical role their design and development
Build and support billing pipeline responsible for handling all impacting events in Selling partners domain in real-time.
You will research, design and code, troubleshoot and support. What you create is also what you own.
Develop the next generation of automation tools for monitoring and measuring data quality, with associated user interfaces.
Have the satisfaction of seeing your work impact thousands of Amazon selling partners world-wide.
Be able to broaden your technical skills and work in an environment that thrives on creativity, efficient execution, and product innovation.

Basic Qualifications
Bachelors (BS/BE) in Computer Science or related field
3+ years of experience in software development and full product life-cycles
Excellent coding skills in Java, Python, C++, or equivalent object-oriented programming language
Understanding of relational and non-relational databases and basic SQL
Proficiency with at least one of these scripting languages: Perl / Python / Ruby / shell script
Preferred Qualifications
Experience with building data pipelines from application databases.
Experience with AWS services - S3, Redshift, Spectrum, EMR etc.
Experience working with Data Lakes.
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Sharp problem solving skills and ability to resolve ambiguous requirements
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1873308"
339,AWS Data Engineer,Deloitte,"San Jose, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
340,Data Engineer,Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
7+ years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
341,Data Engineer,Amazon,"Herndon, VA","Description

Come join a security team who focuses on ease of use and delighting our customers to build security services, used across all of Amazon, that secure our internal systems and networks.

At Amazon Web Services (AWS), Security is our highest priority. Our team is responsible for inventing, scaling, and operating the foundational security services used across Amazon to secure our systems, processes, and data. We build authentication, authorization, and enforcement services that are used across our diverse internal teams as they build and deliver services for our customers. We support all of Amazon.com and AWS with our services, and need solution-oriented thinkers who are passionate about solving problems at scale, thinking through edge cases, and iterating quickly to delight our customers.

Are you a good fit for our team?

You’ll love this team if you are passionate about security, and curious how we design security into the cloud and Amazon infrastructure from the minute a server hits our data center through how we securely administer and operate services. At Amazon, we are looking for engineers who know how to think through a problem, be creative, and who think outside the box in ways that benefit our customers. The intense focus we have on our customers is why we are one of the world’s most beloved brands – it is part of our company DNA. Our Engineers (you!) are obsessed with customer trust, building solutions that address real world risks, and thinking deeply about how humans use security systems to build elegant security that users love to use. We are looking for people who bring diverse perspectives, ask a lot of questions, and challenge the status quo. We are a team of out-of-the-box thinkers, who iterate quickly, and think deeply about how to build secure systems that are easy to use.

Team Culture

We work across the company on services that are always-on; the company depends on us. To perform our best, we value work/life harmony, flexible schedules, and unscheduled time to brain storm. We support established, and rapidly growing services, with a DevOps model, so you will be part of an oncall rotation every couple of months. Our challenges and goals are ambitious, often ambiguous, and we sometimes try things that don’t work. We embrace the learnings from failure and iterate quickly. We emphasize shipping rapidly, using automated releases and testing to deploy frequently, and deeply root causing issues to ensure that we stay fast. Almost all of our team works from home these days, but typically we are together in the office collaborating around a whiteboard, planning over coffee, or playing a video game during downtime.

What you would do

In this role you will join an established and rapidly growing DevOps software team who is maintaining an existing security service while expanding the deployment, features, and capabilities to address significant new initiatives across the company. We are looking for Data Engineers with expertise to help us design, build, and maintain performant, big data solutions that scale to meet the needs of our customers.

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.

Our team also puts a high value on work-life balance. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here, which is why we aren’t focused on how many hours you spend at work or online. Instead, we’re happy to offer a flexible schedule so you can have a more productive and well-balanced life—both in and outside of work.

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.


Basic Qualifications
2+ years of professional experience with data modeling, architecture, and engineering
2+ years experience with query languages and performance tuning
2+ years experience with at least one scripting language
Preferred Qualifications
Bachelors or advanced degree in Computer Science or closely related field
4+ years of experience with query languages
2+ years of experience with scripting languages such as Python, Ruby, or Perl
Experience building/operating highly available, distributed systems for data extraction, ingestion, and processing of large data sets
Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Kinesis and Lambda for serverless ETL)
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using business intelligence reporting and data visualization tools
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workforce. Amazon is an equal opportunity employer and does not discriminate on the basis of race, ethnicity, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Dev Center U.S., Inc.

Job ID: A1538389"
342,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Confidential Company, is seeking the following. Apply via Dice today!

HarmonyTech,Inc., is seeking a highly motivated and enthusiastic Data Engineer providing leadership and support for existing systems, processes, and policies as well as working to implement new technologies and processes. The ideal candidate will be able to conceptualize viable technical solutions that meet customer objectives and project requirements within the cost and schedule constraints. The scope of this position is to possess and apply expertise on multiple complex work assignments that require originality and innovation to accomplish tasks. This position requires Public Trust Clearance and U.S. Citizenship. Required Skills Experience creating and implementing data pipelines, ETL jobs, and database schemas for large scale IT solutions Experience implementing appropriate security controls and data use and access policies Experience supporting the data migration and integration from existing legacy applications Experience implementing data integration with on-prem and external systems and managing the data dictionary and metadata Experience with AWS services such as DMS, Glue, Aurora MySQL, RedShift, Elastic search, Cloud Watch, Glue Data Catalog, S3, Lambda, required Preferred ThoroughWorking knowledge of collection methods, capabilities, and tasking process. Intellectual curiosity creativity and innovation to go beyond current tools to deliver the best solution to complex problems. Strong analytical and critical thinking skills Education and Experience Strong analytical and critical thinking skills Bachelor's degree, preferably in computer science or a directly related field. A minimum of 5 years of relevant experience. If you are interested and feel that you would excel in the position, we invite you to apply. During this phase of our recruiting effort, we will not be able to accept telephone calls. Only those candidates meeting the requirements will be contacted. No recruiters please. About HarmonyTech We have been delivering information technology services and solutions across the Federal government and commercial clients for over a decade. Our employees are the most important assets of our company because they delivery value and care for our clients. We are a company of passionate technologist constantly evolving in our understanding and application of technology to best fulfill our clientrsquos mission needs. We operate under a flat and efficient organizational structure to ensure our hand-picked consultants have the flexibility to take risks and be innovative. We typically work in small, agile teams as we design and develop solutions to address our clientrsquos business challenges. Our success is measured with every satisfied customer and employee. Why you want to join us You have a passion for solving our customers complex business problems Awesome learning and professional development opportunities A culture built on teamwork and excellence Benefits HarmonyTech offers a highly competitive salary and benefits package httpssecure.zenefits.combenefitsPreviewrequiredInformation?token92d03fd4-3d7d-486f-9e9f-d18ceeb9ad81 including MedicalDentalVision Insurance (company contributes 80 of the entire premium) ShortLong Term Disability Coverage Life and ADD Insurance 401(k) Retirement Plan with Company Match Tuition Reimbursement Employee Referral Bonus Paid Federal Holidays Accrued Paid Time Off Click the benefits package link above to check out the benefits we offer Legal HarmonyTech believes in a policy of equal employment and opportunity for all people based on merit. We are an Equal Opportunity Employer (EEO) and Drug Free Workplace Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or national origin or any other category protected by applicable federal, state or local laws. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel. Additionally, they do not establish a contract for employment and are subject to change at the discretion of HarmonyTech."
343,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Exl Service, is seeking the following. Apply via Dice today!

We're looking to add Data Engineers immediately with 2+ years of professional experience and proficiency in Python, SQL and AWS related frameworks and services. By driving requirements gatheringanalysis, development, testing, documentation, and deployment tasks, this technical and business-facing role works independently as well as collaboratively to ensure project success. Sr. Data Engineer consultants have extensive experience developing solutions using modern data tools and platforms, a strong working knowledge of numerous enabling technologies, and are eager to learn new skills. They are also able to fill different positions on multi-functional and multi-shore project teams, including team lead. Required Knowledge Skills 5+ years of experience in a Data ManagementEngineer role using SQL, Python, PySpark and modern cloud based ETL tools like AWS Glue Highly proficient in writing complex SQL Practical experience with data integrationpipeline development Practical experience with data quality preferred Excels in a fast paced, agile environment where critical thinking and strong problem-solving skills are required for success Overview EXL (NASDAQ EXLS) is a leading operations management and analytics company that designs and enables agile, customer-centric operating models to help clients improve their revenue growth and profitability. Our delivery model provides market-leading business outcomes using EXLrsquos proprietary Business EXLerator FrameworkTrademark, cutting-edge analytics, digital transformation and domain expertise. At EXL, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 32,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), South America, Australia and South Africa. For more information, visit www.exlservice.com. Physical Demands and Working Conditions Works in a normal office environment requiring light physical effort by handling objects up to 20 pounds occasionally andor up to 10 pounds frequently. Works with standard office equipment (such as phone, fax and personal computer). Ability to navigate stairs frequently during the course of a business day in various work locations. Sitting at desk for most of day, and working with computer. EEOMinoritiesFemalesVetsDisabilities Please be aware that EXL requires all employees to be vaccinated for COVID-19. This position will require the successful candidate to obtain and show proof of a vaccination. EXL is an equal opportunity employer, and will provide reasonable accommodation to those individuals who are unable to be vaccinated consistent with federal, state, and local law."
344,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, GLOBAL VISSE INC, is seeking the following. Apply via Dice today!

ONLY W2, NO C2C Job Title Data Engineer Location Remote (ET) Duration 6-12 Months+ Data Engineer onshore with the following skills- Total Experience in Analytics (ETL, Big Data, Reporting and Analytics, Multidimensional Modeling) 8+ years Tech-skills (required) Python for Analytics (ETL) Postgresql AWS Services Set Up (S3, EC2, IAM) CICD Soft-skills (required) Capable of working alone with almost no supervision Problem solving (including the ldquoproblemrdquo of learning a new tool or language) BA skills understand the need, translating business ideas to technical implementation (and vice-versa) and even help business in understanding what they want Creativity propose new ideas that business hasnrsquot even thought about Passion for data (collect, prepare, analyze) Global Visse Inc is an Equal Opportunity Employer."
345,*Data Engineer,Amazon,"Dallas, TX","Description

Do you want to build a cutting-edge highly scalable data platform using AWS technologies? How about building this platform to support the fast growth of AWS itself? Is problem solving through teamwork in your DNA? Do you like the idea of seeing how your work impacts the bigger picture? Answer yes to any of these and you’ll fit right in here at Amazon Web Services.

Amazon Web Services (AWS) is seeking a Data Engineer (DE) with a passion for developing data architecture and tools to support self-service data analytics. You will play a critical role supporting AWS Finance BI and building solutions to support data needs in one of the world's largest and most complex data environments. In this role, you will have ownership of end-to-end development of data engineering solutions to complex questions and you’ll play an integral role in strategic decision-making.

The successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, and the ability to work in a fast-paced environment. You will build and manage analytical resources on AWS and support content-related projects. You should have deep expertise in the design, creation, management, and business use of extremely large datasets. Above all you should be passionate about working with huge data sets and eager to learn new solutions to answer business questions and drive change.

In this role, you will have the opportunity to display and develop your skills in the following areas:
Design, implement, and support an analytical platform providing ad hoc access to large datasets and computing power
Managing AWS resources including EC2, RDS, Redshift, Lambda, and etc.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies
Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers

Basic Qualifications
Bachelor’s degree in Computer Science, MIS, related technical field, or equivalent work experience.
At least 2-3 years of relevant work experience in analytics, data engineering, business intelligence or related field
Demonstrable ability in data modeling, ETL development, and Data warehousing, or similar skills
Demonstrable advanced skills and experience using SQL with large data sets (e.g. Oracle, Redshift)
Experience with AWS technologies including Redshift, RDS, S3
Proficient in one programming language (e.g., Python, Java)
Proven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams
Preferred Qualifications
Graduate degree in computer science, business, mathematics, statistics, economics, or other quantitative field
3+ years prior experience in a Data Engineer role with a technology company or financial institution.
Both technically deep and business savvy enough to interface with all levels and disciplines within the organization
Demonstrated ability to coordinate projects across functional teams, including engineering, IT, product management, finance, and operations
Experience with AWS technologies including EMR, Kinesis
Experience with reporting tools like Tableau, Excel or other BI packages
Familiarity with the DevOps concepts
Familiarity with Linux
Experience with Hadoop or other map/reduce ""big data"" systems and services
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1666274"
346,Data Engineer - Artificial Intelligence & Machine Learning,Schneider Electric,"Andover, MA","Schneider Electric has an opportunity for a Data Engineer - Artificial Intelligence & Machine Learning in our Andover, MA location.

Schneider Electric creates connected technologies that reshape industries, transform cities and enrich lives. Our 135,000+ employees thrive in more than 100 countries. From the simplest of switches to complex operational systems, our technology, software and services improve the way our customers manage and automate their operations. Help us deliver solutions that ensure Life Is On everywhere, for everyone and at every moment.

http://www.youtube.com/watch?v=YtExntUe89c

Great People Make Schneider Electric a Great Company.

Does working with data on a day to day basis excite you? Are you interested in building robust data architecture to identify data patterns and optimise data consumption for our customers, who will forecast and predict what actions to undertake based on data? If this is what excites you, then you’ll love working in our intelligent automation team.

Schneider Digital is leading the digital transformation of Schneider Electric by building highly available, massive scalable digital platform for the enterprise.

We are looking for a savvy Data Engineer to join our growing team of AI and machine learning experts. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. AI@SEHub

The Data Engineer will support our software engineers, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Responsibilities
Create and maintain optimal data pipeline architecture; assemble large, complex data sets that meet functional / non-functional requirements design and build production data pipelines from ingestion to consumption within a big data architecture
Build the necessary datamarts, data warehouse required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Create necessary preprocessing and postprocessing for various forms of data for training/ retraining and inference ingestions as required
Create data visualization and business intelligence tools for stakeholders and data scientists for necessary business/ solution insights
Identify, design, and implement internal process improvements: automating manual data processes, optimizing data delivery, etc.
Ensure our data is separated and secure across national boundaries through multiple data centers and AWS regions

Qualifications

We know skills and competencies show up in many different ways and can be based on your life experience. If you do not necessarily meet all the requirements that are listed, we still encourage you to apply for the position.
You should have a bachelors or master’s degree in computer science, Information Technology or other quantitative fields
You should have at least 5 years working as a data engineer in supporting large data transformation initiatives related to machine learning, with experience in building and optimizing ‘big data’ pipelines and data sets
Strong analytic skills related to working with unstructured datasets.
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift and familiarity with various log formats from AWS.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, etc.
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more. Click here to find out more about working with us: http://se.com/us/careers.



We seek out and reward people for putting the customer first, being disruptive to the status quo, embracing different perspectives, continuously learning, and acting like owners. We’re recognized around the world for welcoming people as they are. We create an inclusive culture where all forms of diversity are seen as a real value for the company.  See what our people have to say about working for Schneider Electric.

https://youtu.be/C7sogZ_oQYg

AI@SEHub



Let us learn about you! Apply today.



You must submit an online application to be considered for any position with us. This position will be posted until filled.

As a federal government contractor, all Schneider Electric U.S. employees (including U.S. territories and Puerto Rico) must be fully vaccinated against COVID-19, subject to federal laws.



It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct.



Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.



Schneider Electric is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Schedule: Full-time

Req: 0078VU"
347,Data Engineer,Dice,"Madison, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Alpha Consulting Corp., is seeking the following. Apply via Dice today!

DATA ENGINEER MADISON, NJ 9 Months with possibility of Extension, Fully Remote. Project Description The Data Platform team in our Company's Animal Health IT (MAHI-IT) designs and implements end to end data solutions to support customer facing applications in animal traceability, monitoring, well-being, and more. We seek a data engineer to help the team setting up, maintaining, optimizing, and scaling data pipelines from multiple sources and across different functional teams in a cloud environment. Assist in developing best practices for deploying, monitoring, and scaling data pipelines in the cloud. Identify requirements for ingestion, transformation, and storage of data. Design and implement optimal and scalable data pipelines. Use cloud tools to integrate data from multiple data source into the data lake and design and implement ways to expose it. Identify opportunities for automation and optimization of data pipelines and re-design of data architecture and infrastructure for great scalability and optimal delivery. Implement cloud data infrastructure required to extract, transform, and load data from multiple sources. Identify required security and governance procedures to keep the data safe in a cloud environment. Assist in developing and executing testing plans to help with QA efforts. Required Skills Bachelor's degree in Data Engineering, Computer Science, or related field. Experience designing and implementing data engineering pipelines. Advanced knowledge in Python and PySpark. Working knowledge of one or more SQL languages. 3+ years of hands-on experience with developing data warehouse solutions and data products. 1+ year of hands-on experience developing a distributed data processing platform with Hadoop, Hive, Spark, Airflow, Kafka, etc. 3+ years of hands-on experience in modeling and designing data schemas. Advanced experience with programming languages Python, Pyspark, Scala, etc. Knowledge of scripting languages Perl, Shell, etc. Practice working with, processing, and leading large data sets. Experience with cloud tools for ingesting and processing data. Experience with AWS tools big data platforms S3, EMR, EKS, Lambda, etc. Experience with data ingestion and transformation tools like Streamsets and Databricks. Experience working with DevOps teams. Experience with container technologies such as docker and Kubernetes. Experience with data warehousing tools like Snowflake and Redshift. This 9+ month position starts ASAP. Please E-MAIL your resume (attachment to email) with rate and availability to Cindi mailto ALPHA'S REQUIREMENT 22-01279 MUST BE ELIGIBLE TO WORK IN THE U.S. AS AN HOURLY W2 EMPLOYEE"
348,Data Analytics Engineer,thatgamecompany,United States,"Data Analytics Engineer

thatgamecompany is best recognized for creating award-winning, enriching, and meaningful game titles such as Journey, Flower, and flOw. Our most recent game, Sky, is our most complex undertaking to date. It is a social network built around the values inherited from a powerful humanistic story. It is a live experience continuously evolving inside a global online theme park.
We are seeking passionate engineers to help us build various data-centric products and solutions that will help us process and gain insights from terabytes of data per day generated by millions of active players
As a Data Analytics Engineer, you will serve a crucial role as a nexus between all teams in the studio, gathering requirements, inspiring data projects, and helping facilitate agreement on data availability, standards, and quality.

On any given day at thatgamecompany, you might:
Design and implement the ETL to grab data from Postgres, MongoDB, or a variety of external data sources and store it all in our GCP Data Warehouse
Write and Optimize SQL queries and tables to provide quick, efficient, and intuitive access to our data
Help brainstorm and investigate potential solutions for challenges like Realtime Data Aggregation, Hack Prevention, Player Behavior Analysis, and Data Visibility
Monitor job health and respond to any failures or bugs so our downstream users can trust data accuracy
Help codify our knowledge, processes, pipelines, and schemas into readable documentation
We expect you to:
Have deep passion and thoughts for video games; be a gamer and think on behalf of players.
Be comfortable taking risks and aim for never-been-done engineering achievements
Enjoy working with fast-moving and rapidly-growing small teams
Must Haves:
2+ Years of Experience in Software Development, ideally in the realm of data
Experience writing clean, efficient Python, Java, or Scala code
Experience writing optimized SQL Queries for analysis and building custom data sets
Some exposure to a modern Cloud Platform, preferably AWS or GCP
Some knowledge or experience with Big Data Processing Tools
Like BigQuery, Redshift, Snowflake, Spark/Beam, Hadoop
Comfortable using Git for Source Control
An eye for data quality and the ability to build tests, reporting, and alerting mechanisms for visibility
Strong communication skills, desire to understand the data you’re using, and the ability to explain what you built, why you built it
Eager to learn any new technology and always open to jump out of your comfort zone.
Nice to Haves:
Any of the following would be highly preferred, but most of all, we value engineers who are eager to learn and contribute to the team:
Familiar with tools in the Python Data Stack (Pandas, Airflow, Flask, Plotly)
Familiar with tools in the GCP Data Stack (BigQuery, PubSub, Dataflow, Cloud Functions)
Experience with CICD or IAC (Terraform)
Experience with a data visualization tool like Tableau or Looker
Familiarity with Agile methodology
Interest in Data Science or Machine Learning
We look forward to meeting you!"
349,Data Engineer (Big Data),U.S. Bank,"Irving, TX","At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.

Job Description

Be a part of transformational change where integrity matters, success inspires and great teams collaborate and innovate. As the fifth-largest bank in the United States, we're one of the country's most respected, innovative, ethical and successful financial institutions. We're looking for people who want more than just a job - they want to make a difference! U.S. Bank is seeking a Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.

This position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.

Essential Responsibilities
Responsible for designing, developing, testing, operating and maintaining products.
Takes full stack ownership by consistently writing production-ready and testable code.
Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs.
Makes sound design/coding decisions keeping customer experience in the forefront.
Takes feedback from code review and apply changes to meet standards. Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures.
Accountable for ensuring all aspects of product development follow compliance and security best practices.
Exhibits relentless focus in software reliability engineering standards embedded into development standards.
Embraces emerging technology opportunities and contributes to the best practices in support of the bank's technology transformation.
Contributes to a culture of innovation, collaboration and continuous improvement.
Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery. Anticipates and communicates blockers and delays before they require escalation.
Basic Qualifications
Bachelor's degree, or equivalent work experience
Three to five years of relevant experience
Preferred Skills/Experience
Knowledge of Spark, Scala, Python and other programming languages
Experience in airflow, elastic, google cloud, data quality
Experience in managing big data sets
Experience in Java and object-oriented design skills
Experience building micro-services based applications
Experience in Utilizing tools such as Maven, Docker, Kubernetes, Kibana, ELK, Jenkins and frameworks such as Spring Boot to develop and deploy microservices
Thorough understanding of a feature, the users impacted, the flows impacted and feature's purpose
Technical designs are clear, well thought out, and considers dependencies, failure states, maintainability, testability and ease of support
Considers scalability, reliability and performance of systems/contexts affected when defining technical designs
Understands the team's domain, how work in this domain relates to the team's objectives and deliverables and how it contributes to overall business strategy and how technical strategy maps to this
Thoroughly understands the business model in relation to their current product focus area
Ability to analyze the bigger picture, identifying and prioritizing with the aim to consider more than one domain within an analysis
Looks for opportunities to simplify product and technical design
Adept with agile software development lifecycle and DevOps principles
Able to communicate processes and results with all parties involved in the product team, including engineers, product owner, scrum master, third party vendors and customers
Strong problem-solving and analytical skills
Excellent communication and interpersonal skills
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.

Benefits

Take care of yourself and your family with U.S. Bank employee benefits. We know that healthy employees are happy employees, and we believe that work/life balance should be easy to achieve. That's why we share the cost of benefits and offer a variety of programs, resources and support you need to bring your full self to work and stay present and committed to the people who matter most - your family.

Learn all about U.S. Bank employee benefits, including tuition reimbursement, retirement plans and more, by visiting usbank.com/careers.

EEO is the Law

Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal EEO is the Law poster.

E-Verify

U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.

Due to legal requirements, U.S. Bank requires that the successful candidate hired for some positions be fully-vaccinated for COVID-19, absent being granted an accommodation due to a medical condition, pregnancy, or sincerely held religious belief or other legally required exemption. For these positions, as part of the conditional offer of employment, the successful candidate will be asked to provide proof of vaccination or approval for an accommodation or exemption upon hire."
350,Data Engineer,Schneider Electric,"Franklin, TN","Schneider Electric has an opportunity for a Data Engineer in our Franklin, TN location.

The Data Engineer will develop, maintain and improve data delivery through the North America (NAM) Data Excellence platform. The right person in this role will support overall improvements in data architecture, metric definition and processes to support business data and analytical initiatives. Our Data Engineers provide guidance and support to team members in the Data Services team and our business partners.

Schneider Electric creates connected technologies that reshape industries, transform cities and enrich lives. Our 135,000+ employees thrive in more than 100 countries. From the simplest of switches to complex operational systems, our technology, software and services improve the way our customers manage and automate their operations. Help us deliver solutions that ensure Life Is On everywhere, for everyone and at every moment.

http://www.youtube.com/watch?v=YtExntUe89c

Great People Make Schneider Electric a Great Company.

Key responsibilities include:
Collaborate with team members to conceptualize, design and deliver enterprise and departmental data solutions to support business intelligence, data warehousing and reporting and machine learning requirements.
Implement solutions that are reliable and scalable to meet the service levels associated with mission-critical solutions.
Participate in and enhance our DevOp practice to insure highly available solutions and quick issue resolution.
Translate business requirements into data pipelines and data stores to support of business requirements.
Perform assessments (Proof of Concepts) of the latest tools and technologies.
Work with Data and Solution Architects to define and implement migration strategies from legacy systems to cloud architecture and technologies.
Provide team feedback to optimize delivery of our solutions.

Qualifications

We know skills and competencies show up in many different ways and can be based on your life experience. If you do not necessarily meet all the requirements that are listed, we still encourage you to apply for the position.

Required qualifications:
5+ years of demonstrated experience with modern programming languages such as Python, Scala or Java.
3+ years of experience developing data-related solutions on cloud platforms such as: Amazon Web Services (AWS).
Demonstrated experience with primary AWS services such as EC2, Lambda, EMR, S3, IAM policies, Cloudwatch, Cloud Formation, SES
Demonstrated experience with Cloud Services for data handling and database technologies (DMS, Kafka, Spark, Redshift, Athena, Hadoop, Airflow, etc.).
Knowledge of Data Management, Integration and Data Quality tools, such as: Alteryx, Trifacta, Informatica Power Center, Informatica Cloud, and Oracle Data Integrator.
Command of advanced SQL queries and programming.
Experience contributing to and following architecture, design and implementation best practices.
Have an eye for operational transparency and resiliency at every layer of the application.
Proven analytical and problem-solving abilities. Ability to assimilate information and quickly discern the most relevant facts and recommend creative, practical design solution. Ability to think outside-the-box a real asset.
Experience with DEVOPs tools and processes and CI/CD are an asset.
Excellent communication, presentation, influencing, and reasoning capabilities.
Desire to take the initiative moving projects/ideas forward with clarity
Leadership skills to lead and mentor cross-functional teams towards common solutions.
Knowledge of legacy data warehousing tools and technology is an asset Examples: Dimensional Models, Informatica PowerCenter, Informatica Cloud, MS Integration Services, Alteryx, Oracle, MS SQL Server etc.
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more. Click here to find out more about working with us: http://se.com/us/careers.



We seek out and reward people for putting the customer first, being disruptive to the status quo, embracing different perspectives, continuously learning, and acting like owners. We’re recognized around the world for welcoming people as they are. We create an inclusive culture where all forms of diversity are seen as a real value for the company.  See what our people have to say about working for Schneider Electric.

https://youtu.be/C7sogZ_oQYg



Let us learn about you! Apply today.



You must submit an online application to be considered for any position with us. This position will be posted until filled.

As a federal government contractor, all Schneider Electric U.S. employees (including U.S. territories and Puerto Rico) must be fully vaccinated against COVID-19, subject to federal laws.



It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct.



Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.



Schneider Electric is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.

Schedule: Full-time

Req: 007KT4"
351,"PySpark Data Engineer, Consultant",Deloitte,"New York, NY","The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Qualifications

Required:
3+ years of relevant technology consulting or industry experience to include experience in Information delivery, Analytics and Business Intelligence based on data
3+ years experience in Python and/or R
3+ years experience in SQL
3+ years experience PySpark
2+ years of hands on experience with data core modernization and data ingestion.
1+ years experience leading workstreams or small teams
Bachelor's Degree or equivalent professional experience
Travel up 50% (While 50% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice
Limited immigration sponsorship may be available.
Preferred:
An advanced degree in the area of specialization is preferred.
Experience with Cloud using Amazon Web Services (AWS), Microsoft Azure, and/or Google Cloud Platform (GCP)
Experience with Spark, Scala
Understanding of the benefits of data warehousing, data architecture, data quality processes, data warehousing design and implementation, table structure, fact and dimension tables, logical and physical database design, data modeling, reporting process metadata, and ETL processes.
Experience designing and implementing reporting and visualization for unstructured and structured data sets
Experience designing and developing data cleansing routines utilizing typical data quality functions involving standardization, transformation, rationalization, linking and matching
Knowledge of data, master data and metadata related standards, processes and technology
Experience working with multi-Terabyte data sets
Experience with Data Integration on traditional and Hadoop environments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)."
352,Data Engineer,Guidehouse,Washington DC-Baltimore Area,"Posted by
Kristen Peña
Talent Acquisition Recruiter at Guidehouse
Send InMail
Job Description
Overview

Guidehouse is a leading global provider of consulting services to the public sector and commercial markets, with broad capabilities in management, technology, and risk consulting. By combining our public and private sector expertise, we help clients address their most complex challenges and navigate significant regulatory pressures focusing on transformational change, business resiliency, and technology-driven innovation. Across a range of advisory, consulting, outsourcing, and digital services, we create scalable, innovative solutions that help our clients outwit complexity and position them for future growth and success. The company has more than 12,000 professionals in over 50 locations globally. Guidehouse is a Veritas Capital portfolio company, led by seasoned professionals with proven and diverse expertise in traditional and emerging technologies, markets, and agenda-setting issues driving national and global economies. For more information, please visit www.guidehouse.com.

Responsibilities

Our consultants on the Advanced Analytics & Intelligent Automation team help clients maximize the value of their data and automate business processes. This high performing team works with clients to implement the full spectrum of data analytics and data science, from data engineering, data querying and wrangling, to data visualization and dashboarding, to predictive analytics, machine learning, and artificial intelligence as well as robotic process automation (RPA). Our services enable our clients to define their information strategy, enable mission critical insights and data-driven decision making, reduce cost and complexity, increase trust, and improve operational effectiveness. Our consultants focus on client services, while also supporting business development, internal firm initiatives, and ongoing professional development.
Job Description/Responsibilities:
Manage ETL (Extract, Transform, Load) functions Perform complex queries using SQL and / or other tools (e.g., R, Python Pandas, PySpark)
Develop, build, test, and maintain scalable data pipeline architectures and tools
Partner with data scientists/analysts and cross-functional teams to discover, collect, cleanse, and refine the data needed for analysis and modeling
Operate in a cloud environment and assess environment specifications and requirements for big data
Work with data integration and management tools such as databases (cloud and on-prem; relational and NoSQL), Databricks and a variety of tools from major cloud providers (AWS, Azure, GCP)
Understand client needs and translate to technical requirements and Jira cards for implementation

Qualifications

Required:
US Citizenship and ability to obtain a Public Trust clearance
B.S./B.A. in Data Science, Computer Science, Information Systems, or other computational discipline OR B.S./B.A. along with a professional development/bootcamp on data engineering
3+ years of relevant professional experience
Experience developing data pipelines / ETLs and performing data engineering using Spark and Databricks
Experience performing data engineering to enable data science and machine learning
Experience in Python or Scala
Understanding relational databases, columnar data warehouses, NoSQL, and other storage types
Proficiency in SQL
Ability to instrument basic automation and CI/CD
Familiarity with cloud-based services (AWS, Azure, GCP)
Knowledge/experience designing and building distributed systems for scalability and security
Understanding of Agile development methodologies
Preferred:
M.S./M.A. in Data Science, Computer Science, Information Systems, or other computational discipline
5+ years of relevant professional experience
Experience with micro services architecture and API gateways
Ability to work independently or in a collaborative team
Supporting clients: identifying and addressing needs, building relationships, being flexible, communicating effectively
Business development including RFP/RFQ/RFI responses
Knowledge of Public Sector domains (e.g., National Security, Defense, Public Health, etc.)

Additional Requirements

The successful candidate must not be subject to employment restrictions from a former employer (such as a non-compete) that would prevent the candidate from performing the job responsibilities as described.

Disclaimer

About Guidehouse
Guidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.
Guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the Fair Chance Ordinance of Los Angeles and San Francisco.
If you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.
Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse and Guidehouse will not be obligated to pay a placement fee.
Rewards and Benefits
Guidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace.
Benefits include:
Medical, Rx, Dental & Vision Insurance
Personal and Family Sick Time & Company Paid Holidays
Position may be eligible for a discretionary variable incentive bonus
Parental Leave and Adoption Assistance
401(k) Retirement Plan
Basic Life & Supplemental Life
Health Savings Account, Dental/Vision & Dependent Care Flexible Spending Accounts
Short-Term & Long-Term Disability
Tuition Reimbursement, Personal Development & Learning Opportunities
Skills Development & Certifications
Employee Referral Program
Corporate Sponsored Events & Community Outreach
Emergency Back-Up Childcare Program"
353,Data Engineer,Amazon,"San Diego, CA","Description

The Buyer Abuse Prevention team's mission is to combine advanced analytics with investigator insight to create mechanisms to proactively and reactively reduce the impact of abuse across Amazon.

A day in the life

As a Data Engineer, you will be part of modeling complex problems, discovering insights, and building cutting edge risk algorithms that identify opportunities through statistical models, machine learning, and visualization techniques to improve operational efficiency and reduce monetary losses and improve customer trust.

You will need to collaborate effectively with business and product leaders and cross-functional teams to build scalable solutions against high organizational standards.

About The Hiring Group

Buyer Abuse Prevention is part of Amazon’s Buyer Risk Prevention's (BRP) organization, whose mission is to make Amazon the safest and most trusted place worldwide to transact online. BRP safeguards every financial transaction across all Amazon sites. As such, BRP designs and builds the software systems, risk models, and operational processes that minimize risk and maximize trust in Amazon.com.

Job responsibilities

Design and deliver big data architectures for experimental and production consumption between scientists and software engineering.

Discover, explore, and onboard new data sources to improve the performance of ML models.

Develop model features in a scalable and maintainable way.

Create automated alarming and dashboards to monitor data integrity.

Create and manage capacity and performance plans.

Act as the subject matter expert for the data structure and usage.


Basic Qualifications
5+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Excel in the design, creation, and management of very large datasets
Skilled with writing, tuning, and troubleshooting SQL queries
Experience with Big Data technologies such as Hive, Spark, Hadoop, NoSQL, AWS EMR, Glue, Lambda, Kinesis, Redshift
Proficiency with Python, Java, or Scala (Scala preferred)
Excellent grasp of software development life cycle and/or agile development environment
Strong organizational and planning skills with attention to detail
Experience in understanding system limitations, scaling factors, boundary conditions, and the reasons for architectural decisions
Experience in Designing and building scalable data pipelines

Preferred Qualifications

Experience working with ML solutions and real-time inference pipelines


Company - Amazon.com Services LLC

Job ID: A1761931"
354,Data Engineer,TripActions,"Salt Lake City, UT","Posted by
George Sleiman
Director, Global Talent @ TripActions
Send InMail
At TripActions, “It’s all about the user. All of them.” We’re passionate about providing a seamless one-stop experience for business travelers, no matter how they travel, where they stay, or where they’re going.

TripActions is looking for a Data Engineer who will be working with the data team to help build infrastructure, tooling, and data pipelines to power our data-driven organization and support our rapidly evolving and growing data needs. The ideal candidate will be extremely curious and will use their data skills and business mindset to make a difference every day. We are looking for people who can operate at a company that grows as fast as ours by dealing with multiple moving pieces while still holding up quality, long-term thinking, and delivering value to our customers. We take great pride in our diversity of backgrounds, cultures, and perspectives and we strongly believe this is helping us to grow as a successful and impactful team. To learn more about our team, you can watch this recent talk on the types of problems we work on and this one on how we work together as a group .

What You'll Do
Working closely with the data science and BI teams, design and create engineering products to empower them to work independently. Examples:
Tooling and interfaces
Deployment and testing of infrastructure
Libraries
Workflow automation
Develop, maintain and monitor solutions for data ingestion
Design and implement multi-cloud data infrastructure to support the broader TripActions business
Productionalize machine learning (micro) services
Develop our internal tooling library used by the team
Set best practices and coach analysts and data scientists to become better engineers
What We're Looking For
3+ years of work experience in software engineering or data engineering
Strong Python skills, Java and/or Javascript experience is a plus
A good understanding of web frameworks and networking
Experience with AWS/Azure/GCP
Comfortable with writing SQL
Experience designing, building, and maintaining data processing systems
Experience working with analytical databases (Snowflake, Vertica, Redshift) is a plus
COVID-19 Vaccine Policy - US ONLY

To support the health and safety of our teams and work spaces, we require employees to be fully vaccinated against COVID-19 or to have received an approved accommodation based on medical condition or sincerely held religious belief or practice. Please let us know if you’d like to discuss the policy or available accommodations before proceeding with our recruitment process.

About TripActions

TripActions is the only all-in-one travel, corporate card, and expense solution, providing 8,800+ customers around the globe unprecedented visibility and control over spend. Trusted by finance teams and travelers alike, TripActions leverages real-time data to help companies keep traveling employees safe, reduce spend, and drive productivity.

TripActions’ investors include such visionaries as Andreessen Horowitz, Lightspeed Ventures, Greenoaks, Zeev Ventures, and entrepreneurs Lee Fixel, Adam Bain, and Elad Gil. In October 2021, TripActions announced its Series F funding of $275M at an upround post-money valuation of $7.25B to help accelerate future growth plans. In February 2022, the TripActions Group announced its acquisition of Berlin-based travel management company, Comtravo, which was closely followed by the addition of leading Scandinavian travel agency Resia AB, the group’s third acquisition in nine months, following the acquisition of Reed & Mackay in May of 2021.

At TripActions, we’re never satisfied with the status quo, and we know breakthrough ideas come from diverse perspectives. We are committed to cultivating a workplace that reflects the diversity of the customers we serve while fostering leadership and innovation. All voices are valued here and you’ll have the resources, tools, and training you’ll need to do the best work of your life.

Our Benefits

We realize benefits are important as they support keeping you at your best at all times. Our benefits are here for you if you get sick or hurt, help you save for now and later, encourage you to take time off work and travel, and provide perks specific to being a TripActions employee both in and out of the office.

Equal Opportunity

TripActions is an equal opportunity employer. We make all employment decisions based solely on merit. We provide equal employment opportunity to all applicants and employees without discrimination on the bases of race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We prohibit any such discrimination or harassment. This policy applies to all terms and conditions of employment, including hiring.

Accommodations

TripActions complies with the Americans with Disabilities Act (ADA), as amended by the ADA Amendments Act, and all applicable state or local law. TripActions will reasonably accommodate qualified individuals with a disability in connection with applications for employment as required by law.

If you need any assistance or accommodations due to a disability, you are welcome to email us at talent-accommodations@tripactions.com .

Candidate Privacy Notice

Please review TripActions' Candidate Privacy Notice here ."
355,"Data Engineer, Alexa Information Analytics",Amazon,"Santa Barbara, CA","Job Summary

DESCRIPTION

Alexa is Amazon’s groundbreaking virtual assistant designed for voice interactions. We believe voice is the most natural interface for interacting with technology across many domains. We are looking for a Data Engineer (DE) to join our Analytics team located in beautiful Santa Barbara, CA.

If you love building scalable data infrastructure and machine learning applications, this role is for you. Your work will empower data engineers, knowledge engineers, business intelligence engineers, business analysts, product managers, and fellow SDEs to make Alexa smarter. You will take complete ownership over feature design, prioritization, and implementation. You will commit code several times a day. You will launch fast and iterate quickly to build amazing features at mind-numbing speeds.

Key job responsibilities
You love building tools and data pipelines, can create clear and effective reports and data visualizations, and can partner with stakeholders to answer key business questions. You will also have the opportunity to display your skills in the following areas:
Design, implement, and automate deployment of our distributed system for collecting and processing log events from multiple sources
Design data schema and operate internal data warehouses and SQL/NoSQL database systems
Write Extract-Transform-Load (ETL) jobs and Spark/Hadoop jobs to calculate business metrics
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions
Monitor and troubleshoot operational or data issues in the data pipelines
Drive architectural plans and implementation for future data storage, reporting, and analytic solutions
A day in the life
You will be partnering with other Data Engineers and Business Intelligence Engineers and our internal business stakeholders to develop scalable data pipelines and metrics which improve their productivity
About The Team
We enable the Knowledge Domain within Alexa to make informed decisions and increase productivity by creating scalable data pipelines, reliable and secured data stores, reusable datasets, accurate metrics, self-serve reports, and actionable analyses that support cross-Information use cases
Crosss-Functional Team consists of Data Engineers, Business Intelligince Engineers, Software Development Engineer an Product Manager

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
Graduate degree in Computer Science, Mathematics, Statistics, Finance, related technical field
3+ years of experience in implementing big data processing technology: Hadoop, Apache Spark, etc.
Experience in writing and optimizing SQL queries in a business environment with large-scale, complex datasets
Strong ability to effectively communicate with both business and technical teams
Demonstrated experience delivering actionable insights for a consumer business
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)
Experience with AWS technologies including Redshift, RDS, S3, EMR
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1952821"
356,"Data Engineer, Spark",Deloitte,"Pleasanton, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
357,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Kollasoft Inc., is seeking the following. Apply via Dice today!

Top Skills' Details 5+ years of work experience in software design and implementation using Java or Scala. Experience in data processing using Spark. Experience in designing, implementing, and operating any of the NoSQL databases such as Cassandra, Elasticsearch, Couchbase, Redis."
358,Cloud Data Engineer,Intel Corporation,"Phoenix, AZ","Job Description

The mission of Intel's Incubation Disruptive Innovation (IDI) team is to create an environment to identify new opportunities for innovation and disruptive technologies as a path to create new markets and new organizational capabilities leveraging Intel's competitive advantages.

This innovative environment and start-up-oriented culture is a critical engine for Intel's future growth. Borne out of IDI, OmniBridge is a team of innovators using cutting edge AI to develop an innovative communication tool that goes beyond a simple American Sign Language translation app by creating immediate and seamless conversation between the hearing and the deaf. You will develop cloud applications to collect, process and visualize data sets to support AI Research experiments while reporting to our product lead and data engineering manager.

Responsibilities Include
Design, build and deliver high quality data architecture and pipelines to support experiment design and reporting needs.
Manage AWS resources, frameworks and technologies including EC2, RDS, DynamoDB, Batch, Lambda etc. used to deploy, run, optimize, and scale cloud software, including microservices, containers, orchestration, runtimes, virtualization, networking, storage, resource management, telemetry, observability, security, FaaS, and automation.
Partner with Intel engineering teams, the open-source community and other technology teams to extract, transform, and load data from a wide variety of data sources.
Optimize reporting and analysis processes, automating or simplifying self-service support for decision-makers.
Collaborate with technical and non-technical users to tackle business requirements and deliver against high operational standards of system availability and reliability.
Design and develop monitoring dashboards and creation of critical alarms for the system.
Develops and coordinates cloud architecture over various areas, including application development, identity and access management, network and data management, and security.
The Successful Candidate Must Demonstrate The Following
Excellent written communication skills.
Proven success in communicating with users, other technical teams, and senior management with collecting requirements, describing data modeling, influencing data modeling decisions and defining data engineering strategy.
Qualifications

Minimum Education:
Bachelor's degree in Computer Science, Computer Engineering, Technology, Computer Information Systems, or Mathematics/Statistics or other technical discipline.
Minimum Qualifications
4+ years of experience as a Data Engineer, Cloud Application Development Engineer, Cloud Software Engineer or in a similar role.
2+ years of experience with data modeling, data warehousing, and building ETL pipelines.
2+ years of experience in architecting, designing and optimization of data-processing systems using AWS services/frameworks such as (S3, EMR, Lambda, Glue, Athena, SNS, Cloud Watch, Redshift, Aurora/RDS).
4+ years of hands-on experience using SQL with large data sets, as well as creating and maintaining databases.
4+ years of programming skills (Python, HTML, Java, JavaScript).
Preferred Qualifications
Experience utilizing HTML, JavaScript libraries and Python.
Inside this Business Group

The mission of the Emerging Growth and Incubation (EGI) group is to create an environment to identify, incubate and scale new billion-dollar businesses leveraging Intel's competitive advantages. EGI is a fast paced, dynamic and startup-like organization that is responsible for incubating new ideas within Intel. We have over 1500 employees globally. Our portfolio includes HW, SW, Solutions and SaaS businesses.

Other Locations

US, California, Folsom;US, California, Santa Clara;US, Oregon, Hillsboro

Intel strongly encourages employees to be vaccinated against COVID-19. Intel aligns to federal, state, and local laws and as a contractor to the U.S. Government is subject to government mandates that may be issued. Intel policies for COVID-19 including guidance about testing and vaccination are subject to change over time.

Posting Statement

All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance."
359,"Associate Data & ML Engineer, A2C Program",Amazon Web Services (AWS),"Delaware County, PA","Job Summary

DESCRIPTION

AWS Professional Services Accelerate to Consultant (A2C) is an accelerated career development program for college graduates and technical professionals early in their post-graduate careers. The program consists of self-paced technical curriculum, regionally-based classroom trainings, AWS certification training, shadowing, and hands-on billable project work.

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

Key job responsibilities

AWS Professional Services Accelerate to Consultant (A2C) is an accelerated career development program for college graduates and technical professionals early in their post-graduate careers. The program consists of self-paced technical curriculum, regionally-based classroom trainings, AWS certification training, shadowing, and hands-on billable project work.

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications
Bachelor’s degree in Computer Science, Engineering, - - Mathematics or a related field or equivalent professional or military experience
1+ years of experience of Data platform implementation
1+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation
Preferred Qualifications
Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
2+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
AWS Certification(s) such as Solutions Architect Associate and/or Data Analytics Specialty
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A2032432"
360,Data Engineer - Opportunity for Working Remotely,VMware,"Dallas, TX","The Elevator Pitch: Why will you enjoy this new opportunity?

You want to be a part of an innovative company of 20000+ people working in 50+ locations worldwide and committed to building a community where great people want to work long term by living our values of passion, innovation, execution, teamwork, active learning and giving back. If you are ready to accelerate, innovate and lead, join us as we challenge constraints and problem solve for tomorrow today.

You are highly motivated and would love to be part of VMware Data Engineering team working to solve complex business problems and bring digital transformations

What is primary need, technical challenge, and/or problem you will be responsible for?

VMware Data Engineering team is seeking a highly motivated, experienced Data Engineer within the IT Data Engineering and Analytics group. This position is responsible for hands on development work on all aspects of Data Engineer, data provisioning, modeling, performance tuning and optimization. The candidate will work closely with both Enterprise and Solution Architecture teams to translate the Business/Functional requirements into technical specifications that drive Hadoop/HANA/BI solutions to the meet functional requirements.

Success in the Role: What are the performance goals over the first 6-12 months you will work toward completing?

Within the first few months you will spend time learning VMware’s coding standards, products, and increasing you know how of the technology landscape around data. We want you to be curious, learning both from team members and individual study. You will collaborate with other team members and participate in architecture reviews.

You will closely work with other data product owners/engineers towards taking ownership of few existing artifacts within the data landscape. You will be required to help in troubleshooting any upcoming production defects and perform production support. You will also work on delivering specific enhancements in an agile delivery model

What type of work will you be doing? What assignments, requirements, or skills will you be performing on a regular basis?

You will work in a fast paced and agile work environment.
You will communicate and engage with a range of stakeholders.
You will be responsible for hands on development work building scalable Data engineering pipelines and other data engineering/modelling work using one or more of Python, Kafka, Hadoop/Hive, Presto etc.
You will have to query data using SQL or other techniques. Excellent SQL & Analytical SQL functions knowledge will be needed
Understanding of SAP HANA and Knowledge of Data Integration Platforms - Informatica PowerCenter, SAP BODS, SDI, SLT (is desired but not mandatory) and will help you in understanding existing landscape
Bachelor’s degree or equivalent with 4+ years of Data Engineering experience in Big Data Solutions is required for this role. MS degree would be highly desirable
You will be owner of specific modules. You will collaborate with other team members on improving dev practices, do peer code reviews and provide production support

What is the leadership like for this role? What is the structure and culture of the team like?

This role reports into Sapan Bajpai who is a Senior Manager for IT Data Engineering and Analytics.

IT Data Engineering and Analytics team is spread across VMware offices in Bangalore, Chennai, Palo Alto(USA) , Austin(USA) , Cork(Ireland), Beijing(China) and Costa Rica

The team is headed by Director, IT Data Engineering and Analytics based in Palo Alto

What are the benefits and perks of working at VMware?

You and your loved ones will be supported with a competitive and comprehensive benefits package. Below are some highlights, or you can view the complete benefits package by visiting www.benefits.vmware.com.

Employee Stock Purchase Plan
Medical Coverage, Retirement, and Parental Leave Plans for All Family Types
Generous Time Off Programs
40 hours of paid time to volunteer in your community
Rethink's Neurodiversity program to support parents raising children with learning or behavior challenges, or developmental disabilities
Financial contributions to your ongoing development (conference participation, trainings, course work, etc.)
Healthy and local inspired snacks in all our on-site pantries

Category : Engineering and Technology

Subcategory: Software Engineering

Experience: Manager and Professional

Full Time/ Part Time: Full Time

Posted Date: 2022-04-19

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
361,Data Engineer,"TrueCar, Inc.","Santa Monica, CA","Job Description:

TrueCar envisions a world where car shopping is an uplifting experience. Our shopping experience helps buyers consider choices from every angle, builds confidence in their decisions, and enables every step of the process with tools and information that make car buying easy. Ultimately, TrueCar is helping people in the second largest purchase they will make in their lives. We're removing the complexity out of buying a car, using technology and personalization, to create a one-of-a-kind experience that transforms car buying and ultimately people’s lives.

Come join the team and help us accomplish our mission. TrueCar maintains a Dynamic Workplace, allowing employees to have their primary workstations at home, with office space in Santa Monica, CA and Austin, TX to be made available to individuals and teams to use as needed. Employees enjoy excellent benefits (health/vision/dental coverage, 401k with contribution matching, equity, etc.) as well as perks like monthly credits for at-home food delivery, internet/mobile phone service coverage, fitness expenses, and Caregiver support.

About the Team:

The Data Engineering team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.



About the Job:
Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
Build complex workflows and orchestrate data dependencies.
Monitor and support data pipelines to honor internal and external SLA’s.
Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.
Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
Working with a team of engineers where mentorship is valued.
Ability to learn and adapt to continually evolving technologies in the big data ecosystem.
What You'll Need:
5+ years of experience programming in Java.
2+ years of experience in Big Data technologies.
Experience in any of big data technologies: MapReduce, Spark, HBase,
Proficient in SQL and experience with RDBMS/NoSQL databases.
Experience working with Cloudera/Hortonworks/EMR distribution in AWS.
Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work.
Team-player with strong collaboration and communication skills, who is able to respond positively to feedback.
Bachelor degree (or Master) in Computer Science or related engineering field
TrueCar is headquartered in Santa Monica, California, with an office in Austin, Texas.

Location(s):

Santa Monica, CA"
362,Data Engineer with Hadoop/python,Dice,"Austin, TX","Dice is the leading career destination for tech experts at every stage of their careers. Our client, iQuest Solutions Corp, is seeking the following. Apply via Dice today!

Job Title Data Engineer Data Scientist Location Austin, TX or Santa Clara, CA Duration 12+ Months Its Day 1 Onsite project. Requirements Hadoop Data Engineer, Python, PySpark, Spark and SQL"
363,"Data Engineer, Data Center Engineering Analytics",Amazon Web Services (AWS),"Herndon, VA","Job Summary

DESCRIPTION

Description

Amazon is looking for a highly analytical Data Engineer to join the AWS Data Center Engineering team. The Data Center Engineering team owns the availability of AWS data centers, and has a direct impact on the customer experience. We obsess over customers by developing the most advanced engineering solutions that result in world class uptime, while continuously reducing costs for our customers. Amazon offers a fast paced, fun, and exciting work environment. We continue to grow at exponential rates and are looking for individuals that can support our speed to market, enjoy a challenge, and have a desire for professional growth and continuous learning experiences.

As a Data Engineer, you will partner with Business Intelligence Engineers and partner teams to build data pipelines and solutions to harness the vast amount of operational data from the AWS fleet. You will own the timely delivery of such data for use in downstream business intelligence solutions, as well as all necessary actions to ensure the reliability of data provided for business decision making. Data analysis is at the core Amazon’s culture, and your work will have a direct impact on decision making and strategy for our organization.

The ideal candidates will have excellent analytical abilities, intense curiosity, and strong technical skills. They will have a strong bias toward data driven decision making, and building scalable data pipelines and systems to facilitate such decision making. They will be a self-starter; comfortable with ambiguity; able to think big and be creative, while exercising strong judgment and good instincts to be right a lot. The ideal candidate is motivated by delivering high-quality and innovative solutions within timeframes that most think are impossible. If you are excited about using data to look around corners and drive engineering solutions that are the foundation of AWS data centers, this role is for you!

Key job responsibilities

Responsibilities
Collaborate with Business Intelligence Engineering team members, engineering stakeholders, partner technical teams, and business stakeholders, to gather business and functional requirements, and translate these requirements into a robust, scalable, and operable data infrastructure that works well within the overall AWS data architecture, and leads to improved engineering decisions.
Develop a deep understanding and awareness of operational data from the AWS fleet, and build mechanisms for retrieving and aggregating such data for use by downstream business intelligence solutions.
Develop a deep understanding of our vast data sources, and provide continuous recommendations for use to solve specific business problems.
Take ownership of data reliability by, among other things, performing deep-dives to find root causes of potential data anomalies, and taking subsequent action to address these anomalies.
Continuously optimize the performance of data queries, and address extract, transform, load (ETL) procedures.
Insist on the highest standards by recognizing and adopting best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.

Basic Qualifications
Bachelor’s Degree in Computer Science, Information Systems, Data Analytics, or related technical/engineering field
3+ years Structured Query Language (SQL) experience
2+ years of experience with Python or other relevant scripting language
2+ years of experience with data warehouse technical architecture, infrastructure components, and extract, transform, load (ETL) procedure
1+ years of experience with the AWS tech stack – Glue, Redshift, EMR, S3, EC2, and Lambda will be used regularly in this role
Preferred Qualifications
1+ years of experience with Data Architecture and Design
1+ years of experience in preparing data for direct use in visualization tools, such as Salesforce, Tableau, or Amazon QuickSight
Expert-level knowledge of SQL
Proficient in Scala/Spark/Hadoop
Experience in documenting technical/data systems for technical and business leaders
Experience working with data scientists on research and machine learning problems
Be self-driven, detail-oriented, and show ability to deliver on ambiguous projects with incomplete or dirty data
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A2005567"
364,"Data Engineer, Spark / Scala (Remote) US",Curinos,"New York, NY","We are looking for a Scala Developer (Apache Spark) to work at the intersection of data engineering, software engineering and machine learning on Curinos’ award-winning data/analytics cloud platform. In this role, you will work with a cross-functional team to advance the roadmap of our proprietary innovative technology built on Apache Spark.


Job responsibilities include:

Engage with product owner to understand requirements about new features
Design and develop foundational features as part of an agile development lifecycle
Undertake R&D on new technologies and solutions with engineering leadership
Identify new technologies and tools which could improve the existing SDLC process
Support complex business logic encoded into the data/analytics platform
Conduct peer code reviews
Support continuous build and release process




Qualifications

Bachelor’s degree, preferably in computer science, or quantitative field required
Strong OOP and design patterns knowledge
3+ years of commercial development experience in Scala and Python
2+ years of experience with Apache Spark including understanding on how these technologies work under the hood
Experience with Databricks is a big plus
Strong experience in distributed big data processing (batch/offline, Terabytes+)
Experience working with AWS technologies
High degree of interest in analytics and machine learning
Rigor in high code quality, unit testing, and other engineering best practices
Strong communication, collaboration and interpersonal skills
Solid verbal and written communication skills
Ability to learn quickly
Experience working in Agile environment
Interest in the FinTech space


Additional Information

All candidates must possess work authorization which does not (and will not in the future) require work sponsorship by an employer.

We are proud to be an Equal Opportunity Employer. Please visit www.curinos.com for more information"
365,Data Engineer - Advertising Analytics Data Pipeline,Amazon,"Boulder, CO","Job Summary

DESCRIPTION

Are you passionate about using Big Data to build customer trust and grow new business? Global advertisers rely on our team's performance insights to drive future investment in Amazon's Advertising Platform and improve the relevance of ads shown to customers. We are looking for passionate Data Engineers to own and optimize the big data pipeline that consumes the massive data sources we require to generate unique insights. Data is at the center of every product we will develop as we create brand new systems that serve the needs of our large and growing base of advertisers.

We are open to hiring for this position in Seattle or New York City.

You will share in the ownership of the technical vision and direction for advanced analytics and insight products. You will be a part of a team of top notch technical professionals developing complex systems at scale and with a focus on sustained operational excellence. Members of this team will be challenged to innovate using big data technologies. We are looking for people who are motivated by thinking big, moving fast, and changing the way customers use data to drive profitability. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you!

Amazon is well positioned to grow its share of a fast growing online advertising industry due to its unique assets - e-commerce data, service oriented architecture, and startup culture. Be part of a team of industry leading experts that builds and operates one of the largest big data analytics platform at Amazon. Amazon is applying the latest machine learning and big data technologies available to change the way marketers purchase, track, measure, and optimize their advertising spend. We apply these technologies on terabytes of data (over 10B new events per day) and operate clusters that push scalability limits of the existing technologies. We seek to measure every possible signal indicating impact of advertising to provide the most objective result of marketing spends.

The pay range for this position in Colorado is $113,000 - $160,000 [yr]; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
5+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, Presto, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficiency in, at least, one modern scripting or programming language such as Python, NodeJS, Java, or Scala.
Preferred Qualifications
Experience working with and tuning AWS big data technologies (EMR, Spark/Hive, S3). Experience working with SQL based systems and building data pipelines.
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
#adsto #madsjob

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1778317"
366,Data Engineer,TripActions,"Seattle, WA","Posted by
George Sleiman
Director, Global Talent @ TripActions
Send InMail
At TripActions, “It’s all about the user. All of them.” We’re passionate about providing a seamless one-stop experience for business travelers, no matter how they travel, where they stay, or where they’re going.

TripActions is looking for a Data Engineer who will be working with the data team to help build infrastructure, tooling, and data pipelines to power our data-driven organization and support our rapidly evolving and growing data needs. The ideal candidate will be extremely curious and will use their data skills and business mindset to make a difference every day. We are looking for people who can operate at a company that grows as fast as ours by dealing with multiple moving pieces while still holding up quality, long-term thinking, and delivering value to our customers. We take great pride in our diversity of backgrounds, cultures, and perspectives and we strongly believe this is helping us to grow as a successful and impactful team. To learn more about our team, you can watch this recent talk on the types of problems we work on and this one on how we work together as a group .

What You'll Do
Working closely with the data science and BI teams, design and create engineering products to empower them to work independently. Examples:
Tooling and interfaces
Deployment and testing of infrastructure
Libraries
Workflow automation
Develop, maintain and monitor solutions for data ingestion
Design and implement multi-cloud data infrastructure to support the broader TripActions business
Productionalize machine learning (micro) services
Develop our internal tooling library used by the team
Set best practices and coach analysts and data scientists to become better engineers
What We're Looking For
3+ years of work experience in software engineering or data engineering
Strong Python skills, Java and/or Javascript experience is a plus
A good understanding of web frameworks and networking
Experience with AWS/Azure/GCP
Comfortable with writing SQL
Experience designing, building, and maintaining data processing systems
Experience working with analytical databases (Snowflake, Vertica, Redshift) is a plus
COVID-19 Vaccine Policy - US ONLY

To support the health and safety of our teams and work spaces, we require employees to be fully vaccinated against COVID-19 or to have received an approved accommodation based on medical condition or sincerely held religious belief or practice. Please let us know if you’d like to discuss the policy or available accommodations before proceeding with our recruitment process.

About TripActions

TripActions is the only all-in-one travel, corporate card, and expense solution, providing 8,800+ customers around the globe unprecedented visibility and control over spend. Trusted by finance teams and travelers alike, TripActions leverages real-time data to help companies keep traveling employees safe, reduce spend, and drive productivity.

TripActions’ investors include such visionaries as Andreessen Horowitz, Lightspeed Ventures, Greenoaks, Zeev Ventures, and entrepreneurs Lee Fixel, Adam Bain, and Elad Gil. In October 2021, TripActions announced its Series F funding of $275M at an upround post-money valuation of $7.25B to help accelerate future growth plans. In February 2022, the TripActions Group announced its acquisition of Berlin-based travel management company, Comtravo, which was closely followed by the addition of leading Scandinavian travel agency Resia AB, the group’s third acquisition in nine months, following the acquisition of Reed & Mackay in May of 2021.

At TripActions, we’re never satisfied with the status quo, and we know breakthrough ideas come from diverse perspectives. We are committed to cultivating a workplace that reflects the diversity of the customers we serve while fostering leadership and innovation. All voices are valued here and you’ll have the resources, tools, and training you’ll need to do the best work of your life.

Our Benefits

We realize benefits are important as they support keeping you at your best at all times. Our benefits are here for you if you get sick or hurt, help you save for now and later, encourage you to take time off work and travel, and provide perks specific to being a TripActions employee both in and out of the office.

Equal Opportunity

TripActions is an equal opportunity employer. We make all employment decisions based solely on merit. We provide equal employment opportunity to all applicants and employees without discrimination on the bases of race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We prohibit any such discrimination or harassment. This policy applies to all terms and conditions of employment, including hiring.

Accommodations

TripActions complies with the Americans with Disabilities Act (ADA), as amended by the ADA Amendments Act, and all applicable state or local law. TripActions will reasonably accommodate qualified individuals with a disability in connection with applications for employment as required by law.

If you need any assistance or accommodations due to a disability, you are welcome to email us at talent-accommodations@tripactions.com .

Candidate Privacy Notice

Please review TripActions' Candidate Privacy Notice here ."
367,Senior Data Engineer,Grindr,"Chicago, IL","We’re looking to add a Senior Data Engineer to our Data Platform team. We’re in the process of modernizing our Data Platform with the goal of making data a strategic asset to all aspects of Grindr’s business. We recently finished a lift-and-shift of our PB-scale Data Lake to Snowflake and we’re beginning a next phase of redesigning our pipelines and data models. Goals broadly include near-real time product analytics, ML feature engineering, automated data quality alerts, a common event taxonomy and AB testing. Along the way we’re eliminating redundant code and streamlining processes.

Data Engineering has visibility and exposure to a broad set of crucial business functions at Grindr. The Data Platform team supports Data Engineering for all of Grindr by collaborating with product, engineering, and embedded analytics teams while collaborating with a core analytics team on the Data Platform.

The Data Platform is composed of 3 pods generally: Data Engineering and Core Product Analytics, Data Applications, and Data Science / ML Ops. Data Engineers coordinate across these pods. For example, help us upgrade our event firehose by migrating our 100K events-per-second firehose to a Kafka-based system. Build deep expertise in Airflow and Snowflake tasks as you write SQL that mutates trillions of records. Help develop and curate a new Data Catalog.

What’s the job?
Design, develop and deliver data pipelines and datasets to production, considering internal data governance, security and scale
Develop near real-time data pipelines using change-data-capture and event-based processing.
Maintain and enforce the business contracts on how data should be represented and stored.
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What We’ll Love About You
Significant experience with dimensional data modeling for a Data Lake / Warehouse
Expert in ETL and ELT (managing high-quality reliable ETL pipelines)
Experience with relational databases and expert in SQL development
5+ years of experience working with data at scale or complex data analysis
7+ years experience with Python or equivalent
Experience with agile engineering practices such as TDD, Continuous Integration, automated testing, and deployment.
Familiarity with legal compliance (with data management tools) data classification, and retention.

We’ll really swoon if you have
Experience as a Data Engineer using Snowflake or Data Bricks
Previous business experience working with customers around data-centric processes and analysis
Experience with complex business change management and process implementations

What You’ll Love About Us
Global impact. Grindr is the world leading LGTBQ+ social networking service. Your role will impact the lives of millions of LGTBQ+ people around the world.
High Growth. Like the company itself, this role offers significant room for growth and development
Remote First: We have offices in LA, NYC, SF, Chicago, Toronto and Taipei, though the company is set-up not just as remote-friendly, but remote-first. More than 30% of our employees work outside the cities where we have offices. Today, we are set-up to hire easily in the US, Canada and Taiwan. If you live outside one of these countries, just let us know so we can make sure we can support you.
Comp: Industry competitive compensation, including equity for all employees
Benefits: Unlimited vacation policy
Family insurance: We provide great insurance coverage for health, dental and vision for you and your family.
Retirement Savings: Generous 401(k) plan
Other Perks: home-office stipend, many company-sponsored events

About Grindr

Since launching in 2009, Grindr has grown into the world’s largest social networking app for gay, bi, trans, and queer people. We have millions of daily users who use our location-based technology in almost every country in every corner of the planet.

Today, Grindr proudly represents a modern LGBTQ+ lifestyle that expands into new platforms. From social issues to original content, we continue to blaze innovative paths with a meaningful impact for our community. At Grindr, we create a safe space where you can discover, navigate, and get zero feet away from the queer world around you.

As of June 2020, Grindr has new owners with a track record of multiple successful Bay Area start-ups. The new leadership is demonstrating a renewed commitment to creating an experience for users that is safe, fun, and productive, as well as a positive & uplifting company culture in which everyone can be their best selves. At the heart of Grindr’s mission in this new chapter is a shared set of core values including transparency, accountability, experimentation (failing fast), and strong allegiance to the LGBTQ+ community.

Grindr is an equal opportunity employer"
368,Data Engineer,Cincinnati Children's Hospital Medical Center,"Cincinnati, OH","SUBFUNCTION DEFINITION: Focuses on how to design, integrate, and manage complex data and analytic systems over their life cycles. Uses a combination of core software engineering principles and domain specific data and analytic knowledge to ensure the enterprise as seamless access to actionable, meaningful and well-governed data across all domains.

CCHMC SALARY GRADE:10

Representative Responsibilities
Data Pipelines
Build, test and manage simple data pipelines from data sources or endpoints of acquisition to integration to consumption for production for key data and analytics consumers like business/data analysts, data scientists, etc. Comply with data governance and data security requirements while creating, improving and operationalizing data pipelines, following standards set by more senior data and platform engineers. Perform maintenance changes and updates to ETL processes and support upgrade and testing initiatives as necessary Understand bench-marking and process improvement data requirements and develop solutions to address these requests.
Metadata Management & Data Modeling
Follow team standards for managing metadata to ensure data is used in the right business context and with minimal data duplication. Assist in curation of new data needs, business context association, or sensitivity analysis. Ensure the governance lifecycle for all onboarded data and change workflows as data or business context changes. Update documentation of data models and extract processes. Update support documentation so teams can be cross trained to support users and processes. Modify, test and deploy updates to data models under direction and guidance from senior staff. Work with reporting teams to help them develop best practices approaches to writing and tuning new reporting objects.
Technical & Business Skill
Foundational knowledge of several Data Management practices and architectures, such as Data Modelling, Data Warehousing, Data Lake, Data Hub, etc. and foundational understanding of the others. Proficiency with SQL, object-oriented/object function scripting and DevOps principles. Develop understanding of core CCHMC clinical, business and research processes to help build appropriate data solutions. Obtain Epic certifications as appropriate/needed.
Technical Support & Customer Services
Ensure outstanding end-user support is provided, including ongoing monitoring of Service Level Agreements for incident management and collaboration with other areas to ensure customer-centered incident management and support. Adhere to change management policies and procedures. Model outstanding customer service behavior, including timely and effective follow-up with customers. Work with vendors when necessary to ensure CCHMC investments and requests are being adequately supported and enhanced. Escalate support issues with urgency. Take 24 hour call on a staff rotation.
Project Execution
Execute own project tasks with urgency and to a high level of quality. Communicate status clearly and effectively using departmental project management tools. Follow time-tracking and other project management requirements.

Required

EDUCATION/EXPERIENCE
Bachelor's degree in a computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field.
No directly related experience
Preferred

Unique Skills:

Job

Information Technology

Primary Location

United States-Ohio-Cincinnati-Vernon Place

Schedule

Full-time

Shift

Day Job

Job Type

Standard

Department

Information Services

Employee Status

Regular

FTE

1.0

Weekly Hours

40

Salary Range

26.80"
369,"Data Engineer, Snowflake",Deloitte,"Bethesda, MD","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
370,Data Engineer,MissionWired,United States,"At MissionWired, we help clients create revolutionary digital strategies that advance their mission, change our country, and have a positive impact on the world.

We’re digital-obsessed, tech-savvy do-gooders who care deeply about social change. We’ve brought digital strategies to life for nonprofit organizations working around the world, including Save the Children, Sandy Hook Promise, and Friends of the Earth, as well as progressive political organizations, campaigns and candidates. This cycle, we're excited to support the DGA in flipping and protecting governorships across the country while expanding the Democratic majority in the Senate via our work with the DSCC, Sen. Raphael Warnock, Rep. Val Demings, Sen. Catherine Cortez Masto and Sen. Maggie Hassan.

We’re an equal-opportunity employer and take seriously our commitment to equality and equity. Our efforts to be inclusive and create opportunity don’t end when someone joins us – they begin.

We’ve set our sights on changing the world through our work and with our clients, and representation is at the foundation of what we do. We know that diversity of thought and background makes us stronger. That’s why we’re committed to building and maintaining a diverse community.

Every new team member broadens our perspective and allows us to think bigger. We’ll be at our best when people from underrepresented communities and people with a range of perspectives and lived experiences want to come, stay, and push the boundaries of what’s possible.

Overview: We are looking for a Data Engineer to transform millions of data points into unparalleled opportunities – supporting everything from electing Democrats to combating climate change throughout the world. With us, you’ll put your skills to use for disruptive innovation that powers social good. You’ll do it as part of a team of analysts and data scientists developing first-of-their-kind strategies and creating new products. Join us. Let’s go.

You will be responsible for:
Building data-intensive applications to extract, transform, and load massive data sets from a variety of internal, external, and public data sources;
Assembling large and complex data sets that meet project needs;
Developing processes for data mining, data modeling, and data production;
Using an array of technological languages and tools to connect systems;
Working toward constantly improving data reliability and quality;
Collaborating with cross-functional teams to support their data infrastructure needs.

Must-have qualifications:
Experience with one or more key data analytics tools: (Pandas, PySpark)
Experience extracting data from public APIs;
Ability to build and optimize data pipelines, architectures, and data sets;
Experience managing big data resources through Amazon Web Services or another cloud provider;
Experience with SQL;
Attention to detail;
Intellectual curiosity to innovate on ways to solve data management issues;
Passion, energy, and excitement for progressive and philanthropic causes and all things digital.

Nice-to-have qualifications:
1-3 years of professional experience;
Experience building data-intensive applications that collect data from diverse sources in the service of creating high-performance algorithms and predictive models;
Experience deploying deep-learning models;
Experience managing data warehouses and/or data lakes;
Experience working with cross-functional teams in a dynamic environment.

Salary range for this role is $70,000 to $90,000 per year, depending on experience.

If you feel you can do the job and are excited about this opportunity but are not sure if you meet all the qualifications, consider applying anyway. We’d love to hear from you!"
371,Principal Data Engineer,MissionWired,United States,"At MissionWired, we help clients create revolutionary digital strategies that advance their mission, change our country, and have a positive impact on the world.

We’re digital-obsessed, tech-savvy do-gooders who care deeply about social change. We’ve brought digital strategies to life for nonprofit organizations working around the world, including Save the Children, Sandy Hook Promise, and Friends of the Earth, as well as progressive political organizations, campaigns and candidates. This cycle, we're excited to support the DGA in flipping and protecting governorships across the country while expanding the Democratic majority in the Senate via our work with the DSCC, Sen. Raphael Warnock, Rep. Val Demings, Sen. Catherine Cortez Masto and Sen. Maggie Hassan.

We’re an equal-opportunity employer and take seriously our commitment to equality and equity. Our efforts to be inclusive and create opportunity don’t end when someone joins us – they begin.

We’ve set our sights on changing the world through our work and with our clients, and representation is at the foundation of what we do. We know that diversity of thought and background makes us stronger. That’s why we’re committed to building and maintaining a diverse community.

Every new team member broadens our perspective and allows us to think bigger. We’ll be at our best when people from underrepresented communities and people with a range of perspectives and lived experiences want to come, stay, and push the boundaries of what’s possible.

Overview: We are looking for a Principal Data Engineer to transform millions of data points into unparalleled opportunities – supporting everything from electing Democrats to combating climate change throughout the world. With us, you’ll put your skills to use for disruptive innovation that powers social good. You’ll do it as part of a team of analysts and data scientists developing first-of-their-kind strategies and creating new products. Join us. Let’s go.

You will be responsible for:
Building data-intensive applications to extract, transform, and load massive data sets from a variety of internal, external, and public data sources;
Assembling large and complex data sets that meet project needs;
Developing processes for data mining, data modeling, and data production;
Using an array of technological languages and tools to connect systems;
Developing robust testing and monitoring systems for scheduled processes;
Collaborating with cross-functional teams to support their data infrastructure needs.

Must-have qualifications:
Experience with one or more key data analytics tools: (pandas, PySpark)
Experience extracting data from public APIs;
Ability to build and optimize data pipelines, architectures, and data sets;
Experience managing big data resources through Amazon Web Services or another cloud provider;
Experience with SQL;
Attention to detail;
Intellectual curiosity to innovate on ways to solve data management issues;
Passion, energy, and excitement for progressive and philanthropic causes and all things digital.

Nice-to-have qualifications:
5+ years of professional experience;
Experience building data-intensive applications that collect data from diverse sources in the service of creating high-performance algorithms and predictive models;
Experience building and deploying deep-learning models;
Experience managing data warehouses and/or data lakes;
Experience working with cross-functional teams in a dynamic environment.

We are looking for candidates in a range of seniority levels, including very experienced engineers. The salary floor for this role is $90,000 per year and goes up depending on experience.

If you feel you can do the job and are excited about this opportunity but are not sure if you meet all the qualifications, consider applying anyway. We’d love to hear from you!"
372,"Data Engineer, Snowflake",Deloitte,"Cleveland, OH","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
373,Data Engineer,Deloitte,"McLean, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

A Data Engineer will be responsible for leading the setup of an AWS hosted data lake as well as the ingestion pipeline and processing for 100+ datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The Team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's Degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required security clearance for this position
Travel up to 50%
2+ years of data engineering, data management, and/or data warehousing experience
2+ years of of experience with data tools such as Informatica and Talend
2+ years of experience with Extract, Transform, and Load (ETL)
Preferred:
Professional Amazon Cloud Architecture certification
Ability to thrive in a fast-paced work environment with multiple stakeholders
Knowledge of data mining, machine learning, data visualization and statistical modeling
Prior professional services or federal consulting experience"
374,Senior Data Engineer,Grindr,"Los Angeles, CA","We’re looking to add a Senior Data Engineer to our Data Platform team. We’re in the process of modernizing our Data Platform with the goal of making data a strategic asset to all aspects of Grindr’s business. We recently finished a lift-and-shift of our PB-scale Data Lake to Snowflake and we’re beginning a next phase of redesigning our pipelines and data models. Goals broadly include near-real time product analytics, ML feature engineering, automated data quality alerts, a common event taxonomy and AB testing. Along the way we’re eliminating redundant code and streamlining processes.

Data Engineering has visibility and exposure to a broad set of crucial business functions at Grindr. The Data Platform team supports Data Engineering for all of Grindr by collaborating with product, engineering, and embedded analytics teams while collaborating with a core analytics team on the Data Platform.

The Data Platform is composed of 3 pods generally: Data Engineering and Core Product Analytics, Data Applications, and Data Science / ML Ops. Data Engineers coordinate across these pods. For example, help us upgrade our event firehose by migrating our 100K events-per-second firehose to a Kafka-based system. Build deep expertise in Airflow and Snowflake tasks as you write SQL that mutates trillions of records. Help develop and curate a new Data Catalog.

What’s the job?
Design, develop and deliver data pipelines and datasets to production, considering internal data governance, security and scale
Develop near real-time data pipelines using change-data-capture and event-based processing.
Maintain and enforce the business contracts on how data should be represented and stored.
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.

What We’ll Love About You
Significant experience with dimensional data modeling for a Data Lake / Warehouse
Expert in ETL and ELT (managing high-quality reliable ETL pipelines)
Experience with relational databases and expert in SQL development
5+ years of experience working with data at scale or complex data analysis
7+ years experience with Python or equivalent
Experience with agile engineering practices such as TDD, Continuous Integration, automated testing, and deployment.
Familiarity with legal compliance (with data management tools) data classification, and retention.

We’ll really swoon if you have
Experience as a Data Engineer using Snowflake or Data Bricks
Previous business experience working with customers around data-centric processes and analysis
Experience with complex business change management and process implementations

What You’ll Love About Us
Global impact. Grindr is the world leading LGTBQ+ social networking service. Your role will impact the lives of millions of LGTBQ+ people around the world.
High Growth. Like the company itself, this role offers significant room for growth and development
Remote First: We have offices in LA, NYC, SF, Chicago, Toronto and Taipei, though the company is set-up not just as remote-friendly, but remote-first. More than 30% of our employees work outside the cities where we have offices. Today, we are set-up to hire easily in the US, Canada and Taiwan. If you live outside one of these countries, just let us know so we can make sure we can support you.
Comp: Industry competitive compensation, including equity for all employees
Benefits: Unlimited vacation policy
Family insurance: We provide great insurance coverage for health, dental and vision for you and your family.
Retirement Savings: Generous 401(k) plan
Other Perks: home-office stipend, many company-sponsored events

About Grindr

Since launching in 2009, Grindr has grown into the world’s largest social networking app for gay, bi, trans, and queer people. We have millions of daily users who use our location-based technology in almost every country in every corner of the planet.

Today, Grindr proudly represents a modern LGBTQ+ lifestyle that expands into new platforms. From social issues to original content, we continue to blaze innovative paths with a meaningful impact for our community. At Grindr, we create a safe space where you can discover, navigate, and get zero feet away from the queer world around you.

As of June 2020, Grindr has new owners with a track record of multiple successful Bay Area start-ups. The new leadership is demonstrating a renewed commitment to creating an experience for users that is safe, fun, and productive, as well as a positive & uplifting company culture in which everyone can be their best selves. At the heart of Grindr’s mission in this new chapter is a shared set of core values including transparency, accountability, experimentation (failing fast), and strong allegiance to the LGBTQ+ community.

Grindr is an equal opportunity employer"
375,"Data Engineer, PXT Central Science",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The People eXperience and Technology (PXT) Central Science Team uses economics, behavioral science, statistics, and machine learning to proactively identify mechanisms and process improvements which simultaneously improve Amazon and the lives, wellbeing, and the value of work to Amazonians. We are an interdisciplinary team which combines the talents of science and engineering to develop and deliver solutions that measurably achieve this goal. We invest in innovation and rapid prototyping of scientific models and software solutions to accelerate informed, accurate, and reliable decision backed by science and data.

We are looking for an experienced data engineer engineer to integrate data sources and create a data infrastructure to model and research all aspects of HR, including: workforce planning, recruiting, employee engagement, retention, development, employee benefits, and compensation.

The ideal candidate is expected to work closely with senior scientists and business leaders to study the impact Amazon has had on workers, the market, and the economy. You will build new data engineering solutions end-to-end, will work with multiple stakeholders across HR and Operations, and will build data pipelines/automation that can be used by distributed systems to run statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Preferred Qualifications
5+ years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience in iterative design, working closely with business, product and tech partners from inception through implementation.
Detailed knowledge of data warehouses, architecture, infrastructure components, ETL and reporting tools and environments
Familiarity with AWS services such as S3, Redshift, EMR, Athena etc.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Experience providing technical direction and mentorship of engineers and scientists on best practices in the data engineering space
Be self-motivated and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1909425"
376,"Data Engineer , READI",Amazon,"Arlington, VA","Job Summary

DESCRIPTION

Recruiting the right candidates across all of Amazon is critical to our ability to innovate and build on behalf of customers. The Recruiting Engine Analytics Data and Insights Team is reinventing how we recruit at Amazon with the mission to make Amazon the most scientific Recruiting organization on earth.

Key job responsibilities

You are a proven technical leader within your org and relish identifying and solving data problems before others even see symptoms. You are efficient in applying your trade craft and use the time saved to anticipate next steps and guide/mentor others on your team. Your desire to learn and grow is unbounded. You seek exposure to a diverse set of technical problems and can work among an equally diverse team of Amazonians to solve them.

As the search for talent becomes more competitive, Amazon is bringing together recruiting, science, tech, programs, and process teams to find and hire candidates who raise the bar more quickly and efficiently. You will be joining a new team set to develop a ground-up solution to build a cutting-edge Recruiting data solution.

We are looking for an experienced Data Engineer to work with our leadership team, engineering and product, and recruiting teams to identify key data to drive analysis and those insights into action with the right teams. The ideal candidate will have a natural curiosity about solving problems – familiar with the complexity of human created ambiguous data. They will be a critical thinker with advanced problem solving, data mining, and data engineering skills able to answer ambiguous questions in data that help leaders make great data driven decisions. The successful candidate will work with a highly collaborative team, and have direct influence over data engineering roadmaps, deliver high performant data.

This position can be based in Austin, TX or in any USA Amazon Corporate office location.

A day in the life

You are a proven technical leader within your org and relish identifying and solving data problems before others even see symptoms. You are efficient in applying your trade craft and use the time saved to anticipate next steps and guide/mentor others on your team. Your desire to learn and grow is unbounded. You seek exposure to a diverse set of technical problems and can work among an equally diverse team of Amazonians to solve them.

About The Team

As the search for talent becomes more competitive, Amazon is bringing together recruiting, science, tech, programs, and process teams to find and hire candidates who raise the bar more quickly and efficiently. You will be joining a new team set to develop a ground-up solution to build a cutting-edge Recruiting data solution.

We are looking for an experienced Data Engineer to work with our leadership team, engineering and product, and recruiting teams to identify key data to drive analysis and those insights into action with the right teams. The ideal candidate will have a natural curiosity about solving problems – familiar with the complexity of human created ambiguous data. They will be a critical thinker with advanced problem solving, data mining, and data engineering skills able to answer ambiguous questions in data that help leaders make great data driven decisions. The successful candidate will work with a highly collaborative team, and have direct influence over data engineering roadmaps, deliver high performant data.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor's degree in a quantitative heavy field - Business, Sociology, Engineering, Statistics, Computer Science, Mathematics or related field
Demonstrated strength in SQL, data modeling, ETL development, and data warehousing.
Direct experience with one or more major data visualization tools including Tableau/QuickSight/Power BI
Preferred Qualifications
Advanced technical or business degree (MS or MBA)
Experience using AWS analytical services (e.g., Redshift, Data Pipeline, EMR, Machine Learning, Kinesis)
5+ years of hand-on experience with multiple analytics framework development platforms such as R, Python, SQL, AWS Services etc.
Proven problem solving skills, project management skills, attention to detail, ability to influence others and exceptional organizational skills
Ability to deal with ambiguity and competing objectives in a fast paced environment
Be self-driven, and show ability to deliver on ambiguous projects with incomplete data
Excellent communication (verbal and written) and interpersonal skills
Ability to effectively communicate with both business and technical teams. Ability to work collaboratively with non-technical customers and team members and translate business questions into analytical requirements with ease.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1657795"
377,"Data Engineer, Treasury Technology",Amazon,"Atlanta, GA","Job Summary

DESCRIPTION

The Treasury Technology team has an opportunity to be part of something big! Come work with the Treasury Risk Management (TRM) teams, our expanding technology organization of Product Managers, TPM’s, BI & Data Engineers to support automation and build scale into our systems and processes. Our team builds productivity tools in lieu of hiring additional operational headcount. The Treasury Technology team is looking for a Data Engineer to develop and support Treasury business intelligence, operations reporting and management systems.

About The Team

The Treasury Technology team is responsible for innovating, architecting, and building, global and scalable technology solutions that transform the Treasury Risk Management (TRM) experience. This team and the solutions we build are a critical component in TRMs continued growth.

Innovation is at the center of this team. As a Data Engineer on the Treasury Technology Team you will have an opportunity to collaborate with a team of customers, product managers, data engineers, and software development engineers in developing automated data solutions that will scale with the growing Treasury Risk Management (TRM) organization.

As a Data Engineer, you should be experienced in the architecture of DW solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business analysts and engineers to determine how best to design the data warehouse for reporting and analytics. You will be responsible for designing and implementing scalable ETL processes in the data warehouse platform to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as service which will have an immediate influence on day-to-day decision making. You should have the ability to develop and tune SQL to provide optimized solutions to the business.


Basic Qualifications
1+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
System Engineering experience for tools like Tableau and RPA(UI Path) is a plus.
Infrastructure management of third party applications
Preferred Qualifications
Demonstrated strength in SQL, python/pyspark scripting, data modeling, ETL development, and data warehousing
Experience in translating business needs into technical requirements
5+ years of industry experience as a Data Engineer in Treasury or Risk Management
Authoritative in ETL optimization, designing, coding, and tuning big data processes
Experience in designing and implementing data engineering solutions with AWS data technologies
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2010164"
378,Data Engineer,Amazon,United States,"Job Summary

DESCRIPTION

AWS Support is one of the largest and fastest growing business units within AWS. We are a highly technical, innovative organization revolutionizing the customer engagement processes and offers topnotch technical support for the portfolio of products and features of AWS. We are determined to redefine the word “Support” and lead the industry with best in class technology.

You Will

We are looking for an excellent Data Engineer who is passionate about data and the insights that large amounts of data sets can provide. You should possess both a data engineering background and a business acumen that enables you to think strategically. You will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work in lock-step with BI Engineers, Data scientists, ML scientists, Business analysts, Product Managers and other stakeholders across organization.
Develop and improve the current data architecture, data quality, monitoring and data availability.
Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning
Partner with BAs across teams to build and verify hypothesis to improve the AWS Support business.
Help continually improve ongoing reporting and analysis processes, simplifying self-service support for customers
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data sets of customer experience on AWS.

Basic Qualifications
Bachelor’s/Masters degree in Computer Science or related technical field, or equivalent work experience.
4+ years of work experience with ETL, Data Modeling, and Data Architecture.
2+ years of work experience with Python, Scala or other scripting languages.
Knowledge of AWS services including S3, Redshift, EMR, Kinesis and RDS.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience in ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.
Experience with building data pipelines and applications to stream and process datasets at low latencies.
Experience handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.
Experience with native AWS technologies for data and analytics such as Redshift Spectrum, Athena, S3, Lambda, Glue, EMR, Kinesis, SNS, CloudWatch, etc.
The base pay range for this position in Colorado is $132,090 - $178710/yr; however, base pay offered may vary depending on job-related knowledge, skills, and experience. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, dependent on the position offered. This information is provided per the Colorado Equal Pay Act. Base pay information is based on market location. Applicants should apply via Amazon's internal or external careers site.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1921924"
379,Data Solutions Engineer - Opportunity for Working Remotely,VMware,"New York, NY","Why will you enjoy this new opportunity?

Every new job is an opportunity for growing your career. VMware is growing and growing fast in the multi-cloud space. As part of this, VMware is growing its data products and cloud services portfolio to provide our customers a truly multi-cloud platform. That includes SQL, no-SQL engines, analytics capabilities (everything from BI to AI), and messaging and caching services for modern, cloud-native application development.

You can be a part of this growth story as VMware is the only company perfectly positioned to provide the multi-cloud platform for all the applications our customers need to accelerate their business today, and in the future. Joining VMware gives you long-term opportunities to expand your skills with annual education and/or training reimbursements, job rotation programs, subscriptions to online training platforms and employee networking groups. As a Tanzu Data Solutions Engineer you must be equally comfortable in both a business and technical context, interacting with executives and talking shop with technical audiences.

A Little Bit About The Role & The Team

In this role you will be reporting to the America’s head of solution engineering and be responsible for all current and new accounts. We are customer obsessed and we believe in the value we can add and stay honest about it. We love to learn, are open to giving and receiving feedback and are passionate about making our customers successful. Our team works to ensure data is accessible, usable and valuable to everyone. On this team, you'll be working on hard, worthwhile problems with a collaborative team, accelerating your growth as a Data Architect / Engineer. The challenges we solve for our customers vary from implementing high performance, highly available transactional systems for industry leading financial institutions to designing messaging / eventing architectures for healthcare companies to optimizing deep analytics across petabytes of critical business data for consumer clients.

In this role you will get to:

Learn about customer’s data use cases and design patterns and then position complementary/competitive VMWare Tanzu Data products & services.
Educate/enable technical audience on Tanzu Data products like Greenplum for MPP analytics, Open Source database products like Postgres/MySQL, caching solutions like GemFire (Apache Geode) and messaging products like RabbitMQ.
Present Tanzu Data technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Tanzu Data technology throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position Tanzu Data in relation to them.
Collaborate with Product Management, Engineering, and Marketing to continuously improve Tanzu Data's products and marketing.

Desired Skills And Experience

Ability to connect a customer's specific business problems and Tanzu Data's solutions
Outstanding presentation skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Understanding of traditional DW/BI components (ETL, Staging, DW, ODS, Data Marts, BI Tools)
Understanding of the hardware and networking architecture behind large scale systems
Familiarity with standard statistical/BI packages to perform analytic calculations
Experience with cloud infrastructure architecture and use; AWS, Azure, GCP
Operational knowledge of vSphere, Kubernetes , Docker, containers
Ability to work independently and meet deadlines with little to no supervision
Building, deploying, and managing systems leveraging a variety of data technologies.
Specific experience with Postgres and the associated ecosystems of tooling and related technologies is a plus.
Experience with Massively Parallel Processing (MPP) data warehouses such as Greenplum, Teradata, Netezza, Exadata, Azure SQL Data Warehouse, Redshift, and/or Hadoop.
Data science understanding - AI/ML, GIS, MADlib, etc
Excellent verbal and written communications skills along with the ability to present technical data and approaches to both technical and non-technical audiences
Hands-on expertise with DBaaS, NoSQL, SQL and SQL analytics
University degree in computer science, engineering, mathematics or related fields, or equivalent experience preferred

What are the benefits and perks of working at VMware?

You and your loved ones will be supported with a competitive and comprehensive benefits package. Below are some highlights, or you can view the complete benefits package by visiting www.benefits.vmware.com.

Employee Stock Purchase Plan
Medical Coverage, Retirement, and Parental Leave Plans for All Family Types
Generous Time Off Programs
40 hours of paid time to volunteer in your community
Rethink's Neurodiversity program to support parents raising children with learning or behavior challenges, or developmental disabilities
Financial contributions to your ongoing development (conference participation, trainings, course work, etc.)
Wellness reimbursement and online fitness and wellbeing classes

Category : Sales

Subcategory: Systems Engineering

Experience: Manager and Professional

Full Time/ Part Time: Full Time

Posted Date: 2022-04-08

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
380,Data Engineer II,Memorial Sloan Kettering Cancer Center,"New York, NY","At Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. We’re treating cancer, one patient at a time. Join us and make a difference every day.

MSK requires all new hires, volunteers, and vendors who come onsite to provide proof of COVID-19 vaccination. This is based on the COVID-19 vaccines’ high level of protection and our shared responsibility in protecting our patients.

Beginning on February 21, 2022, MSK will require all new hires, volunteers, and vendors who come onsite to provide proof of complete COVID-19 vaccination series + booster (once eligible). Individuals currently eligible for a COVID-19 booster must get their booster, or have an approved exemption as a condition of employment at MSK. Individuals can get their COVID-19 booster 5 months after receiving their second dose of either the Pfizer-BioNTech or Moderna COVID-19 vaccine, or 2 months after the Johnson & Johnson vaccine. Individuals who are not yet eligible for a booster must meet this criteria within 30 days of becoming eligible.

Job Description

Are you passionate about technology? Do you want to play a key role in redefining the future of cancer care?

We Are

We are seeking a Data Engineer II to join MSK’s Digital Pathology team as we meet the challenge of converting pathology from an analog to a digital practice. You will create pipelines to consolidate and deliver vital data that will drive research and analytics focused on improving patient care.

You are a highly skilled data engineer who is comfortable working with large volumes of data. You can design and develop complex solutions using modern practices and technologies. You are an eager, self-starting learner who can quickly pick up new technologies.

You Will
Engineer infrastructure and tools to deliver critical data to downstream systems and industry partners
Design and build backend integrations using various technologies and taking a creative approach when working with legacy systems.
Engineer features of the data platform that will help ensure quality and robustness.
Collaborate in an agile team with Product Owners, Scrum Masters, System Architects, other Development Teams and Users.
Participate in full SAFe and Agile development life cycle including analysis, design, built and release of data pipelines.
You Are:
Proficient in relational database schema and query design
Proficient in using scripting languages such as Python, bash, and R
Proficient with industry standard tools for ETL design and administration (i.e. Data Stage)
Familiar with linux environments and server administration
Experience using Docker, Kubernetes or similar container technologies and setting up CI/CD pipelines is a plus.
Experience designing RESTful APIs is a plus.
Experience and familiarity with Cloud providers (AWS, Azure) is a plus.
Benefits

Competitive compensation packages | Sick Time |Generous Vacation+ 12 holidays to recharge & refuel| Internal Career Mobility & Performance Consulting | Medical, Dental, Vision, FSA & Dependent Care|403b Retirement Savings Plan Match|Tuition Reimbursement |Parental Leave & Adoption Assistance |Commuter Spending Account |Fitness Discounts &Wellness Program | Resource Networks| Life Insurance & Disability | Remote Flexibility

We believe in communication, openness, and thinking beyond your 8-hour day @ MSK. It’s important to us that you have a sense of impact, community, and work/life balance to be and feel your best.

Closing

MSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.

Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment."
381,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, IrisLogic, Inc, is seeking the following. Apply via Dice today!

Skills AWS, Python, Terraform, Spark REMOTELong Term Contract"
382,Data Engineer-AWS,Baker Tilly US,"New York, NY","Responsibilities

About Baker Tilly Digital

Baker Tilly has an incredible career opportunity for a Data Engineer, AWS to join our growing Digital Consulting team.

Baker Tilly’s Digital Consulting Practice combines deep functional, industry, and technical capabilities to help clients solve their toughest enterprise digital challenges. Baker Tilly Digital is focused on the integration of advanced technologies with core business transformational services to support companies in successfully navigating the complexities of digital transformations.

You Will Enjoy This Role If You
You enjoy building rock solid data solutions and diving deep into technical development to solve complex problems
You enjoy supporting a variety of industries and embedding yourself with client teams to work together to find a solution
You are constantly looking to grow your knowledge and skills in technology and want to stay up to date with the latest technology trends
You have and are interested in maintaining different technical certifications
You are looking for your next challenge, crave variety where no day is the same and feel your skills and experience can be better utilized
You are looking to make an impact as part of a fast growing, innovative practice where your hard work and creativity can help us continue to build our brand as a firm
You have a passion for leading transformation, enabling an organization to meet objectives on time and on budget
You do your best work when you are part of a talented, down-to-earth team that thrives in collaboration and truly enjoys working together to meet client needs in a variety of industries
You feel valued when you are provided the resources and support to continually sharpen your skills and build your career now, for tomorrow
What You Will Do
Develop leading edge data analytics solutions in the AWS stack.
Work with your colleagues, client, and/or within an Agile team to assist in delivery of the data solution as defined by the Architect or Project Manager.
Work to understand business processes and align your data solution design to those processes.
Research tools and approaches, recommend design patterns, and help define best practices in collaboration with the team.
Support the development of the Enterprise Transformation practice from go-to-market activities to delivery methodology standards and team development
Utilize your entrepreneurial skills to network and build strong relationships internally and externally with clients and the community
Invest in your professional development individually and through participation in firm wide learning and development programs
Support the growth and development of team members and clients through the Baker Tilly Value Architect model, helping associates meet their professional goals
Enjoy friendships, social activities and team outings that encourage a work-life balance
Qualifications

Successful Candidates will have:
Minimum of 3-5 years of experience
Demonstrated experience developing and implementing modern data analytics solutions in the AWS stack
Experience with modern AWS analytics tools (Glue, Data Lake Formation, S3, Redshift, QuickSight)
Experience performing data acquisition from source databases and APIs
Experience with data modeling techniques (Kimball, Data Vault 2.0)
Experience in Agile solution development
Experience in data quality and testing
Experience with source control toolsets (Git)
Experience building data pipelines using Python or PySpark
Knowledge of best practices, design patterns, and key principles in data analytics
Preferred / beneficial experience
One or more AWS certifications
Experience developing in an on-prem/hybrid environment
Experience with cloud resource provisioning and configuration
Experience with resource provisioning automation (Terraform)
Experience with other major cloud providers (Azure, Oracle)
Experience with other major ETL tools (Informatica)
Experience with MuleSoft for integration development
Experience with data virtualization, especially Denodo
Experience with other data modeling and visualization tools (Tableau)
Experience developing solutions using Apache airflow, Lamda Functions, and/or Databricks
Experience in Agile product development
Experience with Agile tools (Jira)
Knowledge of CI/CD processes (GitHub Actions, AWSCode)
Exhibit responsibility and accountability towards quality completion of projects and consistently hitting project timelines
Strong verbal and written communication skills and are not ashamed to ask questions or raise concern on projects
Outstanding customer service skills following proper business requirements and human resources expectations
Disciplined to be able to work in a variety of business environments
Ability to travel as needed and work outside of core business hours for client engagements
Bachelors degree in a relevant field
Overview

Baker Tilly US, LLP (Baker Tilly) is a leading advisory, tax and assurance firm, providing clients a genuine coast-to-coast and global advantage with critical mass and top-notch talent in major regions of the U.S. and in many of the world’s leading financial centers - New York, London, San Francisco, Los Angeles and Chicago. Baker Tilly is an independent member of Baker Tilly International, a worldwide network of independent accounting and business advisory firms in 148 territories, with 36,000 professionals and a combined worldwide revenue of $4.0 billion.

Many of Baker Tilly’s roles have the opportunity to work remotely. Please discuss with your talent acquisition professional to understand the requirements for an opportunity you are exploring.

Baker Tilly is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status, gender identity, sexual orientation, or any other legally protected basis, in accordance with applicable federal, state or local law."
383,Data & ML Engineer - Nationwide Opportunities,Amazon Web Services (AWS),"Charlotte, NC","Description

At Amazon Web Services (AWS), we’re hiring highly technical cloud computing architects and engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS Analytics and ML service offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

In our Global Specialist Practice, you will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers, and, most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing solutions.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications
Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field
5+ years’ experience of Data platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Kinesis/Kafka/Spark/Storm implementations.
Experience with analytic solutions applied to the Marketing or Risk needs of enterprises
Basic understanding of machine learning fundamentals.
Ability to take Machine Learning models and implement them as part of data pipeline
5+ years of IT platform implementation experience.
Experience with one or more relevant tools ( Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis ).
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc).
Current hands-on implementation experience required
Preferred Qualifications
Masters or PhD in Computer Science, Physics, Engineering or Math.
Hands on experience working on large-scale data science/data analytics projects.
Ability to lead effectively across organizations.
Hands-on experience with Data Analytics technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Implementing AWS services in a variety of distributed computing, enterprise environments.
Proficiency with at least one the languages such as C++, Java, Scala or Python.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Experience working on a code base with many contributors.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A1437514"
384,"Data Engineer L4, Alexa Feedback",Amazon,"Boston, MA","Job Summary

DESCRIPTION

Alexa Feedback is looking for a Data Engineer to support Alexa Proactive Delivery experiences across multiple channels and experiences. There are millions of proactive experiences delivered to our customers as Device Notifications, “By the Way“ and Home Cards. In order to make decisions about selecting content that is more relevant and timely for our customers, and selecting the right channel for delivery, our systems will need to process and store large volumes of data to understand how customers engage with the content. As a Data Engineer on this team, you will have an opportunity to collaborate with a team of product managers, applied scientists, and software development engineers in order to design and build the best data ingestion, processing and management environment.

Key job responsibilities

As a Data Engineer, you should be familiar in the architecture of data warehousing solutions, using multiple platforms and tools. You should have strong analytical skills and excel in the design, creation, management, and business use of extremely large datasets and combining raw information from different sources. You will be responsible for designing and implementing scalable extract, transform, and load (ETL) processes to support the rapidly growing and dynamic business demand for data, and use it to deliver the data as a service,. Moreover, you will create automated alarming and dashboards to monitor data integrity. Finally, you should have very good communication skills and ability to adjust communication to different groups of stakeholders. Overall, you will be the subject matter expert for the data structures and usage, while striving for efficiency by aligning data systems with business goals.


Basic Qualifications
1+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Experience with object-oriented design
Bachelor’s Degree in Computer Science or similar discipline
Preferred Qualifications
Strong, object-oriented design and coding skills (C/C++ and/or Java preferably)
Experience with distributed (multi-tiered) systems, algorithms, and relational databases
Ability to effectively articulate technical challenges and solutions
Ability to work across many teams and coordinate tasks
Ability to deal with ambiguous/undefined problems;
Previous technical internship(s) preferred
Graduate degree is a plus
By submitting your application here, you can apply once to be considered for multiple Software Engineer openings across various Amazon teams. If you are successful in passing through the initial application review and assessment, you will be asked to submit your career and personal preferences so that our dedicated recruiters can match you to the right role based on these preferences.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1941875"
385,"Data Engineer, Snowflake",Deloitte,"Philadelphia, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
386,Data Engineer,"LanceSoft, Inc.","Woodcliff Lake, NJ","Job Description

Job Title: Data Engineer

Location: Woodcliff Lake, NJ 07677

Duration: 4+ Month

Job Description...

Develops information systems by designing, developing, and installing software solutions.

Python, Rest, SQL, AWS for Cloud technologies, Big Data experience (working with data lakes)

Must Have: Python, SQL, AWS Day to day/Project: Advanced analytics team, preparing/collecting data for data engineers, ingesting data into data lakes, building data pipeline.

Remote or WFH policy: this will start as a WFH role and once BMW opens the office they would expect the person to be onsite at least 3 days a week (unsure when BMW will reopen at this time); candidate must be prepared to work in EST

EEO Employer - Minorities/Females/Disabled/Veterans/Gender Identity/Sexual Orientation"
387,"Data Engineer, Snowflake",Deloitte,"Camp Hill, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
388,"Data Engineer, Cell Quality Engineering",Tesla,"Fremont, CA","Description

Data Engineer, Cell Quality & Field Reliability

The Team

The Cell Quality team is a small team within the R&D organization responsible for incoming, production and field reliability. The group projects focus on mass production of Li-ion cells for all programs, including Model S, X, 3, Y and Energy Products.

The Role

The position is to support daily data analytics activities and process improvements suggestions based on fleet data and known field returns for all cell programs with more focus on Kato cell production build issues. Candidate is expected to lead or support quality focused multidisciplinary cell-related projects, typically involving R&D, Design or Production organization. The engineer must be extremely organized, detail orientated, with strong ability to prioritize and multitask, successfully collaborate on projects with a range of business objectives. This person must exhibit the knowledge, leadership, and drive needed to not only challenge the status quo, but also define and execute the optimal path forward.

Responsibilities
Perform extensive data study on all field failures and correlation studies with upstream cell manufacturing process. Create test models to ensure proper detection and outlier rejection criteria for field rejects, especially from Kato production.
Identify trends from field return data and quantify reliability risks for diagnostics of key critical signals/metrics.
Create data visualizations to communicate analysis results with cross-functional teams and drive decision making of key failure modes on field.
Analyze Field Reliability/Quality data for cell related failures and failure modes to predict expected failure rates, affected populations, verify effectiveness of the corrective actions at Tesla and at suppliers.
Provide data integration and setup quality systems for new in-house manufacturing lines and mass production cell models.
Produce cogent and intelligible data visualizations, author technical presentations and summarize high-impact technical findings with strong data analysis package.
Requirements
Strong SQL, Python queries for data analytics, applied knowledge of statistical analysis (like hypothesis testing), time series analysis, working knowledge of reliability statistics such as Weibull Analysis
Experience building optimal ETL data pipelines across structured and unstructured data sources
Strong data visualization skills Tableau/JMP, and Python packages such as seaborn, matplotlib
Working knowledge of Big Data technologies like Hadoop ecosystem (Spark, HDFS, Presto etc.)
Excellent verbal and written communication skills - ability to break down complex technical topics and deliver visual technical presentations (e.g., PowerPoint) to groups of engineers, scientists, and technicians
B.S/M.S. in Data Science, Data Analytics, Computer Science, Engineering (Industrial Engineering, Mechanical Engineering, Materials Science is a plus) with 1-2+ years of industry experience.
Desirable Attributes
Experience to recommend statistical/analytical models based on manufacturing data and process setup changes to improve quality of product
Quality control experience in test/manufacturing environment
Design of experiments for process optimization"
389,Schneider Internship - Data Engineer (Start Spring 2022),Schneider Electric,"Franklin, TN","What do you get to do in this position?

The Data Services Data Engineer Intern will develop, maintain and improve of data delivery through the North America (NAM) Data Warehouse platforms. The right person in this role will support overall improvements in data architecture, metric definition and processes to support business data and analytical initiatives. Our Data Engineers provide support to team members in the Data Services team and our business partners.

This will be your next career opportunity if you like to:
Translate business requirements into data pipelines and data stores to support of business requirements.
Work with Data and Solution Architects to define and implement migration strategies from legacy systems to cloud architecture and technologies.
Provide team feedback to optimize delivery of our solutions.
Collaborate with team members to conceptualize, design and deliver enterprise and departmental data solutions to support business intelligence, data warehousing and reporting and machine learning requirements.
Implement solutions that are reliable and scalable to meet the service levels associated with mission-critical solutions.
Participate in and enhance our DevOps practice to insure highly available solutions and quick issue resolution.

Qualifications

The right candidate will have the following capabilities and experience:
Currently pursing Bachelor’s Degree in IT/Computer Systems, ideal candidate graduates Spring 2023
Relevant experience in Data Engineering or equivalent education/academic experience preferred.
Prior experience with primary AWS services such as EC2, EMR, S3, IAM policies, Cloudwatch, Cloud Formation, SES
Experience with data handling and database technologies (Lambda, DMS, Kafka, Spark, Redshift, Hadoop, Airflow, etc.).
Experience with modern programming languages such as Python, Scala or Java.
Experience designing and building systems to process large volumes of heterogeneous data, real time and batch to deliver data solutions.
Command of advanced SQL queries and programming.
Adopt best practices in data architecture, design and implementation.
Have an eye for operational transparency and resiliency at every layer of the application.
Proven analytical and problem-solving abilities. Ability to assimilate information and quickly discern the most relevant facts and recommend creative, practical design solution. Ability to think outside-the-box a real asset.
Familiarity with DEVOPs tools and processes and CI/CD are an asset.
Excellent communication, presentation, influencing, and reasoning capabilities.
Desire to take the initiative moving projects/ideas forward with clarity
Knowledge of legacy data warehousing tools and technology is an asset Examples: Dimensional Models, Informatica PowerCenter, Informatica Cloud, MS Integration Services, Alteryx, Oracle, MS SQL Server etc.

Schedule: Full-time

Req: 007PFE"
390,Azure Data Engineer,Infosys,"Jersey City, NJ","Job Description

Infosys is seeking Senior Azure Data Engineers with experience in native Azure components and Data Engineering on Azure. The Senior Data Engineer is a core member of the team responsible for providing digital solutions to data challenges through implementation of Cloud-native data solutions to support use cases. This role will be responsible for developing, optimizing and standardizing data pipelines, creating robust data models for data publishing while complying with, and adding value to, the data architecture. Perform data model design, data formatting, and ETL development optimized for efficient storage, access, and computation to serve various use cases.

Required Qualifications
Candidate must be located within commuting distance of Weehawken, NJ or be willing to relocate to the area.
Bachelor’s Degree or foreign equivalent, will consider work experience in lieu of a degree
Experience securing Cloud based modern data platforms
Hands-on experience in developing, and successfully operationalizing complex/large scale data management projects
Experience in Python/Scala and Spark or Databricks to handle big data transformation and processing
Experience in SQL Development, specifically with complex queries
Hands-on experience implementing data processing using Azure services like ADLS, Azure Data Factory, Synapse/DW, Azure SQL DB, Azure Event Hub, Azure Stream Analytics, Azure Functions, Azure Analysis Service, Databricks etc.
Collaborates with downstream consumers, understand their requirement of data engineering and do implementation
Preferred Qualifications
Below certifications will be a huge plus
DP-200: Implementing an Azure Data Solution
DP-201-Designing an Azure Data Solution
Microsoft Certified: Azure Data Engineer Associate
The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face and will have offshore team co-ordination entailing earling morning or late evening overlap during required days.

About Us

Infosys is a global leader in next-generation digital services and consulting. We enable clients in 50 countries to navigate their digital transformation.

With over three decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

To learn more about Infosys and see our ideas in action please visit us at www.Infosys.com

EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/National Origin"
391,Data Science Engineer I,Nabors Industries,"Houston, TX","Company Overview

Nabors is a leading provider of advanced technology for the energy industry. With operations in about 20 countries, Nabors has established a global network of people, technology and equipment to deploy solutions that deliver safe, efficient and responsible hydrocarbon production. By leveraging its core competencies, particularly in drilling, engineering, automation, data science and manufacturing, Nabors aims to innovate the future of energy and enable the transition to a lower carbon world.

Nabors is committed to providing equal employment opportunities to all employees and applicants and prohibiting discrimination and harassment of any type without regard to race, religion, age, color, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This applies to all terms and conditions of employment including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. To learn more about our Fair Employment practices, please refer to the Nabors Code of Conduct.

JOB SUMMARY

Data Science Engineer will help develop data science applications using machine learning and statistical techniques, help discover the information hidden in large and complex data sets

Apply technical expertise with quantitative analysis, experimentation, data mining, and the presentation of data to develop strategies for our products

Candidate must have experience using a variety of data mining and statistical methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations.

DUTIES AND RESPONSIBILITIES

Understanding of statistics and hypothesis testing (ex. Confidence Interval, Regressions, Time Series, Clustering, Factor Analysis)
Use predictive modeling and machine learning algorithms to solve complex problems
Stay abreast of state-of-the-art machine learning technologies; follow code standards and best practices
Develop and maintain forecasting models and tools using frameworks such as TensorFlow
Doing ad-hoc analysis and presenting results in a clear manner
Use Deep Learning frameworks like PyTorch, Tensorflow and MxNet is a plus
Strong written and verbal communication skills

Desired Skills and Experience


MINIMUM QUALIFICATIONS/SKILLS

• 0-3 years of experience in analytics with at least 1 year of experience in statistical analysis

• Bachelors or master’s degree in quantitative field (Computer Science, Mathematics, Machine Learning, AI, Statistics, or equivalent)

Familiar with the following software/tools:

Experience with data querying languages (e.g. SQL), scripting languages (e.g. Python), and/or statistical/mathematical software (e.g. R)
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
Ability to implement statistical models for forecasting, time series predictions
Experience using web services: REST API, SOAP, WCF etc. is a plus


PREFERRED QUALIFICATIONS

Bachelors in Computer Science and a Master’s in Data Science or another quantitative field

PHYSICAL REQUIREMENTS / WORKING CONDITIONS

Travel to rig for domain knowledge and new product testing."
392,"Data Engineer, WW Installments Competitive Pricing",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

WW Installments is one of the fastest growing businesses within Amazon and we are looking for a Data Engineer to join the team. This group has been entrusted with a massive charter that will impact every customer that visits Amazon.com. We are building the next generation of features and payment products that maximize customer enablement in a simple, transparent, and customer obsessed way. Through these products, we will deliver value directly to Amazon customers improving the shopping experience for hundreds of millions of customers worldwide. Our mission is to delight our customers by building payment experiences and financial services that are trusted, valued, and easy to use from anywhere in any way.

As the Data Engineer within WW Installments, you will be responsible for designing and implementing the WW Installments Competitive Pricing metrics and data pipeline roadmap. This core capability is fundamental to the success of the business. You will have high visibility across Amazon businesses and leadership.

Key job responsibilities

This Role Will Be Responsible For
Build the WW Installments Competitive Pricing Analytics data platform that will be used by teams worldwide to access reporting, deep dives, and installment related insights.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.
Design, implement, and maintain production data pipelines including DQ, SLA, and data agreements across data, ML, and partner teams.
Continually improve the reporting and analysis pipeline, automating and simplifying whenever possible, and enabling self-service support for stakeholders.
Engage business stakeholders in constructive dialogues to convert business problems into data pipeline logic. Identifying new opportunities to influence business strategy and product vision using data.
Support fellow engineers and scientists to deliver analytical projects and build proof of concept applications. Partner with fellow Data/Applied Scientists to implement scalable, automated infrastructure for data extraction, processing, computation, and delivery.
Learn new technology and techniques to support product and process innovation.
Work through significant business and technical ambiguity delivering on analytics roadmap across the team with autonomy.

Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Spark, EMR, etc.)
Basic/Intermediate proficiency in programming language (Python and Scala) for automation of data extraction/processing, statistical computation, and/or web scraping.
Ability to deal with ambiguity and competing objectives in a fast-paced environment.
Preferred Qualifications
Master's degree in computer science, engineering, mathematics, or a related technical discipline
A desire to work in a collaborative, intellectually curious environment.
5+ years of industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist, and Applied Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Coding proficiency in at least one modern programming language (Python, Scala, Java, etc)
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Query performance tuning skills using Unix profiling tools and SQL
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Experience with AWS, cloud computing
Prior experience in tech, ecommerce, retail, or finance/banking industry
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1887266"
393,Sr. Data Engineer,NBCUniversal,"Englewood Cliffs, NJ","Responsibilities

Principal Data Engineer This is an opportunity to play a critical role in the digital evolution of NBC News, one of the world’s best known and most trusted news organizations. We are looking for a Sr. Data Engineer for the Business Intelligence team, a part of NBC News Digital Technology.

You will be responsible for designing and maintaining efficient, flexible, and extensible data frameworks to build advanced analytics pipelines and data applications across the business. You will operate as leader within the data engineering team and work with other development teams to facilitate the extraction of data and maintain data pipelines across the NBC News Digital portfolio. You should have experience working with and mentoring Jr. developers and working to foster a culture of adaptability, respect and ownership.

This position reports to the Head of Data Engineering and is based in Seattle, WA. Responsibilities

Possess an in-depth understanding of the data structures and governance
Deep understanding of modern cloud computing platforms and concepts (AWS preferred)
Work with modern schema-less big data storage solutions
Work closely with machine learning and data science teams to create scale and efficiency
Work closely with DevOps to foster a culture of collaboration
Demonstrate critical thinking for potential roadblocks; comprehend a bigger picture of the business and effectively communicate these issues to stakeholders
Work closely with internal stakeholders to implement solutions that adhere to solution designs and schema

Requirements

Qualifications/Requirements

Embrace a ‘builder’ mentality to push the business

forward

5+ years experience in a software developer role
Proficient with Python, PySpark and Javascript
Experience with distributed data technologies (e.g.

Spark)

Proficient with Linux environment
Familiarity with SQL and Presto
Experience with Serverless cloud platforms a

requirement (AWS)

Passion for media and news
B.S. degree in Computer Science, Information

Technology, or equivalent experience

Desired Characteristics

Desired Characteristics

Working experience on Airflow and Dags is a plus.
Experience building scalabale and reliable Data Streaming

Pipeline using Spark and Kafka.

Sub-Business

News Digital

Career Level

Experienced

City

See List Below

State/Province

Multiple Locations

Country

United States

Multiple Locations

Englewood Cliffs - New Jersey, New York - New York

About Us

NBCUniversal owns and operates over 20 different businesses across 30 countries including a valuable portfolio of news and entertainment television networks, a premier motion picture company, significant television production operations, a leading television stations group, world-renowned theme parks and a premium ad-supported streaming service.

Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. We strive to foster a diverse and inclusive culture where our employees feel supported, embraced and heard. We believe that our workforce should represent the communities we live in, so that together, we can continue to create and deliver content that reflects the current and ever-changing face of the world. Click here to learn more about Comcast NBCUniversal’s commitment and how we are making an impact.

Notices

NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.

NBCUniversal is an equal opportunity employer and will provide reasonable accommodations as required by applicable federal, state, and/or local laws."
394,"Data Engineer, Device Analytics Services",Amazon,"Denver, CO","Job Summary

DESCRIPTION

Do you want to join a team that works with all of Amazon’s devices (Alexa-enabled devices, Fire TV, Fire Tablets, Kindle, etc.) including confidential, emerging products? Do you want to manage data that provide insight into the success of our devices? Do you want to work with world-class, state-of-the-art technologies on a fun, diverse, rapidly growing team in sunny Denver, Colorado? Then you want to join the Device Analytics Services Team in Denver!

Denver enjoys 300+ days of sunshine each year and has a vibrant, active community in which to enjoy them! A half dozen world class ski resorts within a two hour drive, innumerable hiking trails, rivers and streams, hundreds of miles of mountain bike trails, and a network of multi-use paths throughout the Denver metro area await! Amazon’s office is situated in Denver's bustling Union Station district with excellent public transit access via Union Station just steps from the office. Dozens of nearby restaurants and the iconic 16th Street Mall are right outside the office doors. Ball Arena (home of the Colorado Avalanche and Denver Nuggets), Coors Field (home of the Colorado Rockies), and Mile High Stadium (home of the Denver Broncos) are all within walking distance of the office.

The Device Analytics Services team defines, certifies, and maintains metrics that provide insight into the success of our devices.

The Role

As a Data Engineer at Amazon, you will be working in one of the world's largest and most complex data warehouse environments. You should be an expert in the architecture of DW solutions for the enterprise using multiple technologies (RDBMS, Columnar, Cloud). You should excel in the design, creation, management, and business use of extremely large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

The ideal candidate is passionate about new opportunities and has a demonstrated track record of success in delivering new features and products. A commitment to teamwork, hustle, and strong communication skills (to both business and technical partners) are absolute requirements. Creating reliable, scalable, and high-performance products requires exceptional technical expertise and practical experience building large-scale data systems. This person has thrived and succeeded in delivering high quality technology products/services and will flourish in an environment where new opportunities and challenges arise frequently.

We seek a great team player who will naturally elevate the performance of the people around them.

Responsibilities
Work with other senior leaders on the team to build the technical strategy & architecture for a broad area
Design & develop world-class data warehouse solutions
Mentor & grow engineers across the team, leading by example and insisting on high standards
Develop tools & processes to enable best-in-class operations for our services

Basic Qualifications
Bachelors with 6+ years or Masters with 5+ years of experience
Data Warehousing Experience with Oracle, Redshift, Teradata, etc.
Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Experience in functional programming languages (Scala, Python, Perl, etc.)
Query performance tuning skills using Unix profiling tools and SQL
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space
Linux/UNIX experience including to process large data sets
Some experience leveraging SAS, R or matlab to manipulate data and set up automated processes as per business requirement
Strong ability to interact, communicate, present and influence within multiple levels of the organization
Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions
A desire to work in a collaborative, intellectually curious environment
This position starts at $154k to $209k/yr. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a range of medical, financial, and/or other benefits, dependent on the position offered. For more information regarding Amazon benefits, please visit https://www.amazon.jobs/en/benefits. Applicants should apply via Amazon’s internal or external careers site.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1968670"
395,Data Engineer,EvolutionIQ,New York City Metropolitan Area,"EvolutionIQ

We're on a mission to build the world's leading AI platform that will transform the insurance industry making insurance better, faster and cheaper for everyone.

We develop machines with deep comprehension and vast data access to guide humans to the optimal decisions and enable them to handle enormous numbers of claims. We pair the predictive insights with an intuitive and easy to use web application that handles complex claim summarization, guidance and investigation workflows. The result is potential savings in the hundreds of millions, and added precision to insurance pricing.

Our Team

We are founded by a senior Google AI expert and a Bridgewater Associates Algorithmic Investor & Stanford MBA. We’re not looking for employees. We’re looking for partners in work, partners in culture-building, and partners in the future of data-driven insurance. The development team consists of world class engineers and leaders from companies like Google and Bloomberg. Each individual has had great success building large scale enterprise software and is now excited to try their hand at transforming the insurance industry.

Our investors include First Round Capital (seed investors in Uber, Square, Looker), FirstMark (Airbnb, Pinterest, Shopify) and Foundation Capital (Lending Club, CoverWallet, Netflix) and we have deep support within each of their organizations. You will also have full access to their networks and counterparts at portfolio companies for advice, ideas and community.

Your Role

Who are you? A senior data engineer who is excited to work on challenging Data infrastructure problems. You'll be a key contributor to EvolutionIQ’s Data Platform working closely with the Analytics and ML Team to understand their workflows and build a scalable, robust and developer friendly platform that can support multiple different products and use cases.

Outcomes

• Work with Analytics and ML Engineers to map workflows to a scalable platform
• Setup a reliable continuous model deployment infrastructure
• Setup centralized data storage to support ease of data access and EDA
• Setup reliable and secure client data ingestion
• Refactor core algorithms for improve efficiency, readability and test coverage

Role Competencies

• Implemented Data platform used for building and shipping multiple products
• Owned, deployed and maintained ETL pipelines in production
• Extensive experience in deploying systems on GCP or AWS
• Expert developer who writes clean, efficient, easy to understand code with unit tests, functional design patterns
• Expert in Python 3 and Pandas or equivalent data manipulation library
• Excellent technical document writing skills
• Extreme creativity and resourcefulness, appetite to solve previously unsolved problems

Other

• Extreme self-starter and self-motivator
• Holds self to extremely high standards, and inspires others to do the same
• Open to giving and receiving critical feedback
• Able to handle the ups and downs of early startup life
• Enthusiasm for team work and pair work
• Kind, empathetic, polite and professional

Benefits

We think work should be engaging, rewarding and flexible. While there is a standard set of benefits listed below, we are small enough that we can consider any specific requests that you need. We’re a well funded company and we’re able to offer competitive compensation packages and real ownership in your work. Specific compensation and equity varies with experience level.

• Full Medical, Dental Insurance, Vision, Short & Long Term Disability
• 401K matching
• Flexible vacation
• Paid team lunches"
396,Data Engineer,Amazon,"Austin, TX","Description

Supply Chain Optimization Technologies (SCOT) creates the science and technology to drive Amazon's supply chain. SCOT builds software systems to make the most products available to the most people for delivery as quickly as possible. This SCOT Austin team focuses on predicting future supply and demand, and creating the execution plan for our global fulfillment network. To accomplish these goals we build simulation and experimentation systems at scale and leverage cutting-edge technologies across Operations Research, Software Engineering, Machine Learning, Anomaly Detection, and time series Forecasting.

Amazon is seeking a truly innovative Engineer to join the S&OP Services Team.

As an Amazon Engineer you will be working in one of the world's largest and most complex environments. The that we consume, transform, make addressable, and vend to automated systems drives buying, labor hiring, and inventory routing decisions. This is also visualized in UIs and dashboards that are used by thousands of users world-wide.

Our team is responsible for mission critical analytic reports and metrics that are viewed at the highest levels in the organization. We are also working on newer tools that help users discover using and Big technologies. You should have deep expertise in the design, creation, management, and use of significantly large sets. You should have excellent and communication skills to be able to work with owners to develop and define key questions, and to build sets that answer those questions. You should be an expert at designing, implementing, and operating stable, scalable, low cost solutions to flow from production systems into the lakes and end-user facing applications. You should be able to work with customers to understand the requirements and implement reporting solutions. Above all you should be passionate about working with huge sets and someone who loves to bring sets together to answer questions and drive change.

This opportunity is perfect for highly motivated and talented engineers who want to apply and grow their technical depth and breadth while defining and driving key aspects of the customer experience on Amazon.com.


Basic Qualifications
2+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, warehousing, and building pipelines
Experience with SQL
Preferred Qualifications
Understanding of system architecture and experience with large distributed systems.
Experience with AWS technologies.

Company - Amazon.com Services LLC

Job ID: A1669320"
397,Senior Data Engineer - AI Machine Learning,Northwestern Mutual,"New York, NY","At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.

Summary

Northwestern Mutual's data engineering team is growing and we are looking for a Sr. Data Engineer - AI Machine Learning to join our team. In this team, we practice, to implement team initiatives including, enable overall data laboratory architecture & environment, operationalize analytic model created by data science practice, support risk analytic product like risk quantitation and behavior analysis; to be the owner of analytical data SLA & Quality of all analytical model from operationalization to production. Partner with data science ongoing enhancing the analytic model also are one of key benefit of machine leaning, it requires highly efficient execution and infrastructure consumption ratio, modern technology exploring, simplify operationalization efforts, automatic data QA and repeatable process.

Incremental Trustworthy Maturity Path: Enable a MLOps environment, partner with data scientist for analytic model R&D, Support the needs (“the What”) from product team continue enhance trustworthy of product function to generate business value. Centralize ownership & CoE for analytical model release management, operation support, and ongoing maintenance, support the RAP product as immediate needs. Creating a pilot model for EIRC. Operationalization Analytica model including analytic operation environment support, model efficiency enhancement, batch model operation workflow design, automation data ingestion and distribution

Responsibilities
Define Analytic MLOps and ModelOps frameworks, methods, guidelines and principles. Apply subject matter expertise to operationalize analytic model, processes to make machine learning improvements and recommendations for change including execution.
Collaborate with source system technical owners, Product Owners, and Infrastructure resources to create data model of EIRC data lake and data integration solutions to automatic source data ingesting to our AL and ML data science environment on AWS cloud.
Manage the change of analytic model, function, mapping, and analytical model dictionary
Ability to create designs following established analytical model process patterns and establish new patterns to meet business needs
Provide ML/AL subject matter expertise to the team covering data brisk, AI/ML programming, replication, streaming, and virtualization. Can explain complex concepts to others and establish business value of solutions
Complete design, code review and test case reviews to improve and set the bar for the quality of the delivery. Providing guidance for the engineering team while designing the MLOps applications
Maintain analytic model operationalization agile management stories with delivery to dates to ensure Safe Agile processes are adhered to
Provide guidance to MLOps team members and other data science of the Analytic Model Operationalization cross function of EIRC
Remain current on industry specific knowledge and emerging technologies
Hands on development with various technologies including Informatica, AWS capabilities, Kafka, Java, Python, R, Spark and others
What We’re Looking For
Bachelor's degree in Computer Science, MIS, or related field
8+ years of experience in data engineering or related field
MLOps systems architecture experience, designing larger scale ML/AI model solutions, break down to smaller goals, prioritization
Erwin data modeling.
Experience with relational data engineering eco systems (preferably complex disbursed data)
Experience in writing and executing complex Analytical Model
Strong programming skill in R, Python, Spark using data Brick
Enable administration of data laboratory using Data Brack
AWS Cloud experience with Kafka, API’s, Ansible etc.
Able to build CICD pipeline for deployments
Experience in end to end data engineering solutions
Demonstrated success in coaching/mentoring others
Individuals who show a strong sense of ownership for their deliverables
Ability to work across the organization and communicate effectively to various levels
Desired
Database administration experience
Cloud experience, preferably AWS, is a strong plus
Risk and Cybersecurity Services industry background
Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now!

W e are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.

If you work or would be working in Colorado or outside of a Corporate location, please click here for information pertaining to compensation and benefits.

FIND YOUR FUTURE

We’re excited about the potential people bring to Northwestern Mutual. You can grow your career here while enjoying first-class perks, benefits, and commitment to diversity and inclusion."
398,Data Engineer,Amazon,"San Diego, CA","Job Summary

DESCRIPTION

Have you ever thought about what it takes to detect and prevent fraudulent purchases among hundreds of millions of ecommerce transactions in six countries? What would you do to create a trusted marketplace where millions of buyers and sellers can safely transact online? What kinds of processes and systems would you build to maximize customer satisfaction?

Our mission in Payments Risk is to make Amazon.com the safest place to shop online. The Payments Risk team safeguards the order pipelines; monitoring, tracking, and managing risk to ensure long-term buyer satisfaction. The Payments Risk group designs and builds the software systems, risk models and operational processes that minimize risk and maximize trust in Amazon.com. In addition to this, we evaluate new business opportunities from across the company to determine how we can minimize the risk associated with these launches.

Amazon.com is seeking an outstanding Data Engineer to join the Payments Risk Analytics team. Amazon.com has culture of data-driven decision-making, and demands business intelligence that is timely, accurate, and actionable. If you join the Amazon.com Payments Risk team your work will have an immediate influence on day-to-day decision making at Amazon.com and drive the adoption of new Business Intelligence technologies.

Are you passionate about turning large amounts of information into knowledge? Are you passionate about building and leading data teams? Can you architect large data systems? Can you work with business partners and across technical teams?

A key responsibility of this role is to build large data systems that help us drive advanced analytics for Payments Risk. A successful candidate will be uncompromisingly detail oriented, efficient, and customer obsessed. Your work must be accurate, timely, and insightful. You must be a self-starter who is able to think big and work in a fast-paced and ever-changing environment. You will build sustainable and scalable analytics processes, and data systems that can be leveraged across Payments Risk.

As a Data Engineer on the Analytics team, you’ll have huge impact on how customers engage with Amazon through building infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, and recent advances in distributed systems (i.e. MapReduce, noSQL databases). You will work with passionate scientists, business intelligence engineers, software development engineers and product managers, to deliver a variety of stable and performant data feeds used for developing business insights as well as offline machine learning use cases.

We love to work with smart people who have a strong sense of ownership and strong engineering mindset. You are a technical leader for your team and a great mentor. You provide perspective and context for technology choices. You’re up to the challenge of real-time notification strategies, latency, TPS and building an end-to-end platform that internal Amazon teams integrate with. You motivate your team to pursue ambiguous situations and rapidly produce prototypes for a more personalized experience. You outline paths from prototype to product. You deeply invest in each colleague's career growth, improving their technical knowledge, and defining your team's operational metrics.


Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
Industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Experience working with AWS big data technologies (EMR, Redshift, S3, AWS Glue, Kinesis and Lambda for Serverless ETL)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Hands-on experience and advanced knowledge of SQL
Basic scripting skills using Python and Scala
Basic understanding of Machine Learning
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Demonstrated strength in data modeling, ETL development, and data warehousing
Experience using business intelligence reporting tools (Quicksight, Tableau etc.)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Mindset and analytical skills to towards continuous improvement and have an edge to always research on latest technologies
Passion for building great notification experiences which directly impacts our customers
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1834425"
399,Data Engineer,Autodesk,"Utah, United States","Job Requisition ID #

22WD55699

Position Overview

Please note, we are looking for candidates located in the West Coast, U.S.

Autodesk is looking for a talented Data Engineer to join our Observability Analytics team to create robust and scalable data pipelines using and improving existing platforms.

A successful candidate has a strong sense of ownership and will use their expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientists. They will drive performance enhancements, development best practices and collaborate with other Data Engineering teams throughout Autodesk.

Responsibilities
Create, automate, and support reliable data pipelines
Gather customer requirements, sequence work and document technical solutions
Interface with data engineers, data scientists, product managers and internal stakeholders
Cross-train and mentor teammates
Minimum Qualifications
2+ years of data processing experience in large cloud-based infrastructure (AWS preferred)
Familiar with SQL, dimensional modeling, and analytical data warehouses, like Snowflake
Understanding of Data Engineering best practices for medium to large scale production workloads
Hands-on software development experience in Python
Experience with data pipeline orchestration tools, like Airflow
Customer-facing and service-oriented person
Team player with great communication skills
Problem solver with excellent written and interpersonal skills
Experience consuming REST APIs
Preferred Qualifications
Experience with ELT pipelines - DBT
REST API design and implementation
Familiarity with containers and infrastructure-as-code principles
Experience with automation frameworks - Git, Jenkins, and Terraform
At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.

Are you an existing contractor or consultant with Autodesk? Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact Autodesk Careers.



With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It’s disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital artists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world."
400,"Data Engineer, Alexa",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

Alexa is the groundbreaking cloud-based intelligent agent that powers Echo and other devices designed around your voice. The Alexa Economics team is looking for a Data Engineer (DE) to join us to support measurement of the Alexa business and to provide actionable insights across the Alexa ecosystem.

Key job responsibilities
Design and implement a scalable and durable data model for Alexa datasets.
Build the design, development, and maintenance of ongoing metrics to deliver key business insights.
Collaborate with business intelligence and software engineers to implement the design of our data architecture and to automate and scale statistical analyses developed by scientists on the team.
Create automated alarming and dashboard to monitor data integrity.
Guard data security to meet legal and compliance requirements
A day in the life

As a Data Engineer you will partner with software developers and business intelligence engineers to build end-to-end data pipelines and have exposure to senior leadership as we communicate results and provide guidance to the business. You will work with Software Developers, BI Engineers, Data Scientists, and Economists to better understand the features customers love and how to optimize customer discovery of these features.

A successful candidate will be able to partner effectively with both business and technical teams, including clear communication of results across a variety of stakeholders. He/she will be an expert in SQL/ETL and data manipulation. The candidate will also have an eye for optimization and automation in reporting. This high-impact role provides a great opportunity to demonstrate capabilities to dive deep, deliver results, think big, invent and simplify, and earn trust.

About The Team

Our team is identifying the key drivers for engaging customers on the Alexa platform across devices, skills and services. As a part of the larger Alexa Customer Experience team, your area of influence and impact will be all of the Alexa organization across the globe. You will have a front row seat to the evolving voice assistant industry and opportunities to impact the customer experience of a cutting edge product used every day by people you know.


Basic Qualifications
1+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in Mathematics, Statistics, Engineering, or a related technical field
Strong proficiency in SQL and experience with at least one programming language such as Python
Experience in data mining (SQL, ETL, data warehouse, etc.) and using databases in a business environment with large-scale, complex datasets.
Preferred Qualifications
Advanced degree in Math, Finance, Statistics, Engineering, Computer Science or related discipline.
Proficiency with statistics and use of statistical applications such as R or Python.
Experience with CRADLE for data extraction.
Experience with Redshift.
Proven ability to convey rigorous technical concepts and considerations to non-experts
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1813770"
401,Data Engineer (Big Data),U.S. Bank,"Cupertino, CA","At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.

Job Description

Be a part of transformational change where integrity matters, success inspires and great teams collaborate and innovate. As the fifth-largest bank in the United States, we're one of the country's most respected, innovative, ethical and successful financial institutions. We're looking for people who want more than just a job - they want to make a difference! U.S. Bank is seeking a Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.

This position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.

Essential Responsibilities
Responsible for designing, developing, testing, operating and maintaining products.
Takes full stack ownership by consistently writing production-ready and testable code.
Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs.
Makes sound design/coding decisions keeping customer experience in the forefront.
Takes feedback from code review and apply changes to meet standards. Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures.
Accountable for ensuring all aspects of product development follow compliance and security best practices.
Exhibits relentless focus in software reliability engineering standards embedded into development standards.
Embraces emerging technology opportunities and contributes to the best practices in support of the bank's technology transformation.
Contributes to a culture of innovation, collaboration and continuous improvement.
Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery. Anticipates and communicates blockers and delays before they require escalation.
Basic Qualifications
Bachelor's degree, or equivalent work experience
Three to five years of relevant experience
Preferred Skills/Experience
Thorough understanding of a feature, the users impacted, the flows impacted and feature's purpose
Technical designs are clear, well thought out, and considers dependencies, failure states, maintainability, testability and ease of support
Considers scalability, reliability and performance of systems/contexts affected when defining technical designs
Understands the team's domain, how work in this domain relates to the team's objectives and deliverables and how it contributes to overall business strategy and how technical strategy maps to this
Thoroughly understands the business model in relation to their current product focus area
Ability to analyze the bigger picture, identifying and prioritizing with the aim to consider more than one domain within an analysis
Looks for opportunities to simplify product and technical design
Adept with agile software development lifecycle and DevOps principles
Able to communicate processes and results with all parties involved in the product team, including engineers, product owner, scrum master, third party vendors and customers
Strong problem-solving and analytical skills
Excellent communication and interpersonal skills
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.

Benefits

Take care of yourself and your family with U.S. Bank employee benefits. We know that healthy employees are happy employees, and we believe that work/life balance should be easy to achieve. That's why we share the cost of benefits and offer a variety of programs, resources and support you need to bring your full self to work and stay present and committed to the people who matter most - your family.

Learn all about U.S. Bank employee benefits, including tuition reimbursement, retirement plans and more, by visiting usbank.com/careers.

EEO is the Law

Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal EEO is the Law poster.

E-Verify

U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.

Due to legal requirements, U.S. Bank requires that the successful candidate hired for some positions be fully-vaccinated for COVID-19, absent being granted an accommodation due to a medical condition, pregnancy, or sincerely held religious belief or other legally required exemption. For these positions, as part of the conditional offer of employment, the successful candidate will be asked to provide proof of vaccination or approval for an accommodation or exemption upon hire."
402,Principal Data Engineer (Remote),IHS Markit,United States,"Position Summary

We are looking for an adept, action-oriented Principal Data Engineer to design and build out a multi-tenant data mesh to enable our soon-to-be-launched digital transformation product which uses advanced NLP, knowledge engineering, and ML to accelerate innovation in engineering, manufacturing, and scientific operations. The perfect candidates will have strong data infrastructure and data architecture skills, a proven track record of collaborating and iteratively implementing data-intensive solutions, strong operational skills to drive efficiency and speed, strong project leadership, and a strong vision for how data engineering can proactively create positive impact for companies. You will be a part of an early-stage team. You will educate stakeholders, mentor team members, and have a significant stake in defining the future of the Data Engineering function for the product.

Job Responsibilities

Design, build, and maintain a multi-tenant Data Mesh within the AWS cloud comprised of Data Lakes, Warehouses, Streaming, Graphs, and analytical NoSQL stores
Drive adoption and standardization of data governance, lineage, cataloging, and stewardship practices across teams
Work closely with data scientists, micro-service developers, and security experts to build out a big data platform incrementally and securely
Work closely with the product management and development teams to rapidly translate the understanding of customer data and requirements to product and solutions
Maintain an excellent understanding of the business’s long-term goals and strategy and ensures that the design and architecture are aligned with these
Define and manage SLA’s for data sets and processes running in production
Design for disaster recovery balancing availability and consistency in multi-region scenarios
Research and experiment with emerging technologies and tools related to big data
Establish and reinforce disciplined software engineering processes and best-practices

Ideal Qualifications

Comfort and ideally substantial experience operating big data infrastructure in a cloud-based ecosystem (AWS preferred)
Deep understanding of the theoretical and practical tradeoffs of various data formats in object/file stores (Parquet, Avro, JSON, etc.) in combination with a variety of ETL tools (Spark, Presto, etc.)
Deep understanding of the theoretical and practical tradeoffs of various NoSQL stores (Cassandra, Elasticsearch, DynamoDB, etc.) with respect to different read/write patterns and availability/consistency requirements
Mastery of operating and designing stream-based data systems (Kafka, AWS Kinesis, GCP PusSub, etc.) particularly under varying load
Be proficient in modern big data architectural approaches (Kappa/Lambda architectures, Data Lake Zones, etc.)
Experience with data governance, lineage, cataloging tooling (Apache Atlas, Apache Ranger, AWS Glue Catalog, etc.)
Experience with data pipeline and workflow management tools (AWS Data Pipeline, Apache Airflow, Argo, etc.)
Experience with stream-processing systems (ksqlDB, Spark Streaming, Apache Beam/Flink, etc.)
Experience with software engineering standard methodologies (unit testing, code reviews, design document, continuous delivery)
Develop and deploy production-grade services, SDK’s, and data infrastructure emphasizing performance, scalability, and self-service.
Ability to conceptualize and articulate ideas clearly and concisely
Entrepreneurial or intrapreneurial experience where you helped lead the creation of a new product & organization

Nice to Have’s

Strong algorithms, data structures, and coding background with either Java, Python or Scala programming experience
Experience working with knowledge graphs stores (Stardog, TigerGraph, Ontotext GraphDB, Neo4j) and surrounding semantic technology (OWL, RDF, SWRL, SPARQL, JSON-LD)
Experience working with Snowflake data warehouses and dimensional modeling practices
BA/BS or Masters in Computer Science, Math, Physics, or other technical fields
Experience with at least 10+ terabyte datasets, ideally up to multiple petabytes

What We Offer

Competitive base salary, bonus plans and equity.
A comprehensive, benefits package that includes medical, dental, vision and life insurance plans, paid time off, a generous 401k match with no vesting period, parental leave and 3 volunteering days each year. For more information on benefits, please access the benefits page on our careers site: https://careers.ihsmarkit.com/benefits.php.
For work locations in the state of Colorado, the anticipated minimum base salary for this role would be $104,679. Compensation will be determined by the education, experience, knowledge, and abilities of the applicant.

We’re building a software solution that connects data in revolutionary ways, illuminating answers that were previously impossible to find and empowering our clients to envision the future so they can determine the best course of action in the present. Join us!

Equal Opportunity Employer

S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.

US Candidates Only

The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law."
403,Data Engineer,Autodesk,"Denver, CO","Job Requisition ID #

22WD55699

Position Overview

Please note, we are looking for candidates located in the West Coast, U.S.

Autodesk is looking for a talented Data Engineer to join our Observability Analytics team to create robust and scalable data pipelines using and improving existing platforms.

A successful candidate has a strong sense of ownership and will use their expertise to build extensible data models, provide meaningful recommendations and actionable strategies to partnering data scientists. They will drive performance enhancements, development best practices and collaborate with other Data Engineering teams throughout Autodesk.

Responsibilities
Create, automate, and support reliable data pipelines
Gather customer requirements, sequence work and document technical solutions
Interface with data engineers, data scientists, product managers and internal stakeholders
Cross-train and mentor teammates
Minimum Qualifications
2+ years of data processing experience in large cloud-based infrastructure (AWS preferred)
Familiar with SQL, dimensional modeling, and analytical data warehouses, like Snowflake
Understanding of Data Engineering best practices for medium to large scale production workloads
Hands-on software development experience in Python
Experience with data pipeline orchestration tools, like Airflow
Customer-facing and service-oriented person
Team player with great communication skills
Problem solver with excellent written and interpersonal skills
Experience consuming REST APIs
Preferred Qualifications
Experience with ELT pipelines - DBT
REST API design and implementation
Familiarity with containers and infrastructure-as-code principles
Experience with automation frameworks - Git, Jenkins, and Terraform
At Autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. Autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. We also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law.

Are you an existing contractor or consultant with Autodesk? Please search for open jobs and apply internally (not on this external site). If you have any questions or require support, contact Autodesk Careers.



With Autodesk software, you have the power to Make Anything. The future of making is here, bringing with it radical changes in the way things are designed, made, and used. It’s disrupting every industry: architecture, engineering, and construction; manufacturing; and media and entertainment. With the right knowledge and tools, this disruption is your opportunity. Our software is used by everyone - from design professionals, engineers and architects to digital artists, students and hobbyists. We constantly explore new ways to integrate all dimensions of diversity across our employees, customers, partners, and communities. Our ultimate goal is to expand opportunities for anyone to imagine, design, and make a better world."
404,Data Engineer with Scala and Spark,Dice,"Weehawken, NJ","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Wipro Ltd., is seeking the following. Apply via Dice today!

Job Title Data Engineer Location Weehawken, NJ Experience 7 +Years Job Description 8+ years of IT experience and minimum 5+ years of exp. in Cloud nativeBigData development Expertise in Spark and Scala programming languages (Primary Skills) Ability to design high performant data ingestion and calculation frameworks using ScalaSpark. Experience in executing ScalaSpark code using Databricks framework. Expertise in performance tuning and optimization of Spark jobs Familiar with Azure ADF based orchestration Familiar with Azure cloud platform and service About Wipro Wipro Limited (NYSE WIT, BSE 507685, NSE WIPRO) is a leading global information technology, consulting and business process services company. We harness the power of cognitive computing, hyper-automation, robotics, cloud, analytics and emerging technologies to help our clients adapt to the digital world and make them successful. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, we have over 220,000 dedicated employees serving clients across six continents. Together, we discover ideas and connect the dots to build a better and a bold new future."
405,Data Engineer,Amazon Web Services (AWS),"New York, NY","Job Summary

DESCRIPTION

AWS Support is one of the largest and fastest growing business units within AWS. We are a highly technical, innovative organization revolutionizing the customer engagement processes and offers topnotch technical support for the portfolio of products and features of AWS. We are determined to redefine the word “Support” and lead the industry with best in class technology.

He/she Will

We are looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data sets can provide. The ideal candidate will possess both a data engineering background and a strong business acumen that enables him/her to think strategically. He/she will experience a wide range of problem solving situations requiring extensive use of data collection and analysis. The successful candidate will work in lock-step with BI Engineers, Data scientists, ML scientists, Business analysts, Product Managers and other stakeholders across organization.
Develop and improve the current data architecture, data quality, monitoring and data availability.
Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning
Partner with BAs across teams to build and verify hypothesis to improve the AWS Support business.
Help continually improve ongoing reporting and analysis processes, simplifying self-service support for customers
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data sets of customer experience on AWS.

Basic Qualifications
Bachelor’s degree in Computer Science or related technical field, or equivalent work experience.
5+ years of work experience with ETL, Data Modeling, and Data Architecture.
4+years of work experience in writing and optimizing SQL.
Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS.
1+ years of work experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.)
Knowledge of distributed systems as it pertains to data storage and computing
Preferred Qualifications
Experience with ETL optimization, designing, coding, and tuning big data processes using Apache Spark or similar technologies.
Experience with building data pipelines and applications to stream and process datasets at low latencies.
Experience handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.
Knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.
Experience with native AWS technologies for data and analytics such as Redshift Spectrum, Athena, S3, Lambda, Glue, EMR, Kinesis, SNS, CloudWatch, etc.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A2012125"
406,Data Engineer II - Remote,Homesite Insurance,"Boston, MA","Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.

One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.

Join us on our journey as we continue to grow into a powerful contender in the field of insurance.

Compensation may vary based on the job level and your geographic work location.

Compensation Minimum:$90,200

Compensation Maximum:$166,100

Homesite is an innovative national property and casualty insurance company with a fast-paced environment. We're proud to be part of the American Family Mutual Insurance Company and the home for the Enterprise’s Property Center of Excellence. We support multiple products sold through a number of operating companies and distribution methods, including offerings in partnership with other large national brands. We seek a person possessing analytical rigor and strong technical skills with a desire to build a successful career in Data Engineering. The primary role will be to develop cloud-based data solutions intended to support the data needs of statistical predictive modeling projects that grow profits through the development of state of the art pricing and product plans.

Responsibilities
Develop, maintain, and enhance data tooling to enable data integration and availability.
Collaborate with the Modeling and Data Science teams to provide non-production dev/ops and robust monitoring of model performance.
Assist in the implementation of a cloud data strategy including tooling, governance, and documentation.
Develop, maintain, and enhance databases throughout the organization to support analytics projects.
Understand corporate data structure to be able to draw data from transactional data tables existing in the company.
Support the acquisition of external data sets, interpreting data layouts, structures, fields and values to incorporate new data into the core analytics data base.
Analyze databases for performance optimization, including data normalization, indexing, and memory management.
Clearly communicate complex findings to colleagues and external customers.
Monitor the results of statistical models through the use of dashboards and ad hoc analyses.
Minimum Requirements
Bachelor’s degree or higher in Computer Science, Engineering, Information Systems, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.
Three or more years of programming or data engineering experience.
Intermediate software development skills utilizing an Object-Oriented Approach with one or more of the following languages in: Python, Java/Kotlin, Go
Familiarity with implementation of data engineering tooling such as Apache Spark, Apache Airflow, and/or cloud provider-specific tooling.
Familiarity with pipeline and application deployment to various compute approaches including serverless, container orchestration, and cloud-based virtual machines.
Intermediate SQL knowledge, including advanced knowledge of one of more of the following: Microsoft SQL Server, PostgreSQL, Google BigQuery, Amazon Redshift or similar
Familiarity with Linux/Unix shell.
Experience with at least one of the major cloud platforms: Google Cloud Platform, Amazon Web Services, and/or Microsoft Azure
Moderate level of comfort working with Git.
Motivated individual with strong analytic, problem solving, and troubleshooting skills
Preferred Qualifications
Master’s degree or higher in Computer Science, Engineering, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.
Familiarity with statical computing approaches (R, Pandas/NumPy, Mathematica, and such)
Experience with administrative responsibilities on Windows Server environments.
Experience with NoSQL approaches such as DynamoDB, MongoDB, and/or Google BigTable
Familiarly with data lake approaches such as Amazon Athena/S3, Delta Lake, and/or Google Cloud Dataflow/Cloud Storage.
Knowledge of data visualization approaches such as Tableau, D3.js, Looker, and/or Power BI.
Experience with MLOps tooling such as DataRobot, KubeFlow, AWS Sagemaker, MlFlow, H20, GCP AI Platform, and such.
Knowledge of statistical software languages/packages such as R, Python, DataRobot, Tableau, Mathematica, and Spark.
When you work at Homesite you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns and contingent workers are not eligible for American Family Enterprise benefits.

Stay connected: Join Our Enterprise Talent Community!"
407,"Data Engineer, Snowflake",Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
408,"Data Engineer, Exports Visibility",Amazon,"Bellevue, WA","Job Summary

DESCRIPTION

Global Mile (GM) Exports supports Amazon’s strategic vision of bringing worldwide cross-border product selection at the lowest price to our customers and helping sellers and vendors (manufacturers, distributors and brand owners) navigate international trade and logistics. We design and build products and programs with a focus on customer, seller, and vendor experience, and global scalability.

GM Exports works directly with logistics cross-border carriers and partner with several stakeholder teams to constantly improve the international delivery experience. In GM Exports, the Exports Visibility team is seeking a highly skilled and motivated Data Engineer to join our team in Bellevue. GM Exports is growing rapidly in scope, scale and complexity - driving ever growing data needs to support critical business decisions. Our ideal candidate is someone who is always learning as the business grows and as relevant technologies continue to evolve. If you enjoy innovating, thinking big and want to contribute directly to the success of an industry changing business, you may be a prime candidate for this position.

Key job responsibilities
Contribute to the architecture, design and implementation of next generation BI solutions – including streaming data applications.
Manage AWS resources including EC2, RDS, Redshift, Kinesis, Lambda etc.
Collaborate with data scientists, BIEs and BAs to deliver high quality data architecture and pipelines.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers

Basic Qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering).
3+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets.
3+ years of experience in scripting languages like Python etc.
Demonstrated strength in data modeling, ETL development, and Data warehousing. Data Warehousing
Experience with Redshift
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Experience in working and delivering end-to-end projects independently.
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with AWS services including Redshift, EMR, Kinesis, S3 and RDS.
Demonstrated strength in data modeling, ETL development, and data warehousing
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1983555"
409,Data Engineer,Deloitte,"Alexandria, VA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

A Data Engineer will be responsible for leading the setup of an AWS hosted data lake as well as the ingestion pipeline and processing for 100+ datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The Team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's Degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required security clearance for this position
Travel up to 50%
2+ years of data engineering, data management, and/or data warehousing experience
2+ years of of experience with data tools such as Informatica and Talend
2+ years of experience with Extract, Transform, and Load (ETL)
Preferred:
Professional Amazon Cloud Architecture certification
Ability to thrive in a fast-paced work environment with multiple stakeholders
Knowledge of data mining, machine learning, data visualization and statistical modeling
Prior professional services or federal consulting experience"
410,Data Engineer,"LanceSoft, Inc.","White Plains, NY","Job Description

Summary
Company is providing digital capabilities and driving innovation throughout the organization to achieve its VISION2030 Strategy and goals.
The right individual will share our passion of digital technology, transformation, and enjoy working in a dynamic and agile environment based on high collaboration culture.
Company is looking for a technologist to help advance our Analytics platform and capability into the future.
The Data Engineer will work with the Data Services team to design, develop and manage big data centric solutions that meet the strategic business and technology direction of the organization.
Responsibilities
Design and develop Big data solutions using AWS, Spark and Data Bricks that are flexible, extensible, elastic, secure and reliable at large scale.
Work with Lead Data Engineer to provide guidance and direction to project teams ensuring compliance with coding standards and best practices.
Collaborate with Data Governance team to capture and manage meta data and implement data quality rules.
Building and managing data pipelines and promoting to production.
Continuously learn and be at the leading edge of Data Integration, Cloud, Containerization and other industry trends.
Work with stakeholders including product, data and business teams to assist with data-related technical issues and support their data infrastructure needs.
Follow Cyber security guidelines and polices to monitor the company's data security and privacy.
Knowledge, Skills And Abilities
Experience in traditional and cloud data management components (MS SQL, RDS, Athena, or similar).
Experience in metadata driven ingestion framework, building data pipelines and data sets.
Familiarity with DevOps and Agile methodologies.
Strong analytical skills.
Understanding of cloud security policies and concepts.
Experience in Collibra and Trillium is a plus.
Experience with ETL tools: Talend, Pentaho, or similar tools.
Education, Experience And Certifications
Bachelor of Science Degree in Computer Science or IT Engineering.
Minimum of 2 years of Data Engineering experience.
Experience in building micro services using AWS Lambda and similar technologies.
Experience with any one of the big data tools such as: Spark, Kafka, Stream Sets, Hadoop.
Scripting experience with JavaScript, pySpark, Python, T-SQL or other similar languages.
Cloud certification (AWS/Google) is preferred.
EEO Employer

Minorities/ Females/ Disabled/ Veterans/ Gender Identity/ Sexual Orient"
411,"Data Engineer, Snowflake",Deloitte,"New Carrollton, MD","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
412,"Data Engineer, Snowflake",Deloitte,"Jacksonville, FL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
413,Data Engineer - Snowflake,Deloitte,"Houston, TX","Are you an experienced, passionate pioneer in technology? A cloud solutions builder who wants to work in a collaborative environment. As an experienced DATA ENGINEER - SNOWFLAKE, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

Essential Functions:
Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Indeed's Data Warehouse as well as data model changes that align with warehouse standards and backfill or other warehouse data management processes
Develop and execute testing strategies to ensure high quality warehouse data
Provide documentation, training, and consulting for data warehouse users
Perform requirement and data analysis in order to support warehouse project definition
Provide input and feedback to support continuous improvement in team processes
The Team

The US Cloud Engineering Offering focuses on enabling our client's end-to-end journey from On-Premise to Cloud, with opportunities in the areas of Cloud Strategy and Op Model Transformation, Cloud Development & Integration, Cloud Migration, and Cloud Infrastructure & Managed Services. Cloud Engineering supports our clients as they improve agility, resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited Sponsorship: Limited immigration sponsorship may be available"
414,"Data Engineer II, Physical Stores Real Estate",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

Do you want to solve business challenges through data and innovative technology? Do you enjoy working on cutting-edge cloud based big data technologies, designing data models to solve complex business problems, building advanced analytic solutions such as Geo-spatial and machine learning data pipelines in a fast-paced team environment? Do you love working on industry defining projects that move the needle?

At Amazon, we hire the best minds in technology to innovate & build things for the benefit of our customers. The intense focus we have on our customers is why we are one of the world’s most beloved brands – it is part of our company DNA. Our Data Engineers play an instrumental role in utilizing data and technologies to solve complex problems and get to see the impact of their work.

The Amazon Physical Stores team is looking for a Data Engineer to lead building of the next generation self-service data and analytic solutions. We are working to solve complex business problems by collecting data from numerous data assets across different dimensions, and building best-in-class Geo-spatial and machine learning modeling solutions. Our team is constantly inventing on behalf of customers and the ideal candidate will be customer obsessed.

A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.

The Successful Candidate Will
Collaborate with experienced cross-disciplinary Amazonians from the business, data science, and engineers to develop, design, and bring to market innovative big data solutions.
Innovate and build rich data assets using the AWS cloud big data and machine learning technologies for self-service and data science use cases.
Enrich data assets with new data acquisition and feature engineering in close collaboration with the business partners, economists and data science teams.
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

Basic Qualifications
Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
5+ years of industry experience in data engineering related field with a track record of manipulating, processing, and extracting value from large datasets
Knowledge of batch and streaming big data architectures
Experience with AWS technologies including Redshift, RDS, S3, EMR, EML or similar solutions built around Hive/Spark etc.
Experience using big data technologies (Hadoop, Hive, HBase, Spark etc.)
Demonstrated strength in data modeling, ETL development, and data warehousing
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and computing
Proficiency in SQL
Proficiency in Python or other similar languages
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative fields
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Excellent knowledge of Advanced SQL working with large data sets
Knowledge of Advanced Statistics and implementing ML models
Demonstrated ability to mentor junior team members in all aspects of their engineering skill-sets
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Experience communicating with senior management as well as with colleagues from engineering, analytics, and business backgrounds
Strong business acumen, proven ability to influence others, strong attention to detail, excellent organization skills, and ability to manage multiple projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1876277"
415,"Data Engineer, Creator Services",SoundCloud,"Los Angeles, CA","SoundCloud is a next-generation music entertainment company powered by an ecosystem of artists, fans, and thriving communities. As one of the world's most influential cultural platforms, SoundCloud holds a singular market position as both a music-streaming service with the largest catalog of music, and an artist services and distribution business to help artists grow long-term, successful careers.

We're looking for an engineer to join our Creator Insights team in Los Angeles and New York.

At SoundCloud we care about creators. We enable them to engage with their audience by visualizing insights and getting paid accordingly. We deliver accurate reporting to our partners in a timely manner.

You are passionate about building efficient data pipelines, with a solid background in SQL, Python and Scala. You would actively contribute to simplifying the visibility of revenue and provide helpful insights for internal and external users.

You will also help us to improve our reporting infrastructure based on Airflow/BigQuery. You are a self-directed learner, team-player and passionate about software development. You are able to build and voice your opinion, and you're open to receiving feedback and learning from it.

About us:
We are a multinational company with offices in the US (New York and Los Angeles), Germany (Berlin), and the UK (London)
We provide a flexible work culture that offers the opportunity to collaborate and connect in person at our offices as well as accommodating work from home
We are deeply committed to ensuring diversity, equity and inclusion at all levels of our organization and fostering a community where everyone's voice, perspective and experience is respected and heard
We believe a strong team is made by investing in employees through mentorship, workshops and enrichment opportunities

Benefits:
Comprehensive health benefits including medical, dental, and vision plans, as well as mental health resources
Robust 401k program
Employee Stock Ownership Plan
Generous professional development allowance
Interested in a gym membership, photography course or book? We have a Creativity and Wellness benefit!
Flexible vacation and public holiday policy where you can take up to 35 days of PTO annually
16 paid weeks for all parents (birthing and non-birthing), regardless of gender, to welcome newborns, adopted and foster children
Various snacks, goodies, and 2 free lunches weekly when at the office

Diversity, Equity and Inclusion at SoundCloud

SoundCloud is for everyone. Diversity and open expression are fundamental to our organization. With this foundation, we aim to build a social platform and global community for everyone to create, discover, and share sounds. We acknowledge the challenges in our industry and strive to develop an inclusive culture where individual contributions are valued. We are dedicated to creating an inclusive environment for everyone, regardless of gender identity, sexual orientation, race, ethnicity, migration background, national origin, age, disability status, or care-giver status.

At SoundCloud you can find your community or elevate your allyship by joining a Diversity Resource Group (groups focused on people of color, LGBTQIA+ folks, and women). You may also participate in inclusive workshops, contribute to our Cultural Moments series, select organizations for the SoundCloud Community Fund to support, and more!"
416,Data Engineer,Deloitte,"Cleveland, OH","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
7+ years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
417,Data Engineer - Opportunity for Working Remotely,VMware,"Chicago, IL","The Elevator Pitch: Why will you enjoy this new opportunity?

You want to be a part of an innovative company of 20000+ people working in 50+ locations worldwide and committed to building a community where great people want to work long term by living our values of passion, innovation, execution, teamwork, active learning and giving back. If you are ready to accelerate, innovate and lead, join us as we challenge constraints and problem solve for tomorrow today.

You are highly motivated and would love to be part of VMware Data Engineering team working to solve complex business problems and bring digital transformations

What is primary need, technical challenge, and/or problem you will be responsible for?

VMware Data Engineering team is seeking a highly motivated, experienced Data Engineer within the IT Data Engineering and Analytics group. This position is responsible for hands on development work on all aspects of Data Engineer, data provisioning, modeling, performance tuning and optimization. The candidate will work closely with both Enterprise and Solution Architecture teams to translate the Business/Functional requirements into technical specifications that drive Hadoop/HANA/BI solutions to the meet functional requirements.

Success in the Role: What are the performance goals over the first 6-12 months you will work toward completing?

Within the first few months you will spend time learning VMware’s coding standards, products, and increasing you know how of the technology landscape around data. We want you to be curious, learning both from team members and individual study. You will collaborate with other team members and participate in architecture reviews.

You will closely work with other data product owners/engineers towards taking ownership of few existing artifacts within the data landscape. You will be required to help in troubleshooting any upcoming production defects and perform production support. You will also work on delivering specific enhancements in an agile delivery model

What type of work will you be doing? What assignments, requirements, or skills will you be performing on a regular basis?

You will work in a fast paced and agile work environment.
You will communicate and engage with a range of stakeholders.
You will be responsible for hands on development work building scalable Data engineering pipelines and other data engineering/modelling work using one or more of Python, Kafka, Hadoop/Hive, Presto etc.
You will have to query data using SQL or other techniques. Excellent SQL & Analytical SQL functions knowledge will be needed
Understanding of SAP HANA and Knowledge of Data Integration Platforms - Informatica PowerCenter, SAP BODS, SDI, SLT (is desired but not mandatory) and will help you in understanding existing landscape
Bachelor’s degree or equivalent with 4+ years of Data Engineering experience in Big Data Solutions is required for this role. MS degree would be highly desirable
You will be owner of specific modules. You will collaborate with other team members on improving dev practices, do peer code reviews and provide production support

What is the leadership like for this role? What is the structure and culture of the team like?

This role reports into Sapan Bajpai who is a Senior Manager for IT Data Engineering and Analytics.

IT Data Engineering and Analytics team is spread across VMware offices in Bangalore, Chennai, Palo Alto(USA) , Austin(USA) , Cork(Ireland), Beijing(China) and Costa Rica

The team is headed by Director, IT Data Engineering and Analytics based in Palo Alto

What are the benefits and perks of working at VMware?

You and your loved ones will be supported with a competitive and comprehensive benefits package. Below are some highlights, or you can view the complete benefits package by visiting www.benefits.vmware.com.

Employee Stock Purchase Plan
Medical Coverage, Retirement, and Parental Leave Plans for All Family Types
Generous Time Off Programs
40 hours of paid time to volunteer in your community
Rethink's Neurodiversity program to support parents raising children with learning or behavior challenges, or developmental disabilities
Financial contributions to your ongoing development (conference participation, trainings, course work, etc.)
Healthy and local inspired snacks in all our on-site pantries

Category : Engineering and Technology

Subcategory: Software Engineering

Experience: Manager and Professional

Full Time/ Part Time: Full Time

Posted Date: 2022-04-19

VMware Company Overview: At VMware, we believe that software has the power to unlock new opportunities for people and our planet. We look beyond the barriers of compromise to engineer new ways to make technologies work together seamlessly. Our cloud, mobility, and security software form a flexible, consistent digital foundation for securely delivering the apps, services and experiences that are transforming business innovation around the globe. At the core of what we do are our people who deeply value execution, passion, integrity, customers, and community. Shape what’s possible today at http://careers.vmware.com.

Equal Employment Opportunity Statement: VMware is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind: VMware is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at VMware are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. VMware will not tolerate discrimination or harassment based on any of these characteristics. VMware encourages applicants of all ages. Vmware will provide reasonable accommodation to employees who have protected disabilities consistent with local law."
418,Data Engineer - Google Cloud,Algo,United States,"Posted by
Phil Dean
Director, People & Culture at Algo | Supply Chain Intelligence | B2B SaaS | We're Hiring!
Send InMail
Data Ingestion
Partner with IT source teams to ingest data from varied sources into a big data environment
Establish processes and workstreams to ingest data while preserving data integrity
Act as a key partner and stakeholder in new data initiatives
Manage data pipelines from source systems to a big data environment

Data Architecture
Design data models and architectures with standards for consistency and integrity
Develop and maintain processes to preserve robust data supply chains

Master Data Management
Develop and maintain detailed data dictionaries for all data sources
Partner with data quality and data governance to ensure proper data management
Manage user access and change management within the MDM environment

Teamwork
Ensure data architecture and management aligns with organization goals
Act as a technical anchor and integrator for data operations and IT teams
Partner with stakeholders to develop data integration requirements and strategies

Lead New Data Integrations
Lead discussions as a key stakeholder to ensure all new business initiatives have data architecture at the forefront
Develop and create new data processes and procedures to simplify and scale data integration

Qualifications
Bachelor’s Degree in Information Technology, Information Systems, Computer Science, or related field
4+ years of relevant work experience
Experience with programming languages (SQL, Python)
Experience with Google Cloud
Experience with big data platforms, data architecture, data modeling
Experience profiling and curating large datasets
Experience integrating data sourced from multiple sites, technologies, and platforms
Ability to manage and enforce data standards, relationships, and processes
Strong self-starter with a resilient, solution minded approach to complex problems
Ability to handle multiple priorities to meet multiple deadlines"
419,Data Engineer III - International,Walmart,"Bentonville, AR","Position Summary... What You'll Do...

At Walmart, we help people save money, so they can live better. This mission serves as the foundation for every decision we make and drives us to create the future of retail. We can’t do that without the best talent – talent that is innovative, curious, and driven to create exceptional experiences for our customers.

Do you have boundless energy and passion for engineering data used to solve dynamic problems that will shape the future of retail? With the sheer scale of Walmart’s environment comes the biggest of big data sets. As a Walmart Data Engineer, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers’ lives.

Qualifications
3+ years of experience with 1+ years of Big data development experience
Experience in HDFS, Hive, Hive UDF’s, MapReduce, Druid, Spark, Python, Hue, Shell Scripting, Unix
Demonstrates expertise in writing complex, highly optimized queries across large data sets
Retail experience and knowledge of commercial data is a huge plus
Experience with BI Tool Tableau or Looker is a plus
Responsibilities
Design, develop and build database to power Big Data analytical systems.
Design data integration pipeline architecture and ensure successful creation of the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Spark, SQL, HQL and other technologies.
Build robust and scalable applications using SQL, Scala/Python and Spark.
Create real time data streaming and processing using Kafka and/or Spark streaming.
Work on creating data ingestion processes to maintain Global Data lake on Google cloud or Azure
Engage with architects and senior technical leads to create and enhance complex software components.
Design, configure and implement systems that can scale to process terabytes of data between heterogeneous systems on premise and cloud.
Work with business customers, product managers and engineers to design feature-based solutions and implement them in an agile fashion.
Develop proof-of-concept prototype with fast iteration and experimentation.
Develop and maintain design documentation, test cases, performance and monitoring and performance evaluation using Git, Crontab, Putty, Jenkins, Maven, Confluence, ETL, Automic, Zookeeper, Cluster Manager
Perform continuous integration and deployment using Jenkins and Git
The above information has been designed to indicate the general nature and level of work performed in the role.  It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.

Benefits & Perks:

Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.

Equal Opportunity Employer

Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.

Who We Are

Join Walmart and your work could help over 275 million global customers live better every week. Yes, we are the Fortune #1 company. But you’ll quickly find we’re a company who wants you to feel comfortable bringing your whole self to work. A career at Walmart is where the world’s most complex challenges meet a kinder way of life. Our mission spreads far beyond the walls of our stores. Join us and you'll discover why we are a world leader in diversity and inclusion, sustainability, and community involvement.  From day one, you’ll be empowered and equipped to do the best work of your life.  careers.walmart.com

About Global Tech

Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.

We’re virtual

Working virtually this year has helped us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Today, we are reimagining the tech workplace of the future by making a permanent transition to virtual work for most of our team. Of course, being together in person is an important part of our culture and shared success. We’ll collaborate in person at a regular cadence and with purpose.

Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, Master’s degree in Computer Science or related field and 2 years' experience in software engineering or related field

Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

As permitted by applicable law, provide evidence of full vaccination as defined by CDC guidelines OR secure approval of medical or religious accommodation for the vaccination mandate., Option 1: Bachelor’s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years’ experience in

software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, Master’s degree in Computer Science or related field and 2 years' experience in software engineering or related field Primary Location...8TH AND J STREETS, BENTONVILLE, AR 72712-0000, United States of America"
420,Data Engineer (ADF/Pyspark) - Remote Now,Dice,"Las Vegas, NV","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Maxonic, Inc., is seeking the following. Apply via Dice today!

Remote Now. Data Engineer (ADFPyspark) We are looking for a Data engineer with developer skills who will be working in a Devops environment and is going to support applications, provide enhancements to existing code base, triage production problems. This will be done primarily in Azure Data Factory, copy pipelines and using Pyspark. Other skills needed Notebook, Data Bricks, Python. PRINCIPAL DUTIES AND RESPONSIBILITIES Develop production-grade data engineering code through vision, definition, development, deployment and sustainment. Partner with Product Management to ensure data design, engineering and implementation is in accordance with design intention and meets business needs. Work with technology leadership to identify new technologies and techniques that can improve our overall platform performance and ultimate customer experience. Analyze product tradeoffs and deliver simple and intuitive products that just work. This person will open ADF, open pipelines, dig into code, find what caused the issue , escalate if necessary. A big part of this role is data provisioning. REQUIRED Bachelor's degree in Computer Science, Engineering or a related technical field. Two (2) years of experience min on delivering data integration, data engineering services and data platform solutions. Strong experience with ADF and copy pipelines Experience with Python, Pyspark, Data Bricks and Notebook. PREFERRED Previous experience with data warehouse integration and data engineering using real-time messaging, batch ETLELT, etc. in data management discipline. Previous working experience using cloud environments and various cloud services. Previous experience using various scripting tools and programing languages and experience working in a DevOps culture. Previous experience working in the hospitality industry."
421,AWS Data Engineer,Deloitte,"Philadelphia, PA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines.
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business.
Establish solid design and best engineering practice for engineers as well as non-technical people.
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
5+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
422,Data Engineer Intern,Western Union,"Denver, CO","Data Engineer Intern – Working from Home, USA

Are you looking for work experience with a global Fortune 500 company? Do you want to build transferable skills while learning about the commercial aspect of running a business? Join Western Union as a Data Engineer Intern.

Motivated by our values: purpose driven, globally minded, and trustworthy & respectful

We’re a FinTech that’s using insight from customers and colleagues worldwide to improve financial services for families, small businesses, multinational corporations, and non-profit organizations. We’re a team of over 10,000 people spanning 200 countries and territories. We believe when money moves, better things happen.

Better requires process, discipline, and simplification

The virtual Western Union HQ internship program provides current college and graduate students the opportunity to experience our culture of service while challenging themselves through a structured blend of individual project work, deep-dive sessions, networking opportunities, and enriching group projects. Our paid, 12-week summer session will run from May 31st – August 19th, 2022, which tentatively includes a sponsored trip to our Denver headquarters the week of July 25th.

[* To apply, you must be enrolled full-time in a post-secondary school, adult education, college, or certificate/training program.]

Better is enabled by trust and respect

This is a rare opportunity for someone with no previous office experience to learn, create and support our Data Management team.

Skills

While you may or may not have professional work experience, we are looking for you to bring the following general skills to the table
Energetic, with a passion
Strong analytical, communication, and problem-solving skills.
Actively pursuing a Bachelor's degree in computer science, computer engineering or a related field
Ability to think logically and structurally and have attention to detail is required
Ability to work effectively and efficiently in high performing and self-governing teams
You will gain experience in different areas working with the Data Management team.

In Addition, You Will Be Involved With
Data Engineering
Chat Box for our WU.com, Contact Us page for 40 Countries
Loyalty Ecosystems
Block chain/crypto offerings
Join us, and let’s move money for better

Western Union is transforming its business and shaping the financial services sector by driving quality, convenience, and customer service to new levels of excellence. It’s an exciting time for our organization, as the largest cross-border money transfer operator, trusted by millions of consumers around the world. If you’re ready to unleash your potential to help drive change through bottom-up innovation, apply now.

We are passionate about our diversity. Our commitment is to provide an inclusive culture that celebrates the unique backgrounds and perspectives of our global teams, while reflecting the communities we serve. We do not discriminate on the basis of race, color, national origin, religion, political affiliation, sex (including pregnancy), sexual orientation, gender identity, age, disability, marital status, or veteran status.

For internship: The hourly rate for this position is $25.00/hour"
423,"Data Engineer, Spark",Deloitte,"Panama City, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
424,Senior Data Engineer,Vanguard,"Malvern, PA","The start of a new year can be the start of a new and exciting opportunity in your career! Be part of the DREAM Team in Enterprise Advice (EA) that leverages cutting-edge technology to bring data to life! If you like to work in a fast-paced, exciting environment where you will be challenged to approach solutions in new and creative ways, work on a high-performing team, and want to be a key player in our Advice technology space, then this is the place for you.

This role will provide expert level data solutions by using software to process, store, and serve data to others. Tests data quality and optimizes data availability. Ensures that data pipelines are scalable, repeatable, and secure. Utilizes the deepest dive analytical skillset on a variety of internal and external data. Leads, instructs, and mentors newer Data Engineering crew.

Core Responsibilities

Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
Participates in special projects and performs other duties as assigned.

Qualifications

Minimum of eight years data analytics, programming, database administration, or data management experience.
Undergraduate degree or equivalent combination of training and experience. Graduate degree preferred.

An ideal candidate will have Spark, NumPy, Hadoop, Hive. Experience in AWS infrastructure.

We are data engineers creating empowerment for business in the form of higher standard data products and outstanding data services with best in class data engineering solutions.

Special Factor

Vanguard is not offering visa sponsorship for this position.

About Vanguard

We are Vanguard. Together, we’re changing the way the world invests.

For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.

We want to make success accessible to everyone. This is our opportunity. Let’s make it count.

Inclusion Statement

Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”

We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.

When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.

Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.

Future of Work

During the pandemic, we transitioned to a work from home model for the majority of our crew and we continue to interview, hire, and on-board future crew remotely.

As we have developed the path forward, we have taken a thoughtful approach that both maximizes the advantages of working remotely and the many benefits of coming together and collaborating in a shared workspace. We believe that in-person interactions among our crew are important for preserving our unique culture and advantageous for the personal development of our crew.

When our Crew return to the office, many will work in our hybrid model. A smaller proportion of our crew will operate in the Work from Home work model (for example, field sales crew); or in the Work from Office model (for example, portfolio managers).

The working model that your role falls into will be communicated to you in the interview process – please do ask if you are unsure. We encourage you to make the decision regarding your job interview and offer knowing which model your role will fall into. We will test and learn as our ways of working evolve and will continue to evaluate working models along the way."
425,Data Engineer,Amazon,"Tempe, AZ","Job Summary

DESCRIPTION

Our team is passionate about Brands who sell on Amazon - we help them grow their businesses, build their story, and serve their customers. How do we do this? Data! Help us serve this valuable data to our Brands in digestible ways so they can run their businesses more effectively.

The ideal candidate will have excellent problem investigation abilities, and the ability to synthesize data into crisp and clear recommendations for scientists and product leaders. To be successful in this role, you should have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business needs.

In the role, you will work closely with scientists, product managers and software engineers to build out infrastructure, data pipelines, and reporting mechanisms for our team and our Brands.

Our Data Engineer Duties & Responsibilities Will Include

Design and deliver big data architectures for experimental and production consumption between scientists and software engineering

Develop the end-to-end automation of data pipelines, making datasets readily-consumable by visualization tools and notification systems.

Create automated alarming and dashboards to monitor data integrity.

Create and manage capacity and performance plans.

Act as the subject matter expert for the data structure and usage.


Basic Qualifications
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline.
4+ years of industry experience in Software Development, Data Engineering, Business Intelligence, Data Science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
Hands-on experience and advanced knowledge of SQL.
Experience in Data Modeling, ETL Development, and Data Warehousing.Experience using business intelligence reporting tools (Power BI, Tableau, Cognos, etc.).
Experience using big data technologies (Hadoop, Hive, Hbase, Spark, EMR, etc.).
Knowledge of Data Management fundamentals and Data Storage fundamentals.
Experience coding and automating processes using Python or R.
Strong customer focus, ownership, urgency, and drive.
Excellent communication skills and the ability to work well in a team.
Effective investigation, troubleshooting, and problem-solving skills.
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative fields.
Experience working with AWS big data technologies (Redshift, S3, EMR, Glue).
Proven success in communication with users, other technical teams, and senior management to collect requirements, describe data modeling choices and data engineering strategy.
Experience providing technical leadership and supporting other engineers for best practices on data engineering.
Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1881097"
426,Senior Data Engineer (Python/SQL),The Execu|Search Group,"Whippany, NJ","A health services company in New Jersey is currently seeking a new Data Engineer (Python/SQL) for a Senior-level opportunity with their staff. In this role, the Senior Data Engineer (Python/SQL) will be responsible for helping to scale the company's data ingestion pipelines at the core of the Enterprise Data Platform which supports client reporting as well as the Internal Analytics & Operational teams.

Responsibilities

The Senior Data Engineer (Python/SQL) will:
Work with Senior Leadership, Architects, Engineers, Data Analysts, Product Managers and Cloud Infrastructure teams to deliver a new features and capabilities
Write clean, robust, and well-thought-out code with an emphasis on quality, performance, scalability, and maintainability
Demonstrate strong end to end ownership & craftsmanship - analysis, design, code, test, debug, and deploy
Perform other duties, as needed
Qualifications
3+ years of related work experience
Strong Python & Strong SQL
Extensive relational DB experience (Redshift, SQL Server, PostgresSQL) with exposure to document DBs such as DynamoDB. ElasticSearch.
Experience with designing solutions that run in AWS cloud technologies (Lambda, ECS, DynamoDB etc), docker containers
Experience with Change data capture concepts, Database Triggers, AWS DMS
Experience with Data lake concepts, data catalogs, meta data etc
Great interpersonal skills
Excellent communication skills (written and verbal)
Strong attention to detail
Highly organized"
427,"Data Engineer, ASK International Data",Amazon,"Boston, MA","Job Summary

DESCRIPTION

We are seeking a customer-obsessed Data Engineer to be part of the Spoken Language Understanding science team for Alexa Skills Kit. This role will work as part of our International Data team to deliver our technical strategy by defining how we store and organize data, and build data pipeline to power machine learning, analytic and BI solutions to our hardest customer-facing problems. Your work with directly impact the developer experience in building skills, as well as the customer experience when interacting with them.

Key job responsibilities
Collaborate with colleagues from linguistics, science, engineering, and business backgrounds to collection requirements and provide data solutions
Use standard technologies (SQL, Python, Spark, AWS services) to develop stable and scalable data pipeline to support tooling and analytic/BI reporting
Ensure data quality throughout all stages of acquisition and processing
Design and build data product with best practices, be compliant with data privacy and data security requirements, and ensure customer trust
A day in the life

This role requires working closely with Language Data Researcher, Language Engineers, Solutions Architects, and Product/Program Managers, to drive analysis and performance improvements for our speech and language technology. A successful candidate will be a self-starter, comfortable with ambiguity, with strong attention to detail, ability to work in a fast-paced and ever-changing environment, self-driven, constantly seeking new ideas to improvement performance, independent and good at project/time management.

About The Team

Our team is made up of Language Data Researchers, Data Scientists, Language Engineers, BIE, Team Managers, Program/Product Managers, and Data Specialists, with the goal of understanding and improving the Spoken Language Understanding (SLU) experience for ASK users and developers.


Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, or a related field or 4+ years industry experience
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building data pipelines
Experience in SQL, Python, AWS services
Demonstrated strength in data modeling, ETL development, and data warehousing.
Data Warehousing Experience with Oracle, Redshift, PostgreSQL, etc.
Preferred Qualifications
Industry experience as a Data Engineer or related fields (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with 4+ years’ experience in building data pipeline and processing large datasets.
Experience building distributed systems for data extraction, ingestion, and processing from multiple data sources
Experience leading large-scale data warehousing and analytics projects using AWS technologies, such as Redshift, S3, EC2, State Machines, etc.
Proficient in Python, Spark or Scalar
Proficient in Linux environment
Experience providing technical leadership for the best practices and policy compliance
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2006556"
428,Data Engineer - Global Shipping Analytics,Amazon,"Bellevue, WA","Job Summary

DESCRIPTION

Over the past 20 years, Amazon has reinvented on behalf of customers and has become the largest internet retailer in the world. Amazon is now reinventing the global supply chain and international e-commerce and is recruiting a data engineer to help make this vision a reality.

The Global Mile team’s mission to provide the earth’s most customer-eccentric global shipping experience for Amazon’s buying customers, selling partners, internal Amazon businesses, and external shippers. We enable this by leveraging cutting edge technology to building cost-effective, convenient, and delightful global logistics products that connect global manufacturing locations, distribution centers, and customers worldwide. This multicultural, multilingual team works with vendors, sellers, providers, governments, and trade officials from the busiest to the most remote locations around the world, applying a global perspective with a local approach. Our rapidly expanding product portfolio impacts hundreds of thousands of sellers and vendors and tens of millions of customers around the world (and it is still only Day One).

We are seeking an experienced candidate to develop and manage the data pipeline for our Global Shipping businesses. You will work in a multidisciplinary team, alongside PMs, BIEs, DSs, and SDEs to build a new data platform for a new global logistics program. You will collaborate with various internal stakeholders (including Operations, Sales/Marketing, Supply Chain, Finance, Accounting, Product Managers and senior leaders. You will use a wide-variety of AWS technologies such as AWS Redshift, QuickSight, Glue, S3, EC2, EMR, DynamoDb along with Spark, Docker and some Amazon internal frameworks and tools. We are looking for an outstanding individual who combines superb technical, communication, and analytical capabilities with a demonstrated ability to get the right things done quickly and effectively.

Key job responsibilities

In This Role, You Have The Opportunity To
Design, implement and operate large-scale, high-volume, high-performance structures for analytics and science.
Implement ingestion routines both real time and batch using best practices in modeling, ETL/ELT processes by leveraging AWS technologies and big tools (e.g. Glue, Airflow, Kinesis. S3)
Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions with a flexible and adaptable architecture.
Collaborate with Software Development Engineers to help adopt best practices in system creation, integrity, test design, analysis, validation, and documentation
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service modeling and production support for customers.

Basic Qualifications
2+ years of experience analyzing and interpreting data and experience with Redshift, Oracle, NoSQL etc.
Experience with data modeling, data warehousing, and building ETL pipelines
3+ years of experience as a Data Engineer or in a similar role
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing
Experience in at least one modern object-oriented programming language (Ruby, Python, Java)
Experience with Redshift, Oracle, NoSQL etc.
3+ years of experience as a Data Engineer or in a similar role.
Experience with AWS Technologies to build ETL pipelines (e.g. Glue, Airflow, Kinesis. S3)
Bachelor’s degree in Computer Science or related field, or equivalent experience.
4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.
Experience using big data technologies (Hadoop, Hive, HBase, Spark, EMR, etc.).
Knowledge of data management fundamentals and data storage principles.
Experience in building CI/CD pipelines
Expertise in Python, Scala, Java
Preferred Qualifications
5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.
Demonstrated strength in data modeling, ETL development, and data warehousing.
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.
Experience providing technical leadership and mentoring other engineers for best practices on data engineering.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1869664"
429,Senior Data Engineer,NVIDIA,"Santa Clara, CA","Posted by
Sumit Garg
Sr Staff - Data Analytics & Automation at NVIDIA
Send InMail
NVIDIA is hiring senior distributed systems with a data engineering emphasis to develop and scale its AI and deep learning platforms. Our team is building a software 2.0 developer platform with a focus on datasets for AI application development. Together, we will advance NVIDIA's capacity to build and deploy leading solutions for a broad range of AI-based applications such as autonomous vehicles, healthcare, virtual reality, graphics engines, and visual computing. Together, with NVIDIA partners, we will bring autonomous vehicles to life!
What You'll Be Doing
Architect and build scalable and distributed commitment to improvement, compute, and data pipelines that will help power the IT Data Lake as a centralized data platform
Design and build PB sized scalable data lake and structured/unstructured data query interfaces and microservices to ingest, index, mine, transform, and compose large datasets.
Build and implement support for versioned, traceable, and immutable datasets in a data lake in a distributed and scalable manner.
Enable efficient and thoughtful data selection - one of the key ingredients for successful machine learning!
Hands-on writing code of high quality, good design & architecture, fully tested and peer reviews.
Collaborate with multiple product/engineering teams to understand their data and compute requirements (SW, HW, Automobile, AI) to integrate amazing innovations and algorithms into our production systems
Automate everything for measuring, testing, updating, monitoring, and alerting the data platform
What We Need To See
Bachelors (or equivalent experience) or Masters in Computer Architecture, Computer Science, or related data-intensive Engineering Degree.
8+ years of proven experience in Data Engineering, worked on designing and developing software with Big Data, Data Lake/ Lake House ecosystem, Data Analytics, backend microservices architecture, and heterogeneous data types at scale
Proven in-depth experience in creating ETL pipelines using Databricks, Spark, Python, SQL, Scala, Kafka, Presto, Parquet, Streaming, events, bots, AWS/cloud ecosystem
Proficient in developing Micro Services and using AWS frameworks such as SQS, Stream, Kubernetes, EC2, S3, Lambda etc
Experience with data pipelines/analysis/visualization tooling such as Elastic stack, Logstash, Kibana, Kafka, Grafana, Splunk, Pandas, Message brokers, Data modeling.
Expertise in Data Lakehouse architecture and end-to-end Databricks techniques including Data Science components
Worked on end-to-end data lifecycle from Data Ingestion, Data Transformation, and Data Consumption layer. Versed with API and its usability
Knowledge of Cloud solutions like Kendra, SageMaker, Auto-ML, Big Query, RedShift, Glue, Athena.
Ways To Stand Out From The Crowd
Expert in Spark, Parquet, steaming, events, Kafka, telemetry, MapReduce, Hadoop, Hive, Presto, Spark, data query approaches, and dashboarding.
The one who have implemented Enterprise use cases like CMDB, Governance, time series classification, telemetry anomaly detection, logs, and real-time data ingestion through APIs
Experience with structured data such as Avro, Parquet, Protobuf, Thrift, and concepts like schema evolution.
Working knowledge of Amazon Web Services, Kubernetes, Docker is a plus.
NVIDIA is widely considered to be one of the technology world’s most desirable employers. We have some of the most forward-thinking and hardworking people on the planet working for us. If you're creative and autonomous, we want to hear from you!
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law."
430,Data Engineer,Underdog.io,"San Francisco, CA","We're looking for a Data Engineer to join a company in the Underdog.io network.

The Underdog.io network is a curated group of some of the fastest growing startups and tech companies in the country. We actively turn away more than 50% of companies that attempt to join.

We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active.

Our companies look for Data Engineers proficient in Python, Java, SQL, Hadoop, C++ and more. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many. Many of our companies are looking for mid-to-senior level talent, both individual contributors and managers.

To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free.

If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer.

Apply today!

Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.

C++,Python (Programming Language),Java,Data Products,Pipelines,Hadoop,MapReduce,SQL,Data Warehousing,Data Architecture"
431,"Data Engineer, Snowflake",Deloitte,"Huntsville, AL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
432,AWS Data Engineer,Deloitte,"Miami, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
433,Data Engineer,Verition Fund Management LLC,New York City Metropolitan Area,"Summary:
The firm is looking to hire a data engineer to join in the building of the firm’s data reservoir program.
The right candidate will be motivated, hard working, and curious to learn new technologies. In short, this is a fantastic opportunity for the right candidate to join the build out of a top tier hedge fund’s data program. The role’s responsibilities include:
· Carrying out the design, development, and implementation of the firm’s real-time and historical data capture, storage and retrieval systems
· Designing, writing, and automating data integrity and optimization routines to optimize the firm’s data analysis and research
Required:
· Bachelor’s degree required
· 4-5 year’s work experience (preferably in financial sector)
- Traditional databases sql/no sql
- Programming Language expertise in Python/Scala/Scripting
- ETL tools
· 2+ year’s work experience with the following (MUST HAVE)
- AWS
- Lake Formation
- Glue
- RDS
o SQL and No-SQL DBS
- Understanding of server-less architecture"
434,Data Engineer,Amazon,"Dallas, TX","Description

We are looking for an outstanding Data Engineer who is data-driven, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed. You have passion for technology. You are keen to leverage existing skills while trying new approaches. You are not tool-centric; you determine what technology works best for the problem at hand and apply it accordingly. You can explain complex concepts to your non-technical customers in simple terms.

We are moving from traditional database technologies to near real time data processing and advanced reporting services built around natural language processing and machine learning.

You will help us analyze large amounts of data, discover and solve real world problems and build metrics and business cases to help business teams to make decisions. You should be motivated self-starter that can work independently in a fast paced, ambiguous environment.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, scripting, data warehousing, and building ETL pipelines
Experience in SQL
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Preferred Qualifications
Experience in big-data using Hadoop, Hive, and other open-source tools/technologies
Familiar with AWS tools
Ability to work independently with minimum supervision
Experience in designing and building large data warehouse systems
Strong organizational and multitasking skills with ability to balance competing priorities
Good work experience in BI Reporting tools and databases in a business environment.

Company - Amazon.com Services LLC

Job ID: A1735235"
435,"Data Engineer, Snowflake",Deloitte,"Pittsburgh, PA","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
436,"Data Engineer, Spark",Deloitte,"Omaha, NE","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced Data Engineer - Spark, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities

As a Spark Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

The Core Technology Operations delivers large scale software applications and integrated systems and assists its clients with architecture design, assessment and optimization, and definition. The practice aims at developing service-oriented architecture (SOA) and other integration solutions to enable information sharing and management between business partners and disparate processes and systems. It would focus on key client issues that impact the core business by delivering operational value, driving down the cost of quality, and enhancing technology innovation.

Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Limited immigration sponsorship may be available."
437,Senior Data Engineer,Publicis Sapient,"New York, NY","Publicis Sapient is looking for a Senior Associate Data Engineer to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.

Your Impact:

Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our clients’ business
Translate clients requirements to system design and develop a solution that delivers business value
Lead, design, develop and deliver large-scale data systems, data processing and data transformation projects
Automate data platform operations and manage the post-production system and processes
Conduct technical feasibility assessments and provide project estimates for the design and development of the solution
Mentor, help and grow junior team members

Qualifications

Your Skills & Experience:

Demonstrable experience in data platforms involving implementation of end to end data pipelines
Good communication and willingness to work as a team
Hands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)
Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)
Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.
Ability to handle module or track level responsibilities and contributing to tasks “hands-on”
Experience in data modeling, warehouse design and fact/dimension implementations
Experience working with code repositories and continuous integration
Set Yourself Apart With:

Developer certifications for any of the cloud services like AWS, Google Cloud or Azure
Understanding of development and project methodologies

Additional Information

Benefits of Working Here:

Flexible vacation policy; time is not limited, allocated, or accrued
Generous parental leave and new parent transition program
Tuition reimbursement
Corporate gift matching program
As part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to Equal Employment Opportunity without regard for race, color, national origin, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity, or religion. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at hiring@publicissapient.com or you may call us at +1-617-621-0200."
438,Data Engineer with Poly,General Dynamics Information Technology,"Herndon, VA","Type Of Requisition

Job Description:

Regular

Clearance Level Must Currently Possess

Top Secret SCI + Polygraph

Clearance Level Must Be Able To Obtain

Top Secret SCI + Polygraph

Suitability

Polygraph

Job Family

Public Trust/Other Required:

Data Science

Job Description

We are GDIT. As one of the largest IT and mission services providers to the government, we own our opportunities to better enable healthcare organizations to identify theirs.

You can make GDIT your place. You make it your own by turning obstacles into action. By owning your opportunity at GDIT, you’ll play an important role in providing the technologies and services that millions of healthcare professionals depend on, every day. Our work depends on a Data Engineer joining our team to support Customer activities at Herndon.

At GDIT, we put our people first. As a Data Engineer supporting the Customer, a typical day will include:
Assembling large, complex data sets that meet functional / non-functional business requirements
Processing large volumes of data
Building the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies
Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Present information using data visualization techniques
Work directly with engineers to complete tasks
Undertake data collection, preprocessing, and analysis
What You’ll Need
Bachelor’s Degree in a related business or technical discipline, or the equivalent combination of education, technical training, or work/military experience
Ability to comprehend database methodologies
Focus on continual process improvement with a proactive approach to problem solving
Ability to follow directions and finish task
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience building processes supporting data transformation, data structures, metadata, dependency and workload management.
Demonstrated success in manipulating, processing and extracting value from large, disconnected data sets.
Strong analytical and critical thinking skills.
Possess an active TS/SCI security clearance, with Polygraph
What Gdit Can Offer You
401K with company match
Customizable health benefits packages
Internal mobility team dedicated to helping you own your career
Challenging work that makes a real impact on the world around you
Full-flex work week
Not sure this job’s the one for you? Check out our other openings at gdit.com/careers

#OpportunityOwned

#GDITCareers

#GDITLife

#WeAreGDIT

#GDITInterns

Scheduled Weekly Hours

40

Travel Required

None

Telecommuting Options

Telecommuting Not Allowed

Work Location:

USA VA Herndon

Additional Work Locations:

COVID-19 Vaccination: GDIT does not have a vaccination mandate applicable to all employees. To protect the health and safety of its employees and to comply with customer requirements, however, GDIT may require employees in certain positions to be fully vaccinated against COVID-19. Vaccination requirements will depend on the status of the federal contractor mandate and customer site requirements.

We are GDIT. The people supporting some of the most complex government, defense, and intelligence projects across the country. We deliver. Bringing the expertise needed to understand and advance critical missions. We transform. Shifting the ways clients invest in, integrate, and innovate technology solutions. We ensure today is safe and tomorrow is smarter. We are there. On the ground, beside our clients, in the lab, and everywhere in between. Offering the technology transformations, strategy, and mission services needed to get the job done.

GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
439,Data Engineer | Direct Client | 100% Remote | 6-12 Months Contract,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Accion Labs, is seeking the following. Apply via Dice today!

Dear Professional, Hope you doing well, We are engaged with the client as a Partner, 6-12 Months Contract, 100 Remote EST Hours. One interview, and done. I tried reaching you for the below opportunity with my direct client, kindly revert back to me if you are available and interested. Thanks! Job Description Key Requirements 5 to 7 years working experience in Data integration and pipeline development with data warehousing. Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDBDynamoDB ecosystems Strong real-life experience in Python development especially in pySpark in AWS Cloud environment. Design, develop test, deploy, maintain and improve data integration pipeline. Experience in Python and common python libraries. Strong experience with Perl and Unix Scripts Power BI experience preferred Strong analytical experience with database in writing complex queries, query optimization, debugging, and user defined functions, views, indexes etc. Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools. Experience with continuous deployment(CICD) Databricks, Airflow and Apache Spark Experience is a plus. Experience with databases (Oracle, SQL Server, PostgreSQL, Redshift, MySQL, or similar) Strong experience with performance tuning, analytical understanding with business and program. Exposure to ETL tools including Informatica and any other . BSMS degree in CS, CE or EE. Rakesh Alakanti Sr. Recruiter mailto US India Singapore Malaysia UAE Australia UK Driving Outcomes Through Actions!"
440,Data Engineer,Amazon,"Seattle, WA","Description

The Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Fire tablets, Fire TV, Amazon Dash, and Amazon Echo.

What will you help us create?

The Team: How often have you had an opportunity to be a founding member of a team that is solving a significant problem through innovative technology? Would you like to know more about how we are envisioning the use of data analytics, machine learning, AI and linear programming to solve these problems? If this sounds intriguing, then we’d like to talk to you about a role on a new Amazon team that's tackling a set of problems requiring significant innovation and scaling.

We are seeking a Data Engineer with strong analytical, communication and project management skills to join our team. This role will be a key member of a Science and Data technology team based in Seattle, WA. Working closely with business stakeholders, software development engineers and scientist colleagues, you will design, evangelize, and implement state-of-the-art solutions for never-before-solved problems, helping Amazon Device to provide customer great products and keep the data secure. You will work with the most complicated data environment, employ right architecture to handle big data and support various analytics use cases, including business reporting, production data pipeline, machine learning, optimization models, statistical models, simulation, etc. Your work will have a direct impact on the day-to-day decision making in the Amazon Devices Sales & Operations Technology, and end customers.

You are an individual with outstanding analytical abilities, excellent communication skills, good business understanding, and technically savvy. The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity problems, can multi-task, and can credibly interface between technical teams and business stakeholders.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
5+ years of experience with data warehouse technical architectures, ETL/ ELT, reporting/analytic tools and, scripting.
5+ years of demonstrated quantitative and qualitative data experience with data modeling, ETL development
Knowledge of data modeling and experience SQL with Redshift, Oracle, MySQL, and Columnar Databases
Experience managing competing priorities simultaneously and driving projects to completion
Preferred Qualifications
Desire to create and maintain data warehouse systems
Experience with big data technologies
Master's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or Engineering
Experience with AWS services including S3, Lambda, EMR, RDS, Data-pipeline and other big data technologies
Experience with scripting (Python experience is a strong plus).
Proficient in the composition of Advanced SQL (analytical functions) and query performance tuning skills
Ability to interact, communicate, present and influence with both business and technical teams
A self-starter who loves data and who enjoys spotting the trends in it!
Amazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age

#d2ctech tag


Company - Amazon.com Services LLC

Job ID: A1566951"
441,Data Engineer - Data Marketplaces,Equifax,"Alpharetta, GA","Equifax is hiring a Data Engineer that combines software and systems engineering for building and running large-scale, distributed, fault-tolerant systems and ensuring the consistent delivery of data into multiple data marketplaces. This engineer ensures that internal and external services meet or exceed reliability and performance expectations while adhering to Equifax engineering principles.



What you’ll do
You will engage in and improve the software development lifecycle – from inception and design, through development, deployment, operation and refinement
You will influence and design infrastructure, architecture, standards and methods for large-scale data distribution into data marketplaces
You will create new data pipelines for delivering new and existing data sets into data marketplaces
You will support services prior to production via software platform development, capacity planning and launch reviews
You will maintain services during deployment and in production by measuring and monitoring key performance and service level indicators including availability, latency, and overall system health
You will automate system scalability and continually work to improve system resiliency, performance and efficiency

What Experience You Need
BS degree in Computer Science or related technical field involving coding (e.g., physics or mathematics), or equivalent job experience required
5+ years of experience developing and/or administering software in public cloud and/or integrating with data marketplaces
5+ years experience in monitoring infrastructure and application uptime and availability to ensure functional and performance objectives.
2 years experience with structured and unstructured data and accessing data using languages such as SQL or GraphQL
AWS
What could set you apart
Demonstrable cross-functional knowledge with systems, storage, networking, security and databases
You have supported a suite of data sets within Snowflake or another data marketplace
You have expertise designing, analyzing and troubleshooting large-scale distributed systems.
You’ve built software or maintained systems in a highly secure, regulated or compliant industry
(IAM, S3, Lambda, Redshift, Glue)

We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.

If this sounds like somewhere you want to work, don’t delay, apply today - we’re looking for you!

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran."
442,"Data Engineer - Amazon Lending, Lending Business Intelligence Infrastructure (BII)",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

Help us bring Amazon’s awesome customer experience to the financial industry! Lead the development of new systems for high impact product launches.

Amazon Lending is passionate about helping third party sellers on Amazon grow their business. Since inception, we have provided fast, easy, access to capital for small and medium sized businesses. We operate at scale and have lent billions of dollars world-wide. We are dedicated to supporting Amazon Sellers by focusing on meeting their credit needs while also seeking to understand how these capabilities can be extended to other B2B focused businesses within Amazon, such as AWS, Advertising, Last Mile, and Global Shipping.

We are a fast-paced data engineering team that builds scalable, high-quality ETL pipelines and database infrastructure to support the needs of our internal customers. We encourage innovation and expect engineers to take a high level of ownership throughout the development life cycle. Alongside ingestion, cleansing and publication of data, we are constantly looking to enhance our existing processes by improving performance and increasing automation.

Key job responsibilities

As a Data Engineer II on the Lending Business Intelligence Infrastructure (BII) team, you will work closely with a talented team of data engineers, as well as partners from a variety of job families (including product management, business intelligence engineering, marketing, finance, software development, data science and accounting). Together you will build and launch new financial products. You will lead the development of scalable ETL and data solutions to handle rapidly increasing workloads across multiple lines of business. You will simplify complex data and ingestion processes through best practices, experienced system design and innovation.


Basic Qualifications
Bachelor’s degree in computer science, engineering, mathematics, or a related technical discipline
4+ years of experience as a Data Engineer, BI Engineer or Business/Financial Analyst in a company with large, complex data sources
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Expertise in SQL, database and storage internals, SQL tuning, and ETL development
Ability to work and communicate effectively with developers and Business users
Strong organizational and multitasking skills with ability to balance competing priorities
2+ years of experiences scripting languages such as Python, Scala, Perl, etc.
Data engineering fundamentals in data set design and architecture
Preferred Qualifications
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with full software development life cycle, including coding standards, code reviews, source control management, build processes, and testing
Experience with AWS technologies (Lambda, RDS, Redshift, Athena, S3)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1896027"
443,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, iBridge Solutions LLC, is seeking the following. Apply via Dice today!

Our client is looking for a motivated Data Engineer for a 100 remote role. States we are able to work with are Florida, Illinois, Kentucky, Nebraska, Oklahoma, Texas, and North Carolina. Due to the nature of our client, Candidates must be s to be considered for the role. Responsibilities Write and optimize queries, provide input on peer code reviews and data modeling, deploy data updates via continuous integration deployment (CICD) practices. Collaborate with software developers, other data engineers, and database architects on projects to ensure optimal data delivery following appropriate change management guidelines. Collaborate with software and reliability engineers to automate repeatable data tasks. Work with development and production support teams to troubleshoot data, application, and service deployment issues. Develop detailed, shared technical procedure documentation, including processes for contingency operations. Research, provide technical input, document, and implement new technology solutions. Implement necessary patches, hotfixes, service packs, cumulative updates, upgrades, and configuration change to maintain security compliance and application integrity. Qualifications Bachelor's degree with a major or specialized courses in Information Technology or commensurate experience Strong verbal and written communication skills 2 years related experience with a combination of the following Knowledge of relational database management systems, such as Oracle RDBMS and PostgreSQL Knowledge of NoSQL, data warehousing solutions Issue tracking tools such as JIRA Strong analytical and problem-solving ability Familiarity with AWS cloud services (for example EC2, S3, RDS, Glue) Source control and leveraging a source code repository Red Hat Enterprise Linux, Oracle Solaris, or other UNIX Linux distributions. Data collection and databasereport design Programming Languages (Java and Python are preferred)"
444,Data Engineer/Data Scientist,Gainwell Technologies,New York City Metropolitan Area,"Job description:

About Us
At Gainwell, we are passionate about helping those most vulnerable get access to high-quality, affordable healthcare. We harness the power of technology to help our clients improve the health and well-being of the members they serve. We believe nothing is impossible when you bring together people who care deeply about making healthcare work better for everyone. Join us and build your career with an inclusive team that thrives on finding innovative solutions to some of healthcare’s biggest challenges.

Job Description
Essential Job Functions

• Responsible for discussing data extraction needs with each individual account and ensuring that data extracted meets the needs of the GIO application in order to process it
• Responsible for validating that data ingested by connectors created by Java/Kafka Developers is ingested correctly and makes sense from a business perspective
• Familiarity with SQL is important in order to query GIO(T-K) to ensure data quality is high. Experience with design and documentation of ETL pipelines preferred.
• Works with the account teams to ensure that after initial data is ingested that data being displayed in the application appears appropriate and explores any anomalies that may exist


Requirements/Qualifications:
• Design and document data pipelines/transformations required for each unique account connector
• An ability to learn and understand the GIO(TK) data schema and how data is processed by TK in order to ensure that data designs developed work well with product.
• An ability to interact with and understand data extraction pipelines created by Gainwell account teams to suggest changes to them to meet product requirement needs."
445,AWS Data Engineer,Deloitte,"San Jose, CA","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.
Work You'll Do/Responsibilities
Creating/managing AWS services
Work with distributed systems as it pertains to data storage and computing
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The AI & Data Operations team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

AI & Data Operations will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually."
446,Data Engineer,Amazon,"Arlington, VA","Job Summary

DESCRIPTION

The Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Fire tablets, Fire TV, Amazon Dash, and Amazon Echo.

What will you help us create?

The Team: How often have you had an opportunity to be a founding member of a team that is solving a significant problem through innovative technology? Would you like to know more about how we are envisioning the use of data analytics, machine learning, AI and linear programming to solve these problems? If this sounds intriguing, then we’d like to talk to you about a role on a new Amazon team that's tackling a set of problems requiring significant innovation and scaling.

We are seeking a Data Engineer with strong analytical, communication and project management skills to join our team. This role will be a key member of a Science and Data technology team based in Seattle, WA. Working closely with business stakeholders, software development engineers and scientist colleagues, you will design, evangelize, and implement state-of-the-art solutions for never-before-solved problems, helping Amazon Device to provide customer great products and keep the data secure. You will work with the most complicated data environment, employ right architecture to handle big data and support various analytics use cases, including business reporting, production data pipeline, machine learning, optimization models, statistical models, simulation, etc. Your work will have a direct impact on the day-to-day decision making in the Amazon Devices Sales & Operations Technology, and end customers.

You are an individual with outstanding analytical abilities, excellent communication skills, good business understanding, and technically savvy. The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity problems, can multi-task, and can credibly interface between technical teams and business stakeholders.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
5+ years of experience with data warehouse technical architectures, ETL/ ELT, reporting/analytic tools and, scripting.
5+ years of demonstrated quantitative and qualitative data experience with data modeling, ETL development
Knowledge of data modeling and experience SQL with Redshift, Oracle, MySQL, and Columnar Databases
Experience managing competing priorities simultaneously and driving projects to completion
Preferred Qualifications
Desire to create and maintain data warehouse systems
Experience with big data technologies
Master's degree in an engineering or technical field such as Computer Science, Physics, Mathematics, Statistics, or Engineering
Experience with AWS services including S3, Lambda, EMR, RDS, Data-pipeline and other big data technologies
Experience with scripting (Python experience is a strong plus).
Proficient in the composition of Advanced SQL (analytical functions) and query performance tuning skills
Ability to interact, communicate, present and influence with both business and technical teams
A self-starter who loves data and who enjoys spotting the trends in it!
Amazon is an Equal Opportunity Employer – Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age

#d2ctech tag

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A2028368"
447,"Data Engineer, AdsEcon",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The AdsEcon Team is looking for a Data Engineer to help and be part of a team that puts cutting edge economic and data science advertising research into production. We are looking for a unique individual who is interested in working with the latest big data technology (Spark, EMR, Glue, SageMaker, and Airflow) and collaborate with rockstar Economist and Data Scientist in creating disruptive solutions for our multiple Advertising Businesses.

Advertising is used daily to surface new selection and provide customers a wider set of product choices along their shopping journeys. Advertisers use Amazon Advertising to grow their businesses. Our team is on a mission to change how advertising campaigns are measured, so that advertisers can make most out of their investment in advertising. If we succeed, we will have a deep impact in Amazon.

Key job responsibilities
Partnering with economists and senior team members in building an environment of large high-quality datasets
Create and support AWS based data platforms that leverage diverse data sources across Amazon, that feed input to and serve output from machine learning / econometric models
Help us move fasted by building fast prototypes that help us prove out our ideas
Develop and extend reusable libraries for quickly rolling out new data pipelines
Create and drive data governance strategies for disparate data sources across the company
Create and review technical requirement and design documents, working backward from customer needs
No On Call. Just focus on building
About The Team

We are a central team of Economists, Data Scientists, and Engineers in GMAC Finance that focuses on the development of disruptive ideas for the Ads business. We also provide tactical support where we are uniquely positioned to do so because of our expertise or independence.

Our optimal failure rate is not zero. We try building hard things and sometimes we fail spectacularly. However, when we success, we have deep impact. We welcome scientific and curious minds, that are interested in learning and trying out new idea, even if they are crazy.

We like to keep things light-weight in terms of documentation and processes, so we can focus on building.


Basic Qualifications
Bachelors or Masters Degree in Computer Science, Information Systems or related field
2+ years experience in Data Engineering or Business Intelligence roles working with ETL, Data Modeling, and Data Architecture
ETL design and SQL skills, knowledge of industry best practices, and an understanding how data is extracted, transformed, scrubbed, and loaded in a large Data Warehouse environment
Experience with Big Data technologies such as Pig/Hive/Spark
Proficiency in at least one programing or scripting language - Java, Scala, Python, Ruby, linux shell, or similar
Preferred Qualifications
Experienced with setting up end to end Data pipelines in an Enterprise environment
Proficient in performance optimizing Spark queries and jobs
Experiences with Amazon Web Services tools such as Redshift, EMR, SageMaker, Step Functions, Managed Airflow, or other similar platforms
Experience leveraging Python, R or Matlab to manipulate data and set up automated processes as per business requirements
Excellent communication skills with both technical and non technical users
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1978797"
448,Data Engineer,Robert Half,"Woodbridge, NJ","Description

You are a highly skilled Data Engineer looking to grow into a Lead role. You are an analytical thinker, problem solver, and innovator that is looking to work on the latest technologies with employees up and down the organization. You love taking on projects where you interact throughout a global organization and are not afraid to be challenged. You have strong customer data experience with high volume transactions. You are looking for amazing perks including excellent vacation time off, a hybrid role, in the office 2 days a week, and more.

What You Will Do
Join the global data team in building data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners.
Prepare, clean, format analytical datasets for processing and analysis.
Build and maintain custom ETL pipelines.
Become an expert in our datasets, their strengths and weaknesses, and write code to pull and verify data.
Conduct database feature engineering to support ongoing quantitative research.
Work with developers to create and deploy systems for anomaly detection.
Interface with data scientists, software developers, and other analytics operations staff as needed.
Serve as a point-of-contact for questions about data structures, definitions, and quality.
Work directly with Product and Systems Owners to deliver data products in a collaborative environment.
Design department-wide principles and workflow for data quality management.
You'll Need To Have
B.S. Computer Science/Engineering/Technology or Statistics/Mathematics required.
Credentialed certifications in Business Intelligence, Data Warehousing, and Analytics a plus.
5+ years of experience working in Data Engineering
5+ years of experience developing in SQL and leading ETL tools such as Talend, Informatica, Data Services
5+ years of experience with data profiling and data pipeline development
We’d Love To See
Experience in the retail industry
Experience working with large data sets
Comfortable participating in tool selection processes regarding data tools and software
Hands-on experience leading a team through the entire software development lifecycle of data management and business intelligence solutions
Knowledge of industry leading data architecture and data management practices
Excellent communication skills, including both oral and written
Excellent active listening and critical thinking skills
Ability to demonstrate professionalism, confidence, and sincerity
Ability to multi-task, exercise excellent time management, and meet multiple deadlines
Demonstrated excellence in project management and organization
High level of critical thinking and analytical skills
Ability to maintain confidentiality and appropriately share information on a need-to-know basis
Ability to consistently deliver excellent customer service
Excellent attention to detail and ability to document information accurately
Capable of resolving escalated issues arising from operations and requiring coordination with other departments
Requirements

SQL, ETL - Extract Transform Load, Data Pipelines, Python, Retail, Informatica

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
449,Data Engineer,Deloitte,"Sacramento, CA","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

The Data Engineer will join a team responsible for developing advanced analytics products; applying data visualization and statistical programming tools to enterprise data to advance and enable the key mission outcomes. In this role, they will support all phases of analytic work product development, from the identification of key business questions, through data collection and ETL, from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. This role requires special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's degree in STEM field
2+ years of experience with programming languages such as Python, R, SPSS, SAS, SQL
2+ years of experience with data visualization tools, such as Tableau, Qlik, PowerBI, or equivalent
2+ years of experience with ETL/ETL Pipeline, Data Warehouse Development and data modelling
Preferred:
Prior professional services or federal consulting experience
Experience with ETL, NoSQL Apache Hadoop, and cloud computing technology, especially Microsoft Azure
How You'll Grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
450,Data Engineer,Insight Global,"San Diego, CA","An employer is looking to add a Data Engineer to their team. This person will be responsible for expanding and optimizing data and pipeline architecture across various sources, including web, application, Oracle, Cassandra and social media. They will be generating analysis and reports, building dashboards for key stakeholders, and provide support for the ETL process. Through the entire process, they will be expected to build and maintain documentation surrounding datasets and pipelines. They will be working with many different stakeholders, including the product development/engineering teams, as well as database administrators and the business analytics team."
451,Senior Data Engineer - Data Operations,UrbanFootprint,"Berkeley, CA","Our Company

UrbanFootprint is the world’s first Urban Intelligence Platform. We provide critical intelligence to the institutions that are rebuilding the world's infrastructure. Where does the energy sector invest in electrification, decarbonization, and asset hardening in the face of climate threats? Where do cities and businesses invest to catch up with e-commerce, last-mile delivery, and new mobility? Where do governments deploy relief and new infrastructure to combat record hunger, homelessness, and hazard vulnerability? UrbanFootprint provides detailed and actionable answers to these questions and more.

UrbanFootprint is ‘Google Maps for the Modern Enterprise.’ We organize, normalize, and align thousands of urban, climate, and community metrics across the continental U.S.. The platform delivers targeted insights via dynamic data streams and collaborative web mapping applications. We enable our customers to answer complex questions in minutes versus weeks, months, or years. Our customers include some of the largest energy utilities, major financial institutions, critical government agencies, top urban planning firms, and fast-growing mobility companies.

We’re growing rapidly in a market with a TAM of $22B, and our competition is old-world manual consulting or outdated software tools that come without the data, the models, or the insights.

Our founders, Joe DiStefano and Peter Calthorpe, are urban planning pioneers who have spent decades providing critical urban intelligence to cities and enterprises across the globe. UrbanFootprint was named one of the World’s Most Innovative companies in 2021, and is on the GovTech 100 list. Our platform was awarded the top spot in FastCo’s Innovation by Design competition.

This is a fully remote position and can be based in any US location.

The role

As a socially-driven Senior Data Engineer focused on Data Operations you will be responsible for building UrbanFootprint’s Data Delivery and Configuration infrastructure - the internal tools, platform, and capabilities that allow us to quickly configure our proprietary data to best meet our customer needs. This means embracing how our customers interact with data - whether through direct data delivery, integration with their systems and tools, or through our GIS mapping and analytics applications. As our Senior Data Engineer, you will impact our customers through quick customization of our data and products. This allows customers to design environmentally conscious growth plans, target government relief dollars, and promote equitable community development.


You are an enthusiastic enabler; you love knowing that the tools and the systems you build empower others to excel. You recognize that sustained success isn’t about burning the midnight oil; it means automating repeatable tasks, designing tooling that reduces delivery friction, and creating discoverable documentation that helps others help themselves. As our Senior Data Engineer, you’re thoughtful and pragmatic; you understand the business needs of today and participate in design discussions for the future. You’re comfortable voicing your opinion and demonstrating technical leadership in an environment where business needs change. You are autonomous, not independent; you take input from stakeholders, actively source ideas from others and demonstrate deep ownership for everything from design to outcomes.

What you’ll do:

Collaborate with Engineering and our Head Architect to design and implement tools that allow our Solutions team to easily configure and deliver data products to customers
Building automated, reliable, high-performance data processing systems and pipelines
Own the end-to-end automation for extracting data from our internal data systems for delivery to our SaaS applications or a customer’s cloud bucket
Collaborate with data scientists to establish protocols that ensure the accuracy of summaries, filters, and spatial intersections

Your background most likely includes:

4+ years of industry experience in designing, building and maintaining production ETLs
High proficiency in at least one SQL dialect, Python, and at least one scalable data analytics framework such as Dask, PySpark, or Apache Beam
Experience with Airflow and/or managed Airflow offerings

Bonus qualifications:

Data Engineering certifications for GCP or AWS (such as GCP Professional Data Engineer or similar)
Experience with Kubernetes
Hands-on experience with geospatial and spatial-temporal data including both raster and vector data (such as US census boundaries)
You are socially driven to leverage data to facilitate a more equitable and resilient society


UrbanFootprint is committed to diversity in its workforce. We are committed to equal employment opportunity regardless of race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class."
452,"Data Engineer, Snowflake",Deloitte,"Charleston, WV","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
453,"Senior Data Engineer, Big Data Quality",Amazon,"Irvine, CA","Job Summary

DESCRIPTION

Exabytes of data in Amazon's data lake feed thousands of systems driving millions of decisions for Earth's Most Customer-centric company.

As the Sr Data Engineer, Big Data Quality supporting the QUality Assurance Framework program, you will by devising, designing, delivering and consulting on tooling and systems to ensure the data driving those decisions is best in class.

Key job responsibilities

You'll work with a cross-functional team of engineers, economists, and program & product managers to deliver exabyte-scale systems empowering teams across Amazon to deliver high quality data, balancing business needs versus technical constraints.

You'll coordinate with the TPM, Big Data Quality. The design and deliver will fundamentally define Amazon CDO's data quality landscape.

About The Team

The QUality Assurance Framework team comprises a TPM, an Economist, and a Data Engineer with a mission to make Amazon's centralized data lake

The team is embedded in Clickstream, Amazon's customer website behavioral data team, and works with a dedicated data quality engineering team in Bangalore as well as engineering teams in Seattle, WA and Irvine, CA. Your customers are the thousands of teams publishing data to the data lake, and the thousands more subscribing to that data.


Basic Qualifications
Bachelor’s degree in Computer Science, MIS, Mathematics, Statistics, Finance, related technical field, or equivalent work experience.
6+ years of years of relevant work experience in analytics, data engineering, business intelligence, market research or related field
Experience with at least one scripting language such as Python
Experience gathering business requirements, using industry standard business intelligence tool(s) to extract data, formulate metrics and build reports
Experience using SQL, ETL and databases in a business environment with large-scale, complex datasets
Experience with large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies
Experience architecting large scale BI solutions
Preferred Qualifications
An understanding that quality is defined by customers
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1901977"
454,"Data Engineer, GFP Analytics",Amazon,"Austin, TX","Job Summary

DESCRIPTION

The GFP Analytics Data & Infrastructure team consists of experienced engineers and manages a suite of core data services that ingests and processes all data related to Amazon’s rapidly growing delivery fleet of vans, trucks, electric vehicles, and more. The tech stack designed, built, and operated by the team uses an event-driven architectural paradigm enabled through creative use of AWS- and Amazon-internal data services.

As a data engineer on this team, you will have the exciting opportunity to work on one of the largest vehicle based data sets in the world. You will work closely with program managers, engineers, analysts and business teams to design solutions for their respective needs, which will require creativity and strong problem solving skills. You will also help in developing data engineering roadmaps that help build platforms to solve complex problems in efficient and scalable ways. This role will work backwards from customer problems to design solutions to meet the goals.

Additionally, you will be responsible for designing, developing, and operating a data service platform using Python, Apache Spark, and SQL to build the various ETL, analytics, and data quality components. You’ll automate deployments using AWS CodeDeploy, AWS CodePipeline, AWS Cloud Development Kit (CDK), and AWS Cloud Formation. You will design and implement complex data models and build the end-to-end infrastructure for reports and dashboards to be created by our customers. You will work with AWS services like Redshift, Glue, S3, IAM, CloudWatch, and more.

This role may be located out of Nashville, Seattle, or Austin.

Responsibilities Include
Work with external data partners to establish EDI connections to ingest various datasets
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using different AWS products such as Lambda, Glue, EMR, Kinesis Firehose
Maintain and enhance existing data pipelines
Create extensible designs and easy to maintain solutions with the long term vision in mind
Interface with cross functional teams, gathering requirements and delivering data solutions
Improve tools, processes, scale existing solutions, create new solutions as required based on team and stakeholder needs
About The Team

Here at Global Fleet and Products (GFP), we are building the safest, most efficient, and most sustainable fleet in the world. GFP Analytics is a growing and dynamic team dedicated to the curation, governance, and analytics/reporting of data about our worldwide fleet of delivery vehicles, especially data related to managing the supply chain and ongoing operation of the fleet. We have two sub-teams, Data/Infra and BI/Analytics, comprised of a diverse set of product/program managers, data engineers, business analysts, and business intelligence engineers. We love data and helping our customers drive the GFP mission!


Basic Qualifications
Degree in Computer Science, Engineering, Mathematics, or a related field or 4+ years industry experience.
3+ years of relevant work experience in Big Data engineering, ETL pipeline development, Data Modeling, and Data Architecture
3+ years of hands-on experience in writing complex, highly-optimized SQL queries across large data sets
Knowledge of software engineering best practices across the development life cycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Experience with coding languages like Python/Java/Scala
Exposure to large databases, BI applications, data quality and performance tuning
Excellent written and spoken communication skills
Preferred Qualifications
Proficiency in the DevOps style of software deployment (infrastructure-as-code)
Proficiency with AWS database/ETL tools including Lambda, Glue, Redshift, DynamoDB
Proficiency with AWS technologies including SNS, SQS, SES, Route 53, Cloudwatch, VPC
Background in Big Data, non-relational databases, Machine Learning and Data Mining is a plus
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1937221"
455,Data Engineer – Webscraping,Balyasny Asset Management L.P.,"Austin, Texas Metropolitan Area","The Data Intelligence Group (DIG) is a key part of BAM’s continued growth. Year over year, the knowledge needed to leverage data plays an increasingly important role in the firm’s core business. The analysis, services, software, and operational expertise that DIG provides are part of BAM’s competitive advantage.

Role Overview
We are looking for a creative and meticulous developer to join our Webscraping team. The data we provide drives investment decisions across the firm and we work hard to make sure it’s timely and accurate.

The optimal candidate will be strongly self-motivated with the ability to work and solve problems independently. In your role, you will:
Collaborate with analysts to understand and anticipate requirements
Design, implement, and maintain webscrapes for a wide variety of alternative datasets
Author tests to validate data availability and integrity
Maintain alerting systems to ensure smooth day-to-day operations
Investigate and defuse time-sensitive data incidents

Minimum Qualifications
Bachelors/Masters degree in Computer Science or a related field
1-3 years web development experience (Python/SQL/HTML/CSS/HTTP)
Linux experience (Windows experience a plus)
Excellent verbal and written communication skills

Preferred Qualifications
Aptitude for designing infrastructure, data products, and tools for Data Scientists
Familiarity with scraping and common scraping tools (Selenium, scrapy, Fiddler, Postman, xpath)
Experience containerizing workloads with Docker (Kubernetes a plus)
Experience with build automation (Jenkins, TeamCity)
Experience with AWS"
456,Data Engineer,SWAROVSKI,"Cranston, RI","At Swarovski, where innovation meets inspiration, our people desire to explore, experience, and create. We are looking for a [job title] where you will get a chance to work in a rewarding role within a diverse team that is pushing boundaries. Be part of a truly iconic global brand, learn and grow with us. We’re bold and inventive, revealing astonishing things like no one else can. A world of wonder awaits you.

About The Job


Manage end to end data pipelines to source and transform data to the Data & Analytics Platform(s).
Integrate and transform new data sources (internal and external) by creating a full pipeline from ingestion to ETL process.
Deliver Data Provisioning to operational applications requiring transformation of data from several sources.
Optimize and expand the Data & Analytics platform data flows and data models, supporting the Data Scientists or Business Intelligence Manager in deploying analytics products.
Assess the stability, robustness, and efficiency of the implemented ETL processes and data pipelines and eventually re-design them.

About You

We are looking for a unique and amazing talent, who brings along the following:


Bachelor or master’s in computer science, Applied Mathematics, Engineering, or any other technology related field
Minimum 3 years of professional experience in a similar role within an international setting
Excellent knowledge and proven professional experience with the SAP BI software suite, especially SAP BW and SAP BPC and / or big data tools (Hadoop, Spark, Kafka, …) as well as cloud services and ETL tools (SAP Data Intelligence, Alteryx, …)
Well structured, analytically thinking and demonstrating the ability to explain processes in a clear and understandable manner to a non-technical audience.
Excellent interpersonal communication and presentation skills

About Swarovski

Swarovski is a Wonderlab where magic and science meet.

Swarovski unifies all parts of its organization under one spellbinding idea and brings forward a wondrous new world of crystal craftsmanship. Founded in 1895 in Austria, the company designs, manufactures and sells the world’s highest quality crystal, genuine gemstones, Swarovski Created Diamonds and zirconia, jewelry, and accessories, as well as crystal objects and home accessories. Together with its sister companies Swarovski Optik (optical devices) and Tyrolit (abrasives), Swarovski Crystal Business forms the Swarovski Group.

A responsible relationship with people and the planet has always been an integral part of Swarovski’s heritage. This manifests today in the company’s well-established sustainability agenda with youth-focused education programs and foundations to promote human empowerment and conserve natural resources to achieve positive social impact.

Swarovski is committed to Equal Employment Opportunity for all employees and will take Affirmative Action in those appropriate employment situations. All employment decisions will be made without regard to race, color, religion, age, national or ancestral origin, gender, sexual orientation, marital status, citizenship status, veteran status and disability."
457,Data Engineer,Cincinnati Children's Hospital Medical Center,"Cincinnati, OH","The Data Engineer Focuses on how to design, integrate, and manage complex data and analytic systems over their life cycles. As a Data Engineer, you will use a combination of core software engineering principles and domain-specific data and analytic knowledge to ensure the enterprise has seamless access to actionable, meaningful, and well-governed data across all domains.

Representative Responsibilities
Data Pipelines
Design, Build, test and manage simple to moderately complex data pipelines from data sources or endpoints of acquisition to integration to consumption for production for key data and analytics consumers like business/data analysts, data scientists etc. Guarantee compliance with data governance and data security requirements while creating, improving and operationalizing data pipelines, partnering effectively with platform engineers and database administrators. Follow best practice development practices to ensure agile updates to data pipelines from development to production and back. Make simple to moderately complex changes to ETL processes and support upgrade and testing initiatives as necessary. Pursue additional options for data extraction and analysis from the Epic source system to deliver data to meet customer needs for research, regulatory, and collaborative initiatives. Understand bench-marking and process improvement data requirements and develop solutions to address these requests. Develop moderately complex solutions to ensure data analytic solutions don't interfere with transactional systems.
Metadata Management & Data Modeling Develop and implement simple to moderately complex data models to support CCHMC strategies. Work with key customer and report/analytic development groups to help ensure solutions are being developed with scalability and efficiencies in mind. Develop documentation of data models and extract processes, so information can be referenced by team members and customers to understand design objectives. Use innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. Continually refine existing solutions, so that best practices are deployed in individual reports, database structure, and extraction techniques. Work with vendors when necessary to ensure CCHMC investments and requests are being adequately supported and enhanced. Address reporting requests that require more complex solutions or require a deeper knowledge of the source system data models.
Technical & Business Skill Proficiency in one or more Data Management practices and architectures, such as Data Modelling, Data Warehousing, Data Lake, Data Hub, etc. and foundational understanding of the others. Proficiency with SQL, object-oriented/object function scripting and DevOps principles. Demonstrate understanding of core CCHMC clinical, business and research processes to help build appropriate data solutions. Obtain Epic certifications as appropriate/needed. Build additional skills through continuing education.
Technical Support & Customer Services
Ensure outstanding end-user support is provided, including ongoing monitoring of Service Level Agreements for incident management and collaboration with other areas to ensure customer-centered incident management and support. Adhere to and promote continual adoption of change management policies and procedures. Model outstanding customer service behavior, including timely and effective follow-up with customers. Escalate support issues with urgency. Collaborate with various stakeholders within the organization. In particular, work in close relationship with data science teams and with business (data) analysts in refining their data requirements for various data and analytics initiatives and their data consumption requirements. Provide second-level incident and problem resolution and support departmental efforts to improve customer satisfaction. Maintain and refine support documentation. Escalate support issues with urgency. Take 24 hour call on a staff rotation.
Project Execution & Management
Execute own project tasks with urgency and to a high level of quality. Communicate status clearly and effectively using departmental project management tools. Follow time-tracking and other project management requirements. Participate actively in project meetings, stand-ups, etc. Serve as technical lead for moderately complex projects, collaborating with project managers to ensure project scope/risk/budget/etc. are adequately managed. Lead project meetings and workgroups.

Required
Bachelor's degree in a computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field.
2 years of work experience in a related job discipline.
Preferred
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Job

Information Technology

Primary Location

United States-Ohio-Cincinnati-Vernon Place

Schedule

Full-time

Shift

Day Job

Job Type

Standard

Department

200100 - Information Services

Employee Status

Regular

FTE

1.0

Weekly Hours

40

Salary Range

39.90"
458,AWS Data Engineer,Deloitte,"Boca Raton, FL","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
459,"Data Engineer, Prime Global Finance",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

We need a Data Engineer! Prime Global Finance enjoys a unique vantage point into everything happening within Amazon. This role will be part of a team that is responsible for managing the data architecture and generating data insights across two of the most impactful and high visibility areas within Amazon: Prime and customer engagement.

The Data Engineer will partner with teams across finance, marketing, research etc. to design and develop the organization's data architecture and processes to enable Business Intelligence Engineers and analysts drive analysis and insights of the business for senior leadership across the entire Prime and Lifecycle Engagement organizations. You will push traditionally accepted process boundaries and work to automate scalable solutions leveraging AWS and BI tools including but not limited to Redshift, Lambda, EMR, etc.

The data flowing through our platform directly contributes to decision-making by WW Prime and Lifecycle Engagement leaders at all levels of organizational leadership. If you’re passionate about building tools that enhance productivity, drive decision making, and operate at a scale that is unique even within Amazon, you're looking at the right posting!

Come innovate with Prime Finance tech team!

Responsibilities Of This Position Include
Working with stakeholders and other engineering teams to identify and scope the right data architecture and technology to be used to facilitate analysis of most common Amazon customer behavioral questions.
Partnering with engineering teams to enhance data infrastructure, data availability, and broad access to customer insights made available through BI tools across the organization.
Design, build and implement the right ETL processes using AWS and similar technologies.
Implement anomaly detection systems to have a proactive approach to any potential data quality issues, using industry standard frameworks.
Enable large scale analytics using EMR and other big data technologies.
Establishing and implementing technology best practices that should be followed across the organization.
Work on proof of concepts for adoption of new technology and tools.

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
3+ years of experience working with big data technologies (Redshift, S3, EMR, EC2)
3+ years of experience with Python and *nix systems.
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative fields.
5+ years of experience as a Data Engineer, BI Engineer or related field in a company with large, complex data sources.
AWS experience a big plus.
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy.
Familiarity with data quality automation.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1979320"
460,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, TalentPartners, is seeking the following. Apply via Dice today!

Sr. Data Engineer - REMOTE The role of the Senior Data Engineer is responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting. This engineerrsquos duty is to monitor the existing metrics, analyze data, and lead partnerships with other Data and Analytics teams in an effort to identify and implement systems and process improvements. This engineer also designs, architects, implements, and supports key datasets. Primary Duties and Key Responsibilities Design and develop highly scalable and reliable data engineering pipelines to process large volumes of data across many data sources in the cloud Identify, design and implement internal process improvements by automating manual processes and optimizing data delivery Develop and promote best practices in data engineering Develop real-time data processing applications using Google Cloud Be part of the on-call rotation supporting our SLArsquos Participate in design and code reviews Required Qualifications Bachelor's degree in Computer Science or equivalent experience in a related field 5+ years of hands-on experience working in data warehousing or data engineering environment Strong Python and Advanced SQL programming skills 3+ years experience developing data solutions on Google Cloud Platform or AWS Experience in ingestion of data from external APIs and data stores Experience in design, build and operationalization of big data pipelines on Cloud Technologies. Can-do attitude on problem-solving, quality and ability to execute Strong experience in authoring, scheduling, and monitoring of workflows (Apache Airflow related technologies) Strong communication interpersonal skills Preferred Qualifications Google Cloud Certified - Professional Data Engineer certification would be a plus Knowledge of Git, Jinja2, Docker, Bitbucket, and Bamboo Familiar with a NoSQL database such as MongoDB Familiar with version control systems (Git and Bitbucket) Familiar with Atlassian products Jira and Confluence Hands-on experience with Apache Airflow or Google Composer"
461,"Data Engineer, Snowflake",Deloitte,"Concord, NH","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm

Work You'll Do/Responsibilities

As a Snowflake Data Engineer at Deloitte Consulting, you will be working on a team providing design, implementation of new cloud-based products while supporting existing products in Oracle and working with the technical team to troubleshoot issues. You will be responsible for: understanding and documenting the business requirements/technical specifications, performing technical development and/or configuration, designing test cases, creating test data and performing unit testing, communicating with the client, and supporting the overall delivery including status reporting, dependency management and risk/issue identification and mitigation.
Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.)
Debug production issues across services and multiple levels of the stack
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products
Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
7+ years in a Data Engineering or Data Warehousing role
7+ years coding experience (java or python preferred)
4+ years hands on experience with big data technologies (Snowflake, Redshift, Hive, or similar technologies)
Master's Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field
Expert level understanding of ETL fundamentals and building efficient data pipelines
Travel up to 10% annually
Limited immigration sponsorship may be available"
462,Data Engineer,Deloitte,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

A Data Engineer will be responsible for leading the setup of an AWS hosted data lake as well as the ingestion pipeline and processing for 100+ datasets, working closely with Agile software development team(s). This role includes responsibilities such as creating and managing schedules for data management (migration, integration, etc.) efforts, working with clients to validate migrated data, working with Agile development teams to understand changes and their impacts towards data migration efforts, among other tasks.

The Team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:
Bachelor's Degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
Must be able to obtain and maintain the required security clearance for this position
Travel up to 50%
2+ years of data engineering, data management, and/or data warehousing experience
2+ years of of experience with data tools such as Informatica and Talend
2+ years of experience with Extract, Transform, and Load (ETL)
Preferred:
Professional Amazon Cloud Architecture certification
Ability to thrive in a fast-paced work environment with multiple stakeholders
Knowledge of data mining, machine learning, data visualization and statistical modeling
Prior professional services or federal consulting experience"
463,Data Engineer - TM1/Planning Analytics,Amazon,"Austin, TX","Job Summary

DESCRIPTION

How often have you had an opportunity to be an early member of a team that is tasked with solving a huge customer need through disruptive and innovative technology?

The Team: WW Operations Finance is poised for a revolution. Historically, automation and technical infusion has taken a back-seat to important financial functions such as controllership, financial oversight, risk mitigation, and strong investment analysis. Finance is set to be the Chief Story Tellers, and along the way inject machine learning, BI, advanced analytics and technical tools – built to suit finance and business needs.

The Financial Systems and Analytics Team (FSAT) is seeking a Data Engineer to help lead a flagship multi-year project to inject automated planning and reporting, predictive forecasting, and help shape an end UI, built on the AWS Data Lake. We are the tech team that supports WW Amazon Operations – one of the fastest growing, largest and most complex Supply Chains in the world. This position requires a high level of technical expertise with data engineering concepts.

Responsibilities
Deliver on data architecture projects and implementation of next generation financial solutions.
Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda etc.
Build and deliver high quality data architecture and pipelines to support customer reporting needs.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.
Collaborate with business users, development teams, and operation engineering teams to tackle business requirements and deliver against high operational standards of system availability and reliability
Dive deep to resolve problems at their root, looking for failure patterns and suggesting fixes
Prepare runbooks, methods of procedures, tutorials, training videos on best practices for the team
Build monitoring dashboards and creation of critical alarms for the system
Build and enhance software to extend system, application, or tool functionality to improve business processes and meet end user needs while working within the overall system architecture
Diagnose and resolve operational issues, perform detailed root cause analysis, respond to suggestions for enhancements
Identify process improvement opportunities to drive innovation
Rotational on-call availability for critical systems support
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
4+ years of relevant IBM Cognos TM1/Planning Analytics experience including system configuration, model building and developing reports or dashboards with TM1
Relevant corporate finance experience exhibiting knowledge of financial planning, budgeting and forecasting functions and related processes
Bachelor's degree in Finance, Accounting, Engineering, Mathematics or other technical discipline
Preferred Qualifications
Experience in design and optimization of data-processing systems using AWS services (S3, EMR, Lambda, Glue, Athena, SNS, Cloud Watch, Redshift, Aurora/RDS)
Experience with Java and experience with scripting languages like Perl, Python, Unix shell scripts, VBA and MS Excel
Working experience with TM1Py, REST API
Relevant corporate finance experience exhibiting knowledge of financial planning, budgeting and forecasting functions and related processes.
Familiarity with other IBM software products
Experience with design & delivery of formal training curriculum and programs
Proficiency with Microsoft Project, Visio, and SharePoint
Project management, scoping, reporting, and scheduling experience a plus
Bachelor's and/or Master's degree in Finance, Accounting, Engineering, Computer Science, Statistics, Mathematics or other technical discipline
Excellent written and verbal communication skills with a customer focused, professional demeanor
Able to work on a diverse team
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com.ca, Inc. (US)

Job ID: A1850407"
464,GCP Data Engineer,Accenture,"New York, NY","We are:

We are a leading partner to the world’s major cloud providers. The formation of Accenture Cloud First, with a $3 billion investment over three years, demonstrates our commitment to deliver greater value to our clients when they need it most. Our Cloud First multi-service group of more than 70,000 cloud professionals delivers a full stack of integrated cloud capabilities like data, edge, integrated infrastructure and applications, deep ecosystem skills, culture of change along with pre-configured industry solutions to shape, move, build and operate our clients’ businesses in the cloud. We combine world-class learning and talent development expertise; deep experience in cloud change management; and cloud-ready operating models with a commitment to responsible business by design — with security, data privacy, responsible use of artificial intelligence, sustainability and ethics and compliance built into the fundamental changes Accenture helps companies achieve.

You are:

As part of our Data & AI group, you will lead technology innovation for our clients through robust delivery of world-class solutions. There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape.

The work:

You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Cloud Data Engineer, Data Modeler or Data Architect covering all aspects of Data including Data Management, Data Governance and Data Migration. Come grow your career in Technology at Accenture!

What you need:

Minimum of 3 years direct experience in Enterprise Data Warehouse technologies

Minimum of 2 years hands on GCP Cloud data implementation projects experience (Dataflow, Cloud Composer, Big Query, Cloud Storage etc.)

Bachelor's degree or equivalent (minimum 12 years) work experience. (If associate degree, must have minimum 6 years work experience)

Bonus points if you have:

Certifications: Google Professional Data Engineer

Experience with developing so􀇅ware code in one or more languages such as Java, Python and SQL.

Experience with architecting, implementing and/or maintaining technical solutions

Minimum of 3 years in a customer facing role working with enterprise clients

Experience designing and deploying large scale distributed data processing systems with few technologies such as Oracle, MS SQL Server, MySQL, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, HBase, Ve􀇈ica, Netezza, Teradata, Tableau, Qlik or MicroStrategy.

Demonstrated excellent communication, presentation, and problem solving skillsData migration and data processing experience on the Google Cloud stack, specifically:

BigQuery

Cloud DataFlow

Cloud DataProc

Cloud Storage

Cloud DataPrep

Cloud PubSub

Cloud Composer

Cloud BigTable

As required by the Colorado Equal Pay Transparency Act, Accenture provides a reasonable range of compensation for roles that may be hired in Colorado. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location. For the state of Colorado only, the range of starting pay for this role is $90,288- $148,900 and information on benefits offered is here.

COVID-19 update:

The safety and well-being of our candidates, our people and their families continues to be a top priority. Until travel restrictions change, interviews will continue to be conducted virtually.

Subject to applicable law, please be aware that Accenture requires all employees to be fully vaccinated as a condition of employment. Accenture will consider requests for accommodation to this vaccination requirement during the recruiting process.

What We Believe

We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.

Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here

Equal Employment Opportunity Statement

Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Accenture is committed to providing veteran employment opportunities to our service men and women.

For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement.

Requesting An Accommodation

Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.

If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.

Other Employment Statements

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information."
465,"ML Data Engineer, Minerva Science & Analytics (Books)",Amazon,"San Diego, CA","Job Summary

DESCRIPTION

Are you passionate about working with Amazon-scale data, analytics, and data science? Do you love bringing data together from diverse systems and sources and working on innovative data engineering solutions for supporting science, machine learning, and converting deep questions into actionable insights? Minerva Science and Analytics team is looking for an experience, self-driven engineer to help us mobilize data across a large number of businesses to help independent authors bring their creativity to customers, detect fraudulent and abusive behavior, and democratize content creation in an safe, efficient, and exciting way. Our team has mature areas and green-field opportunities. We offer technical autonomy, value end-to-end ownership, and have a strong customer-focused culture.

Key job responsibilities

As an ML Data Engineer at Amazon, you will connect with world leaders in your field working on similar problems. You will be building and working with large distributed systems of data and providing technical leadership to scientists, product managers, engineers using these data. You will utilize your deep expertise in creation and management of datasets to translate the data into meaningful insights through collaboration with Business Intelligence Engineers (BIEs) and Business Analysts (BAs). You will have ownership of end-to-end development of data engineering solutions and you’ll play an integral role in strategic decision-making.

About The Team

Minerva is a cross-functional team of highly experienced scientists, BI engineers, and software engineers with a critical business mission making revolutionary leaps forward using massive-scale data with advanced analytics and machine learning and helping democratize the publishing industry. We build science-based systems for marketing and content-discovery for indie authors, fraud/abuse, and content risk. We also use science to optimize manufacturing, fulfillment, and quality processes for our Print On Demand (POD) business.


Basic Qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering)
2+ years of relevant experience in one of the following areas: Data engineering, database engineering, business intelligence or business analytics.
2+ years of experience with columnar databases and relational databases (SQL Server or Oracle as examples)
2+ years of experience designing and implementing relational architecture and database administration
2+ years of experience managing ETL pipelines with big data using relational and non-relational data
2+ years of experience in software development of backend APIs and automations with Python, Javascript, Java, or C#
1+ years of Git version control
Experience working using agile methodologies
Preferred Qualifications
Masters in computer science, mathematics, statistics, economics, or other quantitative fields.
Experience developing, troubleshooting and maintaining cloud-based infrastructure (networking, server provisioning, storage, and real time data streams)
2+ years of experience with AWS Spectrum
2+ years of experience with AWS Lambda, EventBridge events and application development with Lambda
Experience with Tableau Desktop and Server
Experience creating and maintaining Cloud Watch dashboards and metrics
Experience in front end web frameworks such as ReactJS
Experience with Salesforce object management and extraction
Experience on AWS Cloud Formation, Code Pipelines, Code Build and Elastic Beanstalk
Customer service operations experience and knowledge background
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1882746"
466,Spark Data Engineer,Perficient,"Atlanta, GA","Overview

At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Spark Developer proficient in Scala and SQL We are looking for someone to be based in the US, but are flexible on specific location. Candidate will be expected to work US East Coast hours with occasional flexibility needed to work with client team based in India.

Job Overview

One of our large clients is expanding their current data footprint on the cloud to provide analytics, BI and data APIs. Majority of data will be batch processed with data validation, data quality and transformation into a multitude of data platforms such as Redshift, Postgres and Hive.

A Senior Technical Consultant is expected to be knowledgeable in two or more technologies within (a given Solutions/Practice area). The Senior Technical Consultant is expected to have strong development and programming skills in Spark with a focus on Scala/Java and other ETL development experience in the big data space. You are expected to be experienced and fluent in agile development and agile tools as well as code repositories and agile SLDC/DevOps frameworks.

You will work with architects and infrastructure teams to develop, test, deploy and troubleshoot your code as well as provide input into solutions and design of the system. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions focused on our clients' business needs.

Responsibilities
Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.
Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components
Document code artifacts and participate in developing user documentation and run books
Troubleshoot deployment to various environments and provide test support.
Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Qualifications
At least 3+ years of experience in developing large scale data processing/data storage/data distribution systems
At least 3+ years of experience on working with large Hadoop projects using Spark and Scala and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures.
Experience with ELT/ETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred.
Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics
Experience with SQL including Postgres, MySQL RDBMS platforms
Experience with Linux (RHEL or Centos preferred) environments
Experience with various IDE and code repositories as well as unit testing frameworks.
Experience with code build tools such as Maven.
Fundamental knowledge of distributed data processing systems and storage mechanisms.
Ability to produce high quality work products under pressure and within deadlines with specific references
Strong communication and collaborative skills
At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project
At least 5+ years of working with a complex Big Data environment
5+ years of experience with JIRA/GitHub/Git and other code management toolsets
Preferred Skills And Education

Bachelors’s degree in Computer Science or related field

Certification in Spark, AWS or other cloud platform

Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

Select work authorization questions to ask when applicants apply
Are you legally authorized to work in the United States?
Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"
467,Data Engineer II,Amazon,"Boston, MA","Description

Do you want to build the analytics to understand and accelerate large scale migrations to AWS? Migrating to AWS is one of the most impactful business decisions AWS customers make and we want your help to better understand our customers' migration journeys. For customers large and small, migrating to AWS can have an enormously positive impact on their costs, agility, and employee growth. Here in the Migration Services team, we’re working closely with customers to invent new approaches to migrations, build scalable systems, and use machine learning to solve problems that haven’t been solved yet.

We are looking for a self-driven Data Engineer proficient with SQL & ETL pipelines who is familiar with AWS technology and has scripting experience. In this role, you will help a team of data engineers build fault-tolerant systems with scalability and performance in mind. You will get the exciting opportunity to interact with very large data sets in one of the most complex data warehouse environments. Our data pipeline employs serverless AWS components (SNS, SQS & Lambda) to combine metrics from multiple data sources including Amazon Redshift, Salesforce, and Amazon S3. You will have the opportunity to help business and engineering stakeholders determine which migration related metrics they should be tracking and extend our data engineering infrastructure.

Day-to-day You Will
Partner with product management, sales, and business stakeholders to analyze data from disparate data sources and determine how we can accelerate their migrations.
Design, implement, and maintain a data pipeline and analytical environment using third-party and in-house reporting tools, modeling metadata, and building reports and dashboards.
Use creative problem-solving to automate the collection and analysis from available data sources in order to deliver actionable output.
Iteratively improve analysis and identify new metrics to improve analytics.
Work/Life Balance

Our team puts a high value on work-life balance. Most days, our entire team is co-located in the Boston office, but we’re also flexible when people occasionally need to work from home. We generally keep core in-office hours from 10am to 4pm. About half of us come in earlier and the other half of us stay later.

Mentorship & Career Growth

Our team is dedicated to supporting new team members. Our team has a broad mix of experience levels and Amazon tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our senior engineers truly enjoy mentoring more junior engineers and engineers from non-traditional backgrounds through one-on-one mentoring and thorough, but kind, code reviews.

We care about your career growth. We try to assign projects and tasks based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.


Basic Qualifications
2-5 years of work experience with ETL, Data Modeling, and Data Architecture.
2-5 years of writing and optimizing SQL.
1-3 years of programming experience in languages like Python, Ruby or Java.
Demonstrable ability in data modeling, ETL development, and data warehousing, or similar skills
Experience with reporting tools like QuickSight, Tableau, Excel or other BI packages
Experience in working and delivering end-to-end projects independently.
B.S. degree in mathematics, statistics, computer science or a similar quantitative field

Preferred Qualifications

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records.

Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records


Company - Amazon Dev Center U.S., Inc.

Job ID: A1615828"
468,"W2 Position :: Data Engineer at Lansing, MI || Remote",Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Bitsoft International, Inc., is seeking the following. Apply via Dice today!

Data Engineer 1 Year Contract Lansing, MI Remote Responsibilities Previous experience as a data engineer or in a similar role Technical expertise with data models, data mining, and segmentation techniques Experience with big data tools Hadoop, Spark, Kafka, etc. Experience with the Big Data technologies (Hadoop, MR, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc) Experience with ETL(Extraction, Transformation Loading) and architecting data systems Experience with stream-processing systems Storm, Spark-Streaming, etc."
469,Data Engineer,Deloitte,"Indianapolis, IN","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
7+ years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
470,Data Engineer,Deloitte,"Miami, FL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
7+ years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
471,Data Engineer,Deloitte,"Miami, FL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
4 to 6 years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
472,"Data Engineer, PXT Central Science",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The People eXperience and Technology (PXT) Central Science Team uses economics, behavioral science, statistics, and machine learning to proactively identify mechanisms and process improvements which simultaneously improve Amazon and the lives, wellbeing, and the value of work to Amazonians. We are an interdisciplinary team which combines the talents of science and engineering to develop and deliver solutions that measurably achieve this goal. We invest in innovation and rapid prototyping of scientific models and software solutions to accelerate informed, accurate, and reliable decision backed by science and data.

We are looking for an experienced data engineer engineer to integrate data sources and create a data infrastructure to model and research all aspects of HR, including: workforce planning, recruiting, employee engagement, retention, development, employee benefits, and compensation.

The ideal candidate is expected to work closely with senior scientists and business leaders to study the impact Amazon has had on workers, the market, and the economy. You will build new data engineering solutions end-to-end, will work with multiple stakeholders across HR and Operations, and will build data pipelines/automation that can be used by distributed systems to run statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Coding proficiency in at least one modern programming language (Python, Ruby, Java, Scala etc).
Experience with Big Data technologies such as Hive, Spark, Hadoop, NoSQL, AWS EMR, Glue, Lambda, Kinesis, Redshift.
Excellent grasp of software development life cycle and/or agile development environment.
Preferred Qualifications
5+ years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience in iterative design, working closely with business, product and tech partners from inception through implementation.
Detailed knowledge of data warehouses, architecture, infrastructure components, ETL and reporting tools and environments
Familiarity with AWS services such as S3, Redshift, EMR, Athena etc.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Experience providing technical direction and mentorship of engineers and scientists on best practices in the data engineering space
Be self-motivated and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1930604"
473,"Data Engineer, PXT Central Science",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The People eXperience and Technology (PXT) Central Science Team uses economics, behavioral science, statistics, and machine learning to proactively identify mechanisms and process improvements which simultaneously improve Amazon and the lives, wellbeing, and the value of work to Amazonians. We are an interdisciplinary team which combines the talents of science and engineering to develop and deliver solutions that measurably achieve this goal. We invest in innovation and rapid prototyping of scientific models and software solutions to accelerate informed, accurate, and reliable decision backed by science and data.

We are looking for an experienced data engineer engineer to integrate data sources and create a data infrastructure to model and research all aspects of HR, including: workforce planning, recruiting, employee engagement, retention, development, employee benefits, and compensation.

The ideal candidate is expected to work closely with senior scientists and business leaders to study the impact Amazon has had on workers, the market, and the economy. You will build new data engineering solutions end-to-end, will work with multiple stakeholders across HR and Operations, and will build data pipelines/automation that can be used by distributed systems to run statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Coding proficiency in at least one modern programming language (Python, Ruby, Java, Scala etc).
Experience with Big Data technologies such as Hive, Spark, Hadoop, NoSQL, AWS EMR, Glue, Lambda, Kinesis, Redshift.
Excellent grasp of software development life cycle and/or agile development environment.
Preferred Qualifications
5+ years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience in iterative design, working closely with business, product and tech partners from inception through implementation.
Detailed knowledge of data warehouses, architecture, infrastructure components, ETL and reporting tools and environments
Familiarity with AWS services such as S3, Redshift, EMR, Athena etc.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Experience providing technical direction and mentorship of engineers and scientists on best practices in the data engineering space
Be self-motivated and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1930603"
474,"Data Engineer, PXT Central Science",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The People eXperience and Technology (PXT) Central Science Team uses economics, behavioral science, statistics, and machine learning to proactively identify mechanisms and process improvements which simultaneously improve Amazon and the lives, wellbeing, and the value of work to Amazonians. We are an interdisciplinary team which combines the talents of science and engineering to develop and deliver solutions that measurably achieve this goal. We invest in innovation and rapid prototyping of scientific models and software solutions to accelerate informed, accurate, and reliable decision backed by science and data.

We are looking for an experienced data engineer engineer to integrate data sources and create a data infrastructure to model and research all aspects of HR, including: workforce planning, recruiting, employee engagement, retention, development, employee benefits, and compensation.

The ideal candidate is expected to work closely with senior scientists and business leaders to study the impact Amazon has had on workers, the market, and the economy. You will build new data engineering solutions end-to-end, will work with multiple stakeholders across HR and Operations, and will build data pipelines/automation that can be used by distributed systems to run statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Coding proficiency in at least one modern programming language (Python, Ruby, Java, Scala etc).
Experience with Big Data technologies such as Hive, Spark, Hadoop, NoSQL, AWS EMR, Glue, Lambda, Kinesis, Redshift.
Excellent grasp of software development life cycle and/or agile development environment.
Preferred Qualifications
5+ years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience in iterative design, working closely with business, product and tech partners from inception through implementation.
Detailed knowledge of data warehouses, architecture, infrastructure components, ETL and reporting tools and environments
Familiarity with AWS services such as S3, Redshift, EMR, Athena etc.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Experience providing technical direction and mentorship of engineers and scientists on best practices in the data engineering space
Be self-motivated and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1930068"
475,Spark Data Engineer,Perficient,"Philadelphia, PA","Overview

At Perficient you’ll deliver mission-critical technology and business solutions to Fortune 500 companies and some of the most recognized brands on the planet. And you’ll do it with cutting-edge technologies, thanks to our close partnerships with the world’s biggest vendors. Our network of offices across North America, as well as locations in India and China, will give you the opportunity to spread your wings, too.

We’re proud to be publicly recognized as a “Top Workplace” year after year. This is due, in no small part, to our entrepreneurial attitude and collaborative spirit that sets us apart and keeps our colleagues impassioned, driven, and fulfilled.

Perficient currently has a career opportunity for a Spark Developer proficient in Scala and SQL We are looking for someone to be based in the US, but are flexible on specific location. Candidate will be expected to work US East Coast hours with occasional flexibility needed to work with client team based in India.

Job Overview

One of our large clients is expanding their current data footprint on the cloud to provide analytics, BI and data APIs. Majority of data will be batch processed with data validation, data quality and transformation into a multitude of data platforms such as Redshift, Postgres and Hive.

A Senior Technical Consultant is expected to be knowledgeable in two or more technologies within (a given Solutions/Practice area). The Senior Technical Consultant is expected to have strong development and programming skills in Spark with a focus on Scala/Java and other ETL development experience in the big data space. You are expected to be experienced and fluent in agile development and agile tools as well as code repositories and agile SLDC/DevOps frameworks.

You will work with architects and infrastructure teams to develop, test, deploy and troubleshoot your code as well as provide input into solutions and design of the system. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions focused on our clients' business needs.

Responsibilities
Work with data engineering team to define and develop data ingestion, validation, transformation and data engineering code.
Develop open source platform components using Spark, Scala, Java, Oozie, Hive and other components
Document code artifacts and participate in developing user documentation and run books
Troubleshoot deployment to various environments and provide test support.
Participate in design sessions, demos and prototype sessions, testing and training workshops with business users and other IT associates
Qualifications
At least 3+ years of experience in developing large scale data processing/data storage/data distribution systems
At least 3+ years of experience on working with large Hadoop projects using Spark and Scala and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures.
Experience with ELT/ETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred.
Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics
Experience with SQL including Postgres, MySQL RDBMS platforms
Experience with Linux (RHEL or Centos preferred) environments
Experience with various IDE and code repositories as well as unit testing frameworks.
Experience with code build tools such as Maven.
Fundamental knowledge of distributed data processing systems and storage mechanisms.
Ability to produce high quality work products under pressure and within deadlines with specific references
Strong communication and collaborative skills
At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project
At least 5+ years of working with a complex Big Data environment
5+ years of experience with JIRA/GitHub/Git and other code management toolsets
Preferred Skills And Education

Bachelors’s degree in Computer Science or related field

Certification in Spark, AWS or other cloud platform

Perficient full-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs including billable bonus opportunities. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makes Perficient a great place to work.

More About Perficient

Perficient is the leading digital transformation consulting firm serving Global 2000 and enterprise customers throughout North America. With unparalleled information technology, management consulting and creative capabilities, Perficient and its Perficient Digital agency deliver vision, execution and value with outstanding digital experience, business optimization and industry solutions.

Our work enables clients to improve productivity and competitiveness; grow and strengthen relationships with customers, suppliers and partners; and reduce costs. Perficient's professionals serve clients from a network of offices across North America and offshore locations in India and China. Traded on the Nasdaq Global Select Market, Perficient is a member of the Russell 2000 index and the S&P SmallCap 600 index.

Perficient is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

Select work authorization questions to ask when applicants apply
Are you legally authorized to work in the United States?
Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?"
476,"Data Engineer, Python & SQL",CVS Health,"Atlanta, GA","Job Description

What We Do

===========

CVS Health seeks to change the way individuals manage their healthcare by focusing on the features that matter to our customers, building software in a modern, lean way, and deploying continuously into the cloud.

Our server teams are focused on building out our provider search and prescription fulfillment APIs. These services are used by multiple applications to help millions of our members effectively find the care and resources they need.

What You'll Do

============
You will immediately be immersed into a highly organized Agile delivery system working tightly within a diverse team of about six people representing Data Engineering, QA, and Product.
You will meet every day with your team to discuss progress and challenges pairing multiple times per week with your teammates as well as colleagues on other teams to teach and learn.
You will contribute to design and planning discussions on a rigorous two week cadence
You will meet every quarter with your team to actively participate in planning sessions to help ensure both smooth technical delivery and good work/life balance.
You will look for ways to improve our service reliability and performance while our customer base continues to grow.
You will be working with our product partners to deliver new features that improve user experience.
You will be refactoring existing data pipelines to decrease time to market and improve developer satisfaction.

Responsibilities

=============
Working collaboratively with other engineers, data scientists, and business product owners in a geographically dispersed agile environment
Developing robust, reusable, and scalable data-driven solutions to automate the ingestion, processing, and transformation of enterprise data into the Cloud
Partnering with architecture to build and support our Cloud data infrastructure and data pipelines
Assisting in the selection and integration of data-related tools and frameworks to expand our data platform capabilities
Monitoring, troubleshooting, and supporting data execution in a Production environment
Identifying and fixing problems in code
Producing extensible and thoroughly-tested data pipelines
Seeking opportunities to improve the foundation of our data platform and frameworks
Learning new software development skills, engineering best practices, and cloud engineering methods

Required Qualifications

3+ years Data Engineering, Data Analysis, and/or Data Integration
3+ years Python Software Development
Expert SQL for data profiling, analysis, and extraction
Ability to work within a team environment
Problem solving and technical communication skills
Experience translating feature requests into working software
An open mind about different solutions and a willingness to discuss alternatives
Willingness to work within a cohesive team environment that emphasizes a culture of teaching and learning

COVID Requirements

COVID-19 Vaccination Requirement

CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.

You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.

Preferred Qualifications

SQL implementations (PostgreSQL, SQL Server, Oracle, MySQL)
NoSQL implementations (Elasticsearch, DynamoDB, MongoDB)
Experience working with structured and unstructured data
Continuous Integration / Continuous Delivery within a data delivery environment
Linux operations and shell scripting
Building and administering Big Data and Real-time Streaming Architectures in a Cloud environment (AWS, Google, Azure)
Software development in two or more of the following: Python, JavaScript, Java, Node.js
Education
Bachelor's degree in computer science or technical discipline, or an equivalent combination of formal education and experience

Business Overview

At CVS Health, we are joined in a common purpose: helping people on their path to better health. We are working to transform health care through innovations that make quality care more accessible, easier to use, less expensive and patient-focused. Working together and organizing around the individual, we are pioneering a new approach to total health that puts people at the heart.

We strive to promote and sustain a culture of diversity, inclusion and belonging every day. CVS Health is an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring or promotion based on race, ethnicity, sex/gender, sexual orientation, gender identity or expression, age, disability or protected veteran status or on any other basis or characteristic prohibited by applicable federal, state, or local law. We proudly support and encourage people with military experience (active, veterans, reservists and National Guard) as well as military spouses to apply for CVS Health job opportunities."
477,Senior Data Engineer(Big Data),U.S. Bank,"Atlanta, GA","At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.

Job Description

Be a part of transformational change where integrity matters, success inspires and great teams collaborate and innovate. As the fifth-largest bank in the United States, we're one of the country's most respected, innovative, ethical and successful financial institutions. We're looking for people who want more than just a job - they want to make a difference! U.S. Bank is seeking a Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.

This position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.

Essential Responsibilities
Responsible for designing, developing, testing, operating and maintaining products.
Takes full stack ownership by consistently writing production-ready and testable code.
Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs.
Makes sound design/coding decisions keeping customer experience in the forefront.
Takes feedback from code review and apply changes to meet standards. Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures.
Accountable for ensuring all aspects of product development follow compliance and security best practices.
Exhibits relentless focus in software reliability engineering standards embedded into development standards.
Embraces emerging technology opportunities and contributes to the best practices in support of the bank's technology transformation.
Contributes to a culture of innovation, collaboration and continuous improvement.
Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery. Anticipates and communicates blockers and delays before they require escalation.
Primary Responsibilities
Develop and enhance solutions for Enterprise Landing Zone(ELZ).
Apply Dev/Ops mindset, take ownership of production success, optimize operation success via automation/active alerting/self-healing, and lead the resolution of production issues to ensure high-availability and performance.
Drive the release planning and execution with end-to-end understanding and insights (effort, risk, priority) of the planned features.
Develop high-quality code, define best engineering practice, perform peer code reviews to ensure successful deliverable with engineering excellence.
Document engineering artifacts such as technical design document, flowcharts, system context diagrams, code comments etc.
Collaborate cross-functionally with product owner, data scientists, business users, project managers and other engineers to achieve elegant solutions.
Collaborate with other engineers on the team to elevate technology and consistently apply best
Basic Qualifications
Bachelor's degree, or equivalent work experience
Five to six years of relevant experience
Preferred Skills / Experience
Knowledge of Spark, Scala and other programming languages
Experience in airflow, elastic, google cloud, data quality
Experience in managing big data sets
Experience in Java and object-oriented design skills
Experience building micro-services based applications
Experience in Utilizing tools such as Maven, Docker, Kubernetes, Kibana, ELK, Jenkins and frameworks such as Spring Boot to develop and deploy microservices
Experience using CICD processes for application software integration and deployment using Maven, Git, Jenkins.
Experience building scalable and resilient applications in private or public cloud environments and cloud technologies
Experience with server side languages such as Python, Java, GoLang
Excellent verbal and written communication skills
Considerable technical, logical, analytical and problem-solving skills
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.

Benefits

Take care of yourself and your family with U.S. Bank employee benefits. We know that healthy employees are happy employees, and we believe that work/life balance should be easy to achieve. That's why we share the cost of benefits and offer a variety of programs, resources and support you need to bring your full self to work and stay present and committed to the people who matter most - your family.

Learn all about U.S. Bank employee benefits, including tuition reimbursement, retirement plans and more, by visiting usbank.com/careers.

EEO is the Law

Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal EEO is the Law poster.

E-Verify

U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.

Due to legal requirements, U.S. Bank requires that the successful candidate hired for some positions be fully-vaccinated for COVID-19, absent being granted an accommodation due to a medical condition, pregnancy, or sincerely held religious belief or other legally required exemption. For these positions, as part of the conditional offer of employment, the successful candidate will be asked to provide proof of vaccination or approval for an accommodation or exemption upon hire.

U.S. Bank will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance."
478,Senior Data Engineer(Big Data),U.S. Bank,"Portland, OR","At U.S. Bank, we're passionate about helping customers and the communities where we live and work. The fifth-largest bank in the United States, we’re one of the country's most respected, innovative and successful financial institutions. U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors.

Job Description

Be a part of transformational change where integrity matters, success inspires and great teams collaborate and innovate. As the fifth-largest bank in the United States, we're one of the country's most respected, innovative, ethical and successful financial institutions. We're looking for people who want more than just a job - they want to make a difference! U.S. Bank is seeking a Software Engineer who will contribute toward the success of our technology initiatives in our digital transformation journey.

This position will be responsible for the analysis, design, testing, development and maintenance of best in class software experiences. The candidate is a self-motivated individual who can collaborate with a team and across the organization. The candidate takes responsibility of the software artifacts produced adhering to U.S. Bank standards in order to ensure minimal impact to the customer experience. The candidate will be adept with the agile software development lifecycle and DevOps principles.

Essential Responsibilities
Responsible for designing, developing, testing, operating and maintaining products.
Takes full stack ownership by consistently writing production-ready and testable code.
Consistently creates optimal design adhering to architectural best practices; considers scalability, reliability and performance of systems/contexts affected when defining technical designs.
Makes sound design/coding decisions keeping customer experience in the forefront.
Takes feedback from code review and apply changes to meet standards. Conducts code reviews to provide guidance on engineering best practices and compliance with development procedures.
Accountable for ensuring all aspects of product development follow compliance and security best practices.
Exhibits relentless focus in software reliability engineering standards embedded into development standards.
Embraces emerging technology opportunities and contributes to the best practices in support of the bank's technology transformation.
Contributes to a culture of innovation, collaboration and continuous improvement.
Reviews tasks critically and ensures they are appropriately prioritized and sized for incremental delivery. Anticipates and communicates blockers and delays before they require escalation.
Primary Responsibilities
Develop and enhance solutions for Enterprise Landing Zone(ELZ).
Apply Dev/Ops mindset, take ownership of production success, optimize operation success via automation/active alerting/self-healing, and lead the resolution of production issues to ensure high-availability and performance.
Drive the release planning and execution with end-to-end understanding and insights (effort, risk, priority) of the planned features.
Develop high-quality code, define best engineering practice, perform peer code reviews to ensure successful deliverable with engineering excellence.
Document engineering artifacts such as technical design document, flowcharts, system context diagrams, code comments etc.
Collaborate cross-functionally with product owner, data scientists, business users, project managers and other engineers to achieve elegant solutions.
Collaborate with other engineers on the team to elevate technology and consistently apply best
Basic Qualifications
Bachelor's degree, or equivalent work experience
Five to six years of relevant experience
Preferred Skills / Experience
Knowledge of Spark, Scala and other programming languages
Experience in airflow, elastic, google cloud, data quality
Experience in managing big data sets
Experience in Java and object-oriented design skills
Experience building micro-services based applications
Experience in Utilizing tools such as Maven, Docker, Kubernetes, Kibana, ELK, Jenkins and frameworks such as Spring Boot to develop and deploy microservices
Experience using CICD processes for application software integration and deployment using Maven, Git, Jenkins.
Experience building scalable and resilient applications in private or public cloud environments and cloud technologies
Experience with server side languages such as Python, Java, GoLang
Excellent verbal and written communication skills
Considerable technical, logical, analytical and problem-solving skills
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.

Benefits

Take care of yourself and your family with U.S. Bank employee benefits. We know that healthy employees are happy employees, and we believe that work/life balance should be easy to achieve. That's why we share the cost of benefits and offer a variety of programs, resources and support you need to bring your full self to work and stay present and committed to the people who matter most - your family.

Learn all about U.S. Bank employee benefits, including tuition reimbursement, retirement plans and more, by visiting usbank.com/careers.

EEO is the Law

Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal EEO is the Law poster.

E-Verify

U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.

Due to legal requirements, U.S. Bank requires that the successful candidate hired for some positions be fully-vaccinated for COVID-19, absent being granted an accommodation due to a medical condition, pregnancy, or sincerely held religious belief or other legally required exemption. For these positions, as part of the conditional offer of employment, the successful candidate will be asked to provide proof of vaccination or approval for an accommodation or exemption upon hire.

U.S. Bank will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance."
479,"Data Engineer, PXT Central Science",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The People eXperience and Technology (PXT) Central Science Team uses economics, behavioral science, statistics, and machine learning to proactively identify mechanisms and process improvements which simultaneously improve Amazon and the lives, wellbeing, and the value of work to Amazonians. We are an interdisciplinary team which combines the talents of science and engineering to develop and deliver solutions that measurably achieve this goal. We invest in innovation and rapid prototyping of scientific models and software solutions to accelerate informed, accurate, and reliable decision backed by science and data.

We are looking for an experienced data engineer engineer to integrate data sources and create a data infrastructure to model and research all aspects of HR, including: workforce planning, recruiting, employee engagement, retention, development, employee benefits, and compensation.

The ideal candidate is expected to work closely with senior scientists and business leaders to study the impact Amazon has had on workers, the market, and the economy. You will build new data engineering solutions end-to-end, will work with multiple stakeholders across HR and Operations, and will build data pipelines/automation that can be used by distributed systems to run statistical and ML models. A successful candidate will have a passion for innovation, interest in cutting-edge technology, and excitement about working in a high-impact domain.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Coding proficiency in at least one modern programming language (Python, Ruby, Java, Scala etc).
Experience with Big Data technologies such as Hive, Spark, Hadoop, NoSQL, AWS EMR, Glue, Lambda, Kinesis, Redshift.
Excellent grasp of software development life cycle and/or agile development environment.
Preferred Qualifications
5+ years of industry experience as a Data Engineer or related specialty (e.g. Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience in iterative design, working closely with business, product and tech partners from inception through implementation.
Detailed knowledge of data warehouses, architecture, infrastructure components, ETL and reporting tools and environments
Familiarity with AWS services such as S3, Redshift, EMR, Athena etc.
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing.
Experience providing technical direction and mentorship of engineers and scientists on best practices in the data engineering space
Be self-motivated and show ability to deliver on ambiguous situations and projects
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1930065"
480,Urgent Reqs- Open for C2C- Long Term Remote Positions- Database Developer/Data Engineer*02,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Software Technology Inc, is seeking the following. Apply via Dice today!

Hi All, My name is Poornachandra Rao and I am a Technical Recruiter at STI Org. I am reaching out to you on an exciting job opportunity with one of our clients Remote Positions Job Title Database DeveloperData Engineer No of Positions 02 Duration Long term Required Skills We are looking for candidates who have 7+ years of development experience in Database Development and who have a strong understanding of data transformation processes to AWS Glue, DynamoDB, Data Lake and, Snowflake. They also need to have a strong knowledge of informatica andor ETL tools and be able to write SQL queries. Candidates will also need to have experience building data pipelines using AWS S3, Lambda, Python. Thanks and Regards Poornachandra Rao Software Technology Inc. Email mailto Contact"
481,Data Engineer,Blue Orange Digital,United States,"Posted by
Celia Jones
Blue Orange Digital • We're Hiring • Machine Learning Data Analytics • Operations • Writer
Send InMail
This position is open to REMOTE USA Citizens only.
—

Blue Orange Digital is a cloud-based data transformation and predictive analytics development firm with offices in NYC and Washington, DC. From startups to Fortune 500’s, we help companies make sense of their business challenges by applying modern data analytics techniques, visualizations, and AI/ML. Founded by engineers, we love passionate technologists and data analysts. Our startup DNA means everyone on the team makes a direct contribution to the growth of the company.

Blue Orange Digital is looking for a Data Engineer to join our talented multi-disciplinary team. We build data analytics platforms for our clients that incorporate advanced analytics to solve real business problems. Blue Orange Digital works across multiple industries, this role provides an exciting set of experiences across a wide range of domains.

Your primary focus will be the architecting and developing systems that encompass data integration, data modeling, API ingestion, API development, and Tableau reporting. Major technologies involved include Python, Snowflake, and Tableau. Blue Orange engineers take end-to-end ownership of their code and platforms, so the ideal candidate for this position has experience in Data Engineering, Infrastructure & Reporting.

Core Responsibilities & Skills

Architecting, building, and maintaining modern, scalable data architectures
Integrate data from multiple disparate sources into a single Snowflake data warehouse.
Working with stakeholders to ensure the data model meets the needs of the business.
Building resilient production ETL pipelines using python based workflow orchestration tools such as Airflow, Prefect, etc
Data exploration, analysis, and reporting with an eye toward developing a narrative.
Qualifications

BA/BS degree in Computer Science or a related technical field, or equivalent practical experience.
3+ Years of experience working in a Data Engineering role.
Advanced experience in Python with an excellent understanding of computer science fundamentals, complex data structure, data processing, data quality, data lifecycle, and algorithms.
1+ years of experience with Snowflake
AWS certification, or progress toward, the associate level (Solutions Architect or Developer), or specialty (Data Analytics) a strong advantage.
Enjoys collaborating with other engineers on architecture and sharing designs with the team
Excellent verbal and written English communication.
Interacts with others using sound judgment, good humor, and consistent fairness in a fast-paced environment

Our Benefits Include:
Unlimited PTO
401k Match
Healthcare, Dental, Vision, Life
Professional development support and reimbursement
Company Computer
Remote and flexible schedule

Compensation:
$90,000 - 140,000"
482,Senior Data Engineer,Publicis Sapient,"Chicago, IL","Publicis Sapient is looking for a Senior Associate Data Engineer to be part of our team of top-notch technologists. You will lead and deliver technical solutions for large-scale digital transformation projects. Working with the latest data technologies in the industry, you will be instrumental in helping our clients evolve for a more digital future.

Your Impact:

Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our clients’ business
Translate clients requirements to system design and develop a solution that delivers business value
Lead, design, develop and deliver large-scale data systems, data processing and data transformation projects
Automate data platform operations and manage the post-production system and processes
Conduct technical feasibility assessments and provide project estimates for the design and development of the solution
Mentor, help and grow junior team members

Qualifications

Your Skills & Experience:

Demonstrable experience in data platforms involving implementation of end to end data pipelines
Good communication and willingness to work as a team
Hands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)
Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)
Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.
Ability to handle module or track level responsibilities and contributing to tasks “hands-on”
Experience in data modeling, warehouse design and fact/dimension implementations
Experience working with code repositories and continuous integration
Set Yourself Apart With:

Developer certifications for any of the cloud services like AWS, Google Cloud or Azure
Understanding of development and project methodologies

Additional Information

Benefits of Working Here:

Flexible vacation policy; time is not limited, allocated, or accrued
Generous parental leave and new parent transition program
Tuition reimbursement
Corporate gift matching program
As part of our dedication to an inclusive and diverse workforce, Publicis Sapient is committed to Equal Employment Opportunity without regard for race, color, national origin, ethnicity, gender, protected veteran status, disability, sexual orientation, gender identity, or religion. We are also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at hiring@publicissapient.com or you may call us at +1-617-621-0200."
483,"Data Engineer - FinTech, FinTech TDW",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using Amazon Web Services technologies? Do you want to create the next-generation tools for intuitive data access?

Amazon's Finance Technology team is seeking truly innovative Data Engineer to join the team that is shaping the future of the finance data platform. The team is committed to building the next generation big data platform that will be one of the world's largest finance data warehouse to support Amazon's rapidly growing and dynamic businesses, and use it to deliver the BI applications which will have an immediate influence on day-to-day decision making. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable. Our platform serves Amazon's finance, tax and accounting functions across the globe.

As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of large datasets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The individual is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring datasets together to answer business questions and drive change.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint.

Responsibilities
Design, implement, and support a platform providing secured access to large datasets.
Interface with tax, finance and accounting customers, gathering requirements and delivering complete BI solutions.
Model data and metadata to support ad-hoc and pre-built reporting.
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Tune application and query performance using profiling tools and SQL.
Analyze and solve problems at their root, stepping back to understand the broader context.
Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.
Key job responsibilities

A Data Engineer II is an autonomous contributor who uses a broad set of design approaches to pragmatically solve difficult problems delivering high quality, stable and performant code. They use their expertise in team’s infrastructure and data products like Python framework, TDW USL Lake, FGBSBI to provide technical direction to tax users, works with Tax Tech to understand the business requirements, defines physical and logical data model, designs the solution and contributes great amount to the code base, and guides other engineers on the same team as well as from other teams. They review their design with senior engineers to ensure data engineering best practices are followed. They insist on high standards and set an example with his code, design and implementation decisions. They lead the way in adopting native AWS technologies and implementing new trends in data engineering space. They continuously focus on operational excellence, constructively identify problems and propose solutions to fix them for good.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in CS or related technical field
3+ years experience in dimensional data modeling, ETL development, and Data Warehousing
Experience with Redshift and/or other distributed computing systems.
Excellent knowledge of SQL and Linux OS
SQL performance tuning
Server management and administration including basic scripting
Basic DBA tasks
Solid experience in at least one business intelligence reporting tools
Preferred Qualifications
Bacheakor's degree in Information Systems or a related field.
Knowledge of Big Data Solutions. Experience with Hadoop, Hive or Pig.
Experience with Redshift and other AWS services.
Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams.
Knowledge of a programming or scripting language (R, Python, Ruby, or JavaScript).
Experience with Java and Map Reduce frameworks such as Hive/Hadoop.
Strong organizational and multitasking skills with ability to balance competing priorities.
An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm.
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1826909"
484,"Data Engineer - SCAR team, supporting Global Specialty Fulfillment",Amazon,"Nashville, TN","Description

The Strategies Labor Business Intelligence team, supporting the Global Specialty Fulfillment (GSF) org, is seeking a truly innovate Data Engineer with broad technical skills to help build the infrastructure and tools required to support new, fast growing businesses.

The ideal candidate will have a passion for data and effective simple solutions for complex problems. The applicant will draw upon advanced data engineering knowledge in the data warehousing space, using multiple platforms and technologies. You will work to support and collaborate with a team of business intelligence engineers to find reliable data solutions to meet the business needs. We look for candidates who are excellent communicators, self-motivated, flexible, hardworking, and who like to have fun.

The ownership for this role involves design and development of automated data pipelines, sophisticated analytical model and sustaining operations excellence. In addition, this role is on an analytical team that supports a wide range of businesses including Prime Now, Whole Foods Market, Central Operations, Locker+, Learning, etc. This role has great exposure to a broad scope that can help shape the future of operational fulfillment, personal growth, and job satisfaction.

Main responsibilities of this role include but are not limited to:
Perform data extraction, manipulation and production from database tables
Manage and grow a database infrastructure
Design, implement and validate complex logic using queries and programming languages
Develop automation solutions through programming languages
Support analytical researches and provide recommendations to business challenges

Basic Qualifications
BS degree in quantitative or technical discipline or 4 years’ related Amazon experience.
4+ years of industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
3+ years of experience with SQL
3+ years experience in a modern programming language (ex. Java, Ruby, , etc.)
2+ years experience in database and data table design
Preferred Qualifications
Masters Degree
Experience in analytical projects
2 years of working experience in big data solutions like Hadoop, EMR, PySpark, Hive.
Ability to dive deep on to issues on data pipeline failure, data quality alerts, security alarms.
Strong ability to interact, communicate, present and influence within multiple levels of the organization
Excellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions
Ability to work on a diverse team or with a diverse range of coworkers
Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have 12 affinity groups (employee resource groups) with more than 87,000 employees across hundreds of chapters around the world. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which reminds team members to seek diverse perspectives, learn and be curious, and earn trust.

Flexibility

It isn’t about which hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We offer flexibility and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

We care about your career growth too. Whether your goals are to explore new technologies, take on bigger opportunities, or get to the next level, we'll help you get there. Our business is growing fast and our people will grow with it.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us


Company - Amazon.com Services LLC

Job ID: A1661780"
485,Data Engineer (ETL Developer),Dice,"Stamford, CT","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Torana Inc, is seeking the following. Apply via Dice today!

iceDQ is the 1 Data Testing Monitoring Platform used by Fortune 500 companies such as NYSE, Fidelity, Pfizer and many more. We are looking for data engineers with ETL experience to work with our iceDQ solutions teams. The candidate will liaison with internal product team and provide solutions to clients for their data warehouse, data migration and production data monitoring projects. Time Fulltime Salary Per Experience Location Stamford, CT Responsibilities Work closely with the clients. Suggest and implement best practices for data quality (Data Testing and Monitoring) Determines organizational strategies for data integrity validation processes. Establishes policies and best practices for optimizing ETL data throughputaccessibility Partner with client and product leadership team to ensure the successful implementation of I iceDQ projects. Remain current on new ETL techniques and methodologies and communicate trends and opportunities to management and other developers as needed. Playing a key role in mentoring other staff members and a proven track record in knowledge transfer to other team members and departments. Evaluates existing applications that could address client requirements and makes recommendations from complex projects. Identifies necessary changes in the iceDQ product and recommend to product. Primary Skills ETL Tools Experience with development of complex ETL routines with tools like informatica, SSIS, DataStage, etc. SQL Very good knowledge of SQL, Analytic SQL functions, stored procedures. Data Modeling Must understand data models, star schemas, dimensions, and facts. 3nf data models Nice To Have Databases Snowflake, Redshift, Oracle, Azure SQL, Synapse, BigQuery Big Data Spark, Apache Beam Scripting Python, Shell, DevOpsDataOps Scheduling Tools Autosys, Control-M, Airflow"
486,Data Engineer (Alteryx & Pyspark Exp)- Remote,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Apexon, is seeking the following. Apply via Dice today!

Data Engineer (Alteryx Pyspark Exp Must)- Remote bull Analyze the data requirements, complex source data, and the data model, and determine the best methods in extracting, transforming, and loading the data into the data stagingappropriate database systems. Update and maintain Alteryx workflow. bullAble to createunderstand the complex business logics of the Alteryx workflow. bullconnect different sources, aggregate, and generate report. bullSupport team on migrating workflows from Alteryx to Pyspark. bullShould be capable of designing reviewing the Pyspark. Bhavani Apexon Mobile +1"
487,"Data Engineer, AMXL Analytics",Amazon,"Seattle, WA","Job Summary

DESCRIPTION

The Amazon Core Transportation Technology team is looking for a smart and ambitious individual to support the development of analytical tools and data processing solutions for the Amazon transportation business. This role will work within a multi-functional BI and data engineering team with a focus on AMXL (Amazon Extra Large) and other rapidly growing delivery programs where Amazon is constantly innovating to find new ways to deliver to our customers.

The ideal candidate thrives working with large volumes of data, enjoys the challenge of highly complex technical contexts, and is passionate about data and analytics. The candidate is an expert within data modeling, ETL design and cloud/big-data technologies and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates large-scale business impact. The candidate should be a self-starter; comfortable with ambiguity, able to think big, and enjoy working in a fast-paced and global team. It’s a big ask, and we’re excited to talk to those up to the challenge!

Key job responsibilities
Build end-to-end data pipelines to ingest and transform data from different types of data sources and systems; including traditional ETL pipelines and event data streams
Utilize data from disparate data sources to build meaningful datasets for analytics and reporting use cases
Evaluate and implement various big-data technologies and solutions (e.g. Redshift, Hive/EMR, Spark, SNS, SQS, Kinesis) to optimize processing of extremely large datasets
Understand and analyze business processes, logical data models and relational database implementations
Write high performing and optimized SQL queries
Execute research projects, and generate practical results and recommendations
Design and implement automated data processing solutions and data quality controls

Basic Qualifications
Bachelor's degree or higher in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering, Information Technology)
2+ years of related data engineering experience, preferably for a technology company handling large, complex data sources
Experience with big data technologies and frameworks (e.g. Hive, Spark, Hadoop, SQL on Big Data)
Experience in near real-time data processing and analytics
Experience with one or more scripting languages (e.g. Python, Perl)
Experience with ETL, Data Warehousing, Data Modeling, and working with large-scale data sets
High proficiency in SQL; comfortable optimizing complex queries
Highly motivated, self-driven, and capable of defining own design and test scenarios
Preferred Qualifications
Master's degree in a quantitative/technical field (e.g. Computer Science, Statistics, Engineering, Information Technology)
4+ years of related data engineering experience, preferably for a technology company handling large, complex data sources
Experience with AWS storage, compute, database, application integration, and analytics solutions (e.g. S3, Redshift, Lambda, DynamoDB, Step Functions, EMR, SNS, SQS)
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1954035"
488,Senior Data Engineer - Instant Ink,HP,"San Diego, CA","The data engineering role is a team member that will help enhance and maintain the Instant Ink Business Intelligence system. This role will work with external and internal business partners to capture business requirements and help develop solutions to support the business. You will drive work you're doing to completion with hands-on development responsibilities, and partner with the Data Engineering leaders to provide thought leadership and innovation with those results.

This individual will apply developed subject matter knowledge to solve common and complex business issues and recommend appropriate alternatives. The daily work will consist of solving problems that are of diverse complexity and scope, while exercising independent judgment within generally defined policies and practices. This role will require handling unique or unclear requirements and seek advice from team members and leaders to make decisions on complex business issues.

Responsibilities
Works with other data engineers and architects to establish secure and performant data architectures, enhancements, updates, and programming changes for portions and subsystems of data platform, repositories or models for structured/unstructured data.
Analyzes design and determines coding, programming, and integration activities required based on general objectives and knowledge of overall architecture of product or solution.
Promotes and drives use of agile and DevOps methodologies and patterns including continuous integration, continuous testing, test-driven development, continuous delivery, etc.
Follow, promote and adopt HP’s Release and Change Management processes.
Reviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards; provides tangible feedback to improve product quality and mitigate failure risk.
Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies defects and creates solutions for issues with code and integration into data system architecture.
Collaborates and communicates with project team regarding project progress and issue resolution.
Works with the data engineering team for all phases of larger and more-complex development projects and engages with external users on business and technical requirements.
Collaborates with peers, engineers, data scientists and project team.
Typically interacts with high-level Individual Contributors, Managers and Program Teams on a daily/weekly basis.
Helps define and lead portions of project requirements for data exchanges and business requirements with externals and internal teams
Collaborates with SMEs to develop procedures for collecting, recording, analyzing, and communicating data for review and feedback.
What You Bring

We are looking for world class talent that brings the following key skills and experience to this role:
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering or equivalent.
6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
3+ years of experience with Cloud based DW such as Redshift, Snowflake etc.
Experience working on CI/CD processes and source control tools such as GitHub, etc.
Strong experience in coding languages like Python, Scala & Java
Knowledge And Skills
Fluent in relational based systems and writing complex SQL.
Fluent in complex, distributed and massively parallel systems.
Strong analytical and problem-solving skills with ability to represent complex algorithms in software.
Experience in building lambda, kappa, microservice and batch architecture
Designing data systems/solutions to manage complex data.
Strong understanding of database technologies and management systems.
Strong understanding of data structures and algorithms
Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.
Effective communication skills (with team members, the business, and in code)
Ability to effectively communicate product architectures, design proposals and negotiate options at management levels.
Has a passion for data solutions and willing to pick up new programming languages, technologies, and frameworks
Strong analytical and problem-solving skills.
Nice to Have
Experience with transformation tools such as dbt.

About HP

You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.

So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.

HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.

Our history: HP’s commitment to diversity, equity and inclusion - it's just who we are.

From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!"
489,Data Engineer,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Global Talent Resources, is seeking the following. Apply via Dice today!

Position Overview A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecturemodeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value. Requirements 10+ years of development experience Bachelorrsquos degree in Computer Science, MIS or related field (industry experience substitutable) Expert level in data warehouse designarchitecture, dimensional data modeling and ETL process development Advanced level development in SQLNoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus) Extremely proficient in Python, PySpark, and Java AWS Expertise ndash Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena Streaming Services ndash Confluent Kafka and Kinesis (or equivalent) Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)"
490,Data Engineer--Off Prem Environment--100% Remote Role,Dice,United States,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, EMW Staffing Solutions LLC, is seeking the following. Apply via Dice today!

Thanks for your possible interest. Please apply for further information. Term Full Time Open Remote Data Engineer ndash 100 Remote both mid-level and senior-level candidates are acceptable vertical Financial Dashboard Must haves 3+ years of experience with Data Engineering in cloud-based environments (they are multi cloud with AWS + Azure) Spark or PySpark experience Python experience Nice to have Snowflake, or other cloud based data warehouse experience DevOps experience Summary This role is charged with developing data pipelines integrating dozens of data sources and target systems, ensures data quality, creates models, and then exposes the data in various forms for consumption by other teams and platforms. Looking for a software engineer who has experience with data infrastructure, ETL systems, reporting environments, or machine learning pipelines. What yoursquoll be doing Design, develop, and launch efficient and reliable data pipelines to move and transform data to enable downstream consumption and insights Refine code to ensure performance and reliability of data pipelines and data processing Write unit tests, and automate functional and integration tests Collaborate with other data engineers, analysts, product managers, and customers to implement technical solutions to business problems Support the development process by participating in regular AgileScrum activities, code reviews, and architectural reviews Implement DevOps patterns in conjunction with our DevOpsSREs Diagnose and solve issues in existing data pipelines, and use learnings to build their successors Skills wersquore seeking 3+ years of experience with Data Engineering in cloud-based environments Demonstrated hands-on Engineering experience delivering large scale data, analytics, reporting, or machine learning productssolutions Strong SQL experience Python experience Experience with the issues of performance, availability, scalability, reliability, and maintainability of data pipelines Nice to have experience Experience with Spark and PySpark Data modeling experience DevOps experience"
491,Data Engineer,Deloitte,"Birmingham, AL","Are you an experienced, passionate pioneer in technology - a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You'll Do/Responsibilities
Partner with product, analytics, and data engineering in interpreting business and analytics requirements and converting them into robust data pipelines.
Work with feature and data engineering to drive product reporting and support development.
Support reporting for multiple projects concurrently.
Write, analyze, and debug SQL queries that range in difficulty from simple to complex.
Ensure standards for engineering excellence, scalability, reliability, and reusability.
Ability of manipulating, processing, and extracting value from large, disconnected datasets.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
The Team

Our Core Technology Operations (CTO) team offers differentiated operate services for our clients with solutions to help organizations scale and optimize critical business operations, drive speed to outcome, deliver business transformation, and build resilience in an uncertain future.

Our operate services within CTO include:
Foundry Services: Operate services providing flexible, recurring resource capacity for client initiatives, projects, tasks, and enhancement.
Managed Services: Operate services that provide ongoing maintenance, monitoring, and optimization for IT/Engineering applications & products.
Required

Qualifications
Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
4 to 6 years of hands-on experience as a Data Engineer.
Experience building and optimizing data pipelines, architectures, and datasets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Advanced working knowledge of SQL and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of databases structures.
Working knowledge of message queuing, stream processing, and scalable 'big data' stores.
Travel up to 10% annually.
Limited immigration sponsorship may be available.
Preferred
Background in Financial Services preferred."
492,"Data Engineer II, Alarm BI Team",Amazon,"Malvern, PA","Job Summary

DESCRIPTION

Ring is seeking a Data Engineer with strong analytical, communication, and project management skills to join our Alarm Business Intelligence Team.

The successful candidate will be an analytical problem solver who enjoys diving into data, is excited about solving ambiguity or performance problems, can multi-task, and can effectively interface between technical teams and business stakeholders.

Key job responsibilities

Working closely with Business and Product Stakeholders, Software Development Engineers, Business Intelligence Engineers and the Ring Data Management team, you will design and implement state-of-the-art solutions for high data volume applications. You will work with a complicated data environment and employ the right architecture to handle data and support various analytics use cases including business reporting and production data pipeline.

About The Team

The Ring Alarm BI Team provides multiple layers of product analytics to help drive quality, reliability, usability and profitable growth of the Ring Alarm and Ring Smart Lighting product families. Examples of the reporting we provide include business metrics, new feature adoption and performance levels, identification and analysis of potential issues in the field and support of self-service analytics at the engineering level.


Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
3+ years of experience with data warehousing concepts, including data warehouse technical architectures, infrastructure components, ETL and reporting/analytic tools and environments, and data structures.
3+ years of data modeling experience and proficiency in writing complex SQL with Redshift, PostgreSQL, MySQL, and Columnar Databases
Preferred Qualifications
Experience with big data technologies such as Hive/Spark
Experience operating very large data warehouses or data lakes.
Experience with AWS services including S3, Lambda, EMR, RDS, Data-pipeline and other big data technologies
Ability to analyze current work processes and systems and clearly articulate potential improvement opportunities to leadership
An ability and interest in working in a fast-paced and rapidly-changing environment
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1930208"
493,Data Engineer - Metrics,Datadog,"New York, NY","About Datadog:

We're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale—trillions of data points per day—providing always-on alerting, metrics visualization, logs, and application tracing for tens of thousands of companies. Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way.

You will:
Build distributed, high-volume data pipelines that power the core Datadog product
Do it with Spark, Luigi, Kafka and other open-source technologies
Work all over the stack, moving fluidly between programming languages: Scala, Java, Python, Go, and more
Join a tightly knit team solving hard problems the right way
Own meaningful parts of our service, have an impact, grow with the company

Requirements:
You have a BS/MS/PhD in a scientific field or equivalent experience
You have built and operated data pipelines for real customers in production systems
You are fluent in several programming languages (JVM & otherwise)
You enjoy wrangling huge amounts of data and exploring new data sets
You value code simplicity and performance
You want to work in a fast, high growth startup environment that respects its engineers and customers

Bonus points:
You are deeply familiar with Spark and/or Hadoop
In addition to data pipelines, you’re also quite good with Kubernetes and cloud technology
You’ve built applications that run on AWS
You’ve built your own data pipelines from scratch, know what goes wrong, and have ideas for how to fix it

Is this you? Send your resume and link to your GitHub if available.

Equal Opportunity at Datadog:

Datadog is an Affirmative Action and Equal Opportunity Employer and is proud to offer equal employment opportunity to everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, veteran status, and more. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.

Your Privacy:

Any information you submit to Datadog as part of your application will be processed in accordance with Datadog’s Applicant and Candidate Privacy Notice."
494,AWS Data Engineer,Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.
Work You'll Do/Responsibilities
Creating/managing AWS services
Work with distributed systems as it pertains to data storage and computing
Building and supporting real-time data pipelines
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business
Establish solid design and best engineering practice for engineers as well as non-technical people
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The AI & Data Operations team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

AI & Data Operations will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
3+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually."
495,"Data Engineer, Data Products Engineering",NBCUniversal,United States,"Introduction

Responsibilities

At NBCUniversal, we believe in the talent of our people. It’s our passion and commitment to excellence that drives NBCU’s vast portfolio of brands to succeed. From broadcast and cable networks, news and sports platforms, to film, world-renowned theme parks and a diverse suite of digital properties, we take pride in all that we do and all that we represent. It’s what makes us uniquely NBCU. Here you can create the extraordinary. Join us.

About The Role

Data Engineer for the Data Products Engineering Team. Our team builds data pipelines to land, profile and store multiple internal & external datasets and build applications that surface this data to support our business partners’ strategic decision making. We are an AWS shop that uses open source technologies including Python, Pandas, Spark, Hive, Postgres, Redis, MongoDB, Flask, as well as BI tools such as Tableau and MicroStrategy. We work in a very agile environment, where product specifications are flexible and often change rapidly over time. We are seeking people who are comfortable with ambiguity and figuring out an execute. While the key focus for this role is on backend engineering, engineers who have full stack expertise and can write front-end code will be especially considered.

Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions
Leverage prototyping methodologies to propose and design creative business solutions that exploit our broad toolset of technologies (Big Data, MicroStrategy, Tableau, Python, Spark etc)
2+ years experience with AWS technologies. Strong experience using Python and Pandas in an AWS Lambda framework is highly desired. Experience using EMR and/or DataBricks or the ability to read EMR code and translate it into Lambdas.
Must understand the basics of relational data modeling and be able to clearly articulate the reasons to use non-relational systems in our architecture. Experience in MemSQL is desired but relevant experience in any of the following is acceptable: SnowFlake, MySQL, Redshift, Athena, MSSQL Server, Oracle. Experience in non-relational systems such as Redis, Cassandra, and MongDB is useful for supporting legacy applications.
Decent understanding for the digital media ad sales business and ad serving technologies with experience working with ad serving transactional data logs or Nielsen demographic data.
Educate and inform business partners on architecture, capabilities, best practices and solutions to build out future enhancements
Assist in analyzing business requirements, source systems, understand underlying data sources, transformation requirements, data mapping, data model and metadata for reporting solutions
Writing easily understood documentation and architecture diagrams and keeping them up to date as code and frameworks change over time.

Requirements

Qualifications/Requirements

Bachelor’s degree in Engineering, Computer Science, Information Systems or related field with 3+ years of relevant experience.
Strong Computer Science/Engineering/Information Systems background
3+ Years Experience in Data Modeling, Data architecture, Data Quality, Metadata, ETL and Data Warehouse methodologies and technologies.
Experience in any combination of the following: SQL, Linux, MicroStrategy, Tableau, Python, APIs, Spark, Scala, Pandas
Strong problem-solving skills.
Strong oral and written communication and influencing skills, with the ability to communicate new concepts and drive change in processes and behaviors and to communicate complex technical topics to management and non-technical audiences.

Desired Characteristics

Preferred Qualifications

1+ years in Digital Media Publisher Industry with a solid understanding of Digital Research
Experience with various digital platforms such as Omniture (Site Catalyst), Rentrak, comScore, Operative One, Google DoubleClick, Freewheel, Ad-Juster, MOAT, Nielsen, Facebook, Twitter, etc
Understanding of how to manage code in the Enterprise Git repository with appropriate branching and documentation skills
Ability to design concise and visually appealing reports, user interfaces, mockups and documentation
Ability to read external API documentation and write pipelines to extract data from our partners’ systems
Ability to write and stand up internal API endpoints to share data with other internal teams.
Strong analytical focus, results-oriented and execution driven.
Ability and desire to work within a cross-functional team environment with people from multiple business units, vendors, countries and cultures.
Self-driven/self-initiator and resourceful to achieve goals independently as well as in teams and promotes an open flow of information so that all stakeholders are well informed.
Flexibility to adjust to changing requirements, schedules and priorities.
Ability to work independently under minimum supervision and proactive in solving issues
Energetic, committed and solution focused with the ability to perform under pressure and meeting targets

Sub-Business

Engineering

Career Level

Experienced

City

See List Below

State/Province

Multiple Locations

Country

United States

Multiple Locations

New York - New York, Remote

About Us

NBCUniversal owns and operates over 20 different businesses across 30 countries including a valuable portfolio of news and entertainment television networks, a premier motion picture company, significant television production operations, a leading television stations group, world-renowned theme parks and a premium ad-supported streaming service.

Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. We strive to foster a diverse and inclusive culture where our employees feel supported, embraced and heard. We believe that our workforce should represent the communities we live in, so that together, we can continue to create and deliver content that reflects the current and ever-changing face of the world. Click here to learn more about Comcast NBCUniversal’s commitment and how we are making an impact.

Notices

NBCUniversal’s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.

NBCUniversal is an equal opportunity employer and will provide reasonable accommodations as required by applicable federal, state, and/or local laws."
496,AWS Data Engineer,Deloitte,"Hartford, CT","Are you an experienced, passionate pioneer in technology? A system's professional who wants to work in a collaborative environment. As an experienced AWS Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

Work You Will Do/Responsibilities
Creating/managing AWS services.
Work with distributed systems as it pertains to data storage and computing.
Building and supporting real-time data pipelines.
Design and implement reliable, scalable, robust and extensible big data systems that support core products and business.
Establish solid design and best engineering practice for engineers as well as non-technical people.
Support the implementation of data integration requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
The Team

In this age of disruption, organizations need to navigate the future with confidence, embracing decision making with clear, data-driven choices that deliver enterprise value in a dynamic business environment.

The Analytics & Cognitive team leverages the power of data, analytics, robotics, science and cognitive technologies to uncover hidden relationships from vast troves of data, generate insights, and inform decision-making. Together with the Strategy practice, our Strategy & Analytics portfolio helps clients transform their business by architecting organizational intelligence programs and differentiated strategies to win in their chosen markets.

Analytics & Cognitive will work with our clients to:
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate insights leveraging cloud-based platforms.
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions.
Drive operational efficiency by maintaining their data ecosystems, sourcing analytics expertise and providing As-a-Service offerings for continuous insights and improvements.
Required

Qualifications
Bachelor's or Master's degree in Computer Science, Information Technology, Computer Engineering, or related IT discipline, or related technical field; or equivalent practical experience.
5+ years of hands-on experience as a Data Engineer.
Experience with the Big Data technologies (Hadoop, M/R, Hive, Spark, Metastore, Presto, Flume, Kafka, ClickHouse, Flink etc).
Experience in performing data analysis, data ingestion and data integration.
Experience with ETL(Extraction, Transformation & Loading) and architecting data systems or Python PySpark.
Experience with implementation, migrations and upgrades in the AWS Cloud environment.
Experience with schema design, data modeling and SQL queries.
Passionate and self-motivated about technologies in the Big Data area.
Experience building and deploying micro-based applications in cloud with Continuous Integration & Continuous Deployment (CI/CD) tools and processes.
Experience with traditional and Cloud based API development.
Strong data & logical analysis skills.
Limited immigration sponsorship may be available.
Travel up to 10% annually.
Preferred
Ability to work independently and manage multiple task assignments
Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint)
Strong problem solving and troubleshooting skills with the ability to exercise mature judgment
Strong analytical and technical skills"
497,Data Engineer – Leading Hedge Fund – Front Office Data Science Team,Jobs via eFinancialCareers,"New York, NY","They are one of the most successful tech-driven finance firms in the world, who value modern technology and quality engineering at the core of their success as a business. They are looking for the top financial technologists in the market and are paying the best salaries in order to get them, and they have hired many people from the Big Tech firms in Silicon Valley in the past year.

Requirements
In-depth experience working with Python
Strong understanding of computer science fundamentals (data structures and algorithm knowledge)
An interest in Data Science and Machine Learning
Excellent academics – Bachelor’s degree level in Computer Science or numerate subject from a top college"
498,Cloud Data Engineer-LOCATION OPEN,Deloitte,"Charlotte, NC","Are you an experienced, passionate pioneer in technology; are solutions your focus, a roll-up-your-sleeves individual who thrives in a daily collaborative environment, a think-tank who can share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.

Our US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...

Work You'll Do/Responsibilities
Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Use an analytical, data-driven approach to drive a deep understanding of fast changing business.
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS, Azure or GCP cloud platform.
The Team

From our centers, we work with Deloitte consultants to design, develop and build solutions to help clients reimagine, reshape and rewire the competitive fabric of entire industries. Our centers house a multitude of specialists, ranging from systems designers, architects and integrators - to creative digital experts - to cyber risk and human capital professionals. All work together on diverse projects from advanced preconfigured solutions and methodologies, to brand-building and campaign management.

We are a unique blend of skills and experiences, yet we underline the value of each individual, providing customized career paths, fostering innovation and knowledge development with a focus on quality. The US Delivery Center supports a collaborative team culture where we work and live close to home with limited travel.

Qualifications Required
3+ years of experience in data engineering with an emphasis on data analytics and reporting.
3+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others.
3+ years of experience in SQL, data transformations, statistical analysis, and troubleshooting across more than one Database Platform (Cassandra, MySQL, Snowflake, PostgreSQL, Redshift, Azure Synapse, etc.).
3+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines.
3+ years of experience with one or more of the follow scripting languages: Python, SQL, Scala, PySPark and/or other.
3+ years of experience designing and building solutions utilizing various Cloud services such as EC2, S3, EMR, Kinesis, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, Streaming services etc.
Bachelor's degree or equivalent work experience.
Must live a commutable distance to one of the following cities: Atlanta, GA; Austin, TX; Boston, MA; Charlotte, NC; Chicago, IL; Cincinnati, OH; Cleveland, OH; Dallas, TX; Detroit, MI; Gilbert, AZ; Houston, TX; Indianapolis, IN; Kansas City, MO; Lake Mary, FL; Mechanicsburg, PA; Miami, FL; Minneapolis, MN; Nashville, TN; Philadelphia, PA; Phoenix, AZ; Pittsburgh, PA; Sacramento, CA; St. Louis, MO; San Diego, CA; Seattle, WA; Tallahassee, FL; Tampa, FL; or be willing to relocate to one of the following USDC locations: Gilbert, AZ; Lake Mary, FL; Mechanicsburg, PA.
Limited Immigration sponsorship may be available.
Ability to travel up to 15% (While 15% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice.)
Preferred
AWS, Azure and/or Google Cloud Platform Certification.
Master's degree or higher.
Expertise in one or more programming languages, preferably Scala, PySpark and/or Python.
Experience working with either a Map Reduce or an MPP system on any size/scale.
Experience working with agile development methodologies such as Sprint and Scrum."
499,"Data Engineer , DaS FinTech",Amazon,"Sunnyvale, CA","Job Summary

DESCRIPTION

Amazon’s Digital Finance organization has an immediate opening for a Business Intelligence Software Developer.

Amazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV and Amazon Echo.

As a Senior Data Engineer you will be leading the charge in developing next generation software that would help finance team with data analysis and business decisions. You will work with a wide range of data technologies (e.g. MySQL, Redshift, EMR. etc.) and stay abreast of emerging technologies, investigating and implementing where appropriate.

Our ideal candidate possess a strong passion for object oriented software design and development, analytics and data modeling, sets high standards, is accurate and strives to stay ahead of a dynamic and fast growing business. The successful candidate will also demonstrate strong communication skills and possess a high level of business acumen in order to work with business owners and deliver analysis that leads to business actions. Above all the candidate is passionate about working with huge data sets and is someone that loves to answer business questions that drive change.

Key Responsibilities
Design, implement, and support applications that provide structured and timely access to actionable business information addressing stakeholder needs.
Interface directly with stakeholders, gathering requirements and owning automated end-to-end reporting solutions.
Partner with analysts, data engineers, business intelligence engineers, and software development engineers across Consumables org and Amazon to produce complete data solutions.
Investigate and implement new big data technologies to solve business problems where appropriate.
Participate in developing technical strategy for turning data into actionable information.
Develop complex queries for ad hoc requests and projects, as well as ongoing reporting.

Basic Qualifications
3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
BS or MS in Computer Science or related field
At least 5 years of experience with SQL, Schema design
At least 2 years of experience with a high level language preferably Python
Good operating knowledge of UNIX systems
Familiarity with unix tools like shell, awk, sed
At least one full life-cycle project experience in each of following responsibilities:
Design and develop systems for processing, filtering, and presenting large quantities of data with complex relationships.
Database modelling for structured data sets having high volume and variety.
Creating SQL scripts for one or more of Redshift, MySQL, Oracle
Preferred Qualifications
Experience with Tableau or any other visualization tool.
Experience with statistical analysis, regression modeling and forecasting, time series analysis, data mining, financial analysis, and demand modeling
Strong analytical skills, troubleshooting and problem-solving skills with passions on
Good understanding of design for scalability, performance and reliability.
Excellent communication skills and the ability to work well in a team.
Lab126 is part of the Amazon.com, Inc. group of companies and is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1857375"
